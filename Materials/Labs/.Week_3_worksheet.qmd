---
title: "Worksheet 3"
subtitle: "Linear regression in R"
format: 
  html: 
    code-link: true
# output:
#   html_document:
#     theme: lumen
#     toc: yes
#     toc_float: yes
#     toc_highlight: no
#     tabset: yes
#     number_sections: no
#     anchor_sections: no
#     self_contained: yes
#     code_download: no
execute: 
  warning: false
  echo: true
css: labcss.css
bibliography: references.bib
---

------------------------------------------------------------------------

```         
Timetable week: 6
Topic: "Linear models"
```

------------------------------------------------------------------------

# Learning outcomes

By the end of the session you will know how to:

-   Fit and summarise linear regression models in `R`
-   Interpret results from linear regression models
-   Understand the main assumptions behind linear regression models

# Intro

One of the most commonly encountered statistical methods in social science publications is **linear regression**. This week we will make some first steps towards understanding what this method involves and applying it in practice using the `R` programming language.

We begin with the simple (*but not trivial!* - as Gelman et al. (2020: 93) admonish) case where we model the relationship between two ***numeric*** (***continuous***) variables using a ***simple linear regression model***. The aim of a **simple linear regression model** is to predict the expected values of one ***numeric*** variable (e.g. a child's *age*) from another ***numeric*** variable (e.g. the child's *height*). The variable we want to predict is usually called the *response* (or *outcome*/*dependent*/*explained* variable) and the variable used for predicting this outcome is referred to as the *explanatory* (or *predictor*/*independent* variable).

In actual applied sociological research the *simple linear regression* model is rarely used on its own because we understand that in the social world there are always many factors that influence an outcome at the same time - even in the case of modelling a child's *height*, we know that *age* is an important factor, but there is some considerable variation in height even among children of the same age, so there must be other factors at play too. We also understand that assuming a strict ***linear*** relationship between two variables is too much of an oversimplification (staying with the *height \~ age* example, while the linear assumption may be realistic when applied to children, it is definitely not applicable if we extend the analysis to all ages: an increase in age is associated with an increase in height for children, but once the age of maturity is reached, we no longer get taller as we get older - in fact, in old age, as we get older we tend to get shorter!).

But the simple linear regression model contains many of the statistical components on which other statistical tests and more complex models are built, so understanding it is essential. We will also learn a few concepts and methods related to simple linear regression - such as *correlation* (also known as the *Pearson correlation,* after the mathematician [Karl Pearson](https://en.wikipedia.org/wiki/Karl_Pearson "Wikipedia")) - and we'll make some steps towards expanding the linear model to include non-continuous predictors. The main `R` function that we will learn is `lm()` (check `help(lm)` for technical information).

# Exercise 0: Setup

1.  **Open the `R Studio` interface** by clicking on the ***.Rproj*** file included in the project folder that you created in *Lab2*. The folder should be stored on your Newcastle University **OneDrive** and accessible from any computer.

2.  **Create a new blank Quarto document for this lab session and call it *Lab3.qmd***

3.  At the top of the page briefly detail what the script is about (e.g. "Lab work for Week x").

4.  **Load `R` packages** that we commonly use with the `library()` function

    `r fontawesome::fa("wrench")` **Tip:** You may need to first install the package with the `install.packages()` function if it's not yet installed

    ```{r}
    #| output: false

    library(tidyverse)
    library(easystats)
    library(gtsummary)
    library(ggformula)
    ```

# Exercise 1: Is *inequality* associated with *generalised trust*?

`About 30  minutes`

------------------------------------------------------------------------

For all data analysis tasks, follow a "0 + 5"-step workflow. First (**"Step 0"**, because it's not really a separate analytical step), state the research question you are attempting to answer and identify data that can be used to address the question; then (**Step 1**) describe the variables (data) that you plan to use to answer your question; then (**Step 2**), if needed, modify (wrangle) any variables that need to be adjusted to help the analysis and interpretation; then (**Step 3**) describe how your *response* variable(s) and main *explanatory* variable(s) are related. This will help with the interpretation of the statistical results and with identifying any further changes to the variables that may be needed (returning to **Step 2** again). You may also wish to check the relationship between your predictor variables if you have more than one (but we're not using multiple predictors yet); then (**Step 4**) apply the statistical model that is most appropriate to answer the research question; and finally (**Step 5**) summarise the results from your analysis using tables, figures and your own words.

## Research question and data

As a first exercise, let's reproduce the analysis presented in the lecture. The exercise explores the relationship between inequality and generalised trust at the national level in international comparison. The aim is to assess the arguments put forward by [@WilkinsonPickett2010SpiritLevelWhy] in their Chapter 4. Their measurement of *generalised trust* comes from the World Values Study (1999-2001), while for the measure of inequality they use the so-called "S80/S20" measure, which is the ratio of the average income of the 20% richest to the 20% poorest people in a country. We have similar data obtained from the latest waves of the World Values Survey and European Values Study (2017-2022) and the [World Bank](https://data.worldbank.org/indicator/SI.DST.05TH.20). To ensure that inequality data is available for as many countries as possible, I calculated the "S80/S20" measure as the average of the data available in each country between the years 2010-2022.

The dataset can be loaded into `R` directly from the module's github repository:

```{r}
inequality <- data_read("https://github.com/CGMoreh/SOC2069/raw/main/Data/for_analysis/lab3macro.rds")
```


We can check the list of variables in the dataset and their first few values with the `data_peek()` function from `{datawizard}`:

```{r}
data_peek(inequality)
```

## Describe individual variables

For summary statistics of the numeric variables in the dataset, we can use `describe_distribution()`:

```{r}
describe_distribution(inequality)
```

::: {.questionbox .question}

**Questions**

Examine the descriptive results and try to answer these questions:

-   What is the average (mean) generalised trust score of the countries in the dataset?
-   What is the average GDP per capita of the countries in the dataset?
-   What is the average value for the measure of inequality across all the countries in the data?
-   How spread out are the inequality scores? (**tip**: the *standard deviation* (SD) is a good measure of "spread", or *variation* around the *mean*)
-   What is the *minimum* and *maximum* percentage of the urban (versus rural) population in individual countries in the data? Do you think this value is correct, or might there be something wrong with the data? How would you explain this distribution? (**tip**: check the dataset itself to explore the individual data points; you can open it in a viewer window by double-clicking on the data object in the Environment, or using the `View()` function)
-   Are there any missing values (NAs) on any of the variables in the dataset?
-   How would you describe the distribution of the *population* variable?

:::


As we have practised in Lab2, we can also tabulate factors and character variables with the `data_tabulate()` function. For example:

```{r}
inequality |> data_tabulate(Region)
```
We see that the majority of the countries in the dataset are from Europe and Central Asia.

More importantly, let's explore visually the distribution of our two main variable of interest: *generalised trust* and *inequality*. 

::: {.taskbox .task}

Using the plotting functions we learnt in Lab2, create a histogram or a density plot for each of the two variables `trust_pct` and `s80s20`:

```{r}
#| echo: true
# Insert a new code chunk into your Quarto document, write your functions there and execute the code chunk to see the results




```


```{r}
#| include: false

gf_histogram( ~ trust_pct, data = inequality)
gf_density( ~ trust_pct, data = inequality)

gf_histogram( ~ s80s20, data = inequality)
gf_density( ~ s80s20, data = inequality)

```


**Question**: would you describe the two variables as "normally distributed"?

:::


## Describe the relationship between your variables

To visualise the relationship between two numeric variables we can use a *scatterplot*. Relying on the `{ggformula}` package we got to know in Lab2, we can use the `gf_point()` function. We keep in mind that our *response* variable is *generalised trust* and the *explanatory* variable is *inequality*. So it is customary to place the *response* on the *y* axis and the *explanatory* (predictor) variable on the *x* axis, in line with the general formula notation ***y*** **\~ *x***. The command is the following:

```{r}
gf_point(trust_pct ~ s80s20, data = inequality)
```

To add a linear regression line to the scatterplot we can use the `gf_lm()` function:

```{r}
gf_point(trust_pct ~ s80s20, data = inequality) |> 
  gf_lm()
```


The straight line we plotted is a *regression line* that visualises the linear model $\hat{y} = a + bx$ (see **Agresti 2018: 250-252**; **Gelman et al. 2020: 37-38, 82-95**). In Step 4 we will fit this model statistically to obtain the values for the $a$ and $b$ coefficients so we can interpret the regression line more accurately. For the moment, let's rely on our eyes.

One thing to notice immediately is how the y-axis has expanded into negative values. This is because the linear assumption of the regression line means that at high values of *inequality* very low, below-zero values of *trust* are predicted, even though that is not logically possible, given that our measurement is on a percentage scale (i.e. running between 0 and 100). 

To constrain the *y* axis to the actual values of our data, we can set limits on the scale using the `gf_lims()` function:

```{r}
gf_point(trust_pct ~ s80s20, data = inequality) |> 
  gf_lims(y = c(1, 80)) |> 
  gf_lm()
```

::: {.questionbox .question}
**Questions**

-   Does the graph show a *negative* or *positive* correlation?
-   Does the graph show a *strong* or *weak* correlation?
:::


Another addition to the chart that may come useful are value labels, in this case the names of the countries; this would allow us to identify which are the outliers. We can use the `gf_text()` function for this, with some additional specifications to set the size of the labels and their distance from the value points:

```{r}
gf_point(trust_pct ~ s80s20, data = inequality) |> 
  gf_lm() |> 
  gf_lims(y = c(1, 80)) |> 
  gf_text(label = ~ country, size = 2.5, hjust = 0, nudge_x = 0.15, nudge_y = 0.15)
```

We may wonder whether a linear function is really appropriate for our data, and we could replace the linear model line with a smoothed line that follows the shape of the data more closely:

```{r}
gf_point(trust_pct ~ s80s20, data = inequality) |> 
  gf_smooth() |> 
  gf_text(label = ~ country, size = 2.5, hjust = 0, nudge_x = 0.15)
```
In this case, while it is apparent that the relationship between *inequality* and *trust* is not completely linear, the curvature is not too extreme.

Finally, we may wonder whether there are any group effects in the data that are being missed in the bivariate plot. We could add a third variable to the plot by using colouring. Below, we use the `Region` variable a third grouping variable (notice the `~` sign when adding the colour aesthetic):


```{r}
gf_point(trust_pct ~ s80s20, data = inequality, color = ~ Region) |> 
  gf_lims(y = c(1, 80)) |> 
  gf_lm()
```

By adding a third variable to our visual model, the linear regression line also breaks down by `Region` and we can see more clearly that the effect (and indeed the direction) of inequality on trust is not the same within each regional grouping; in some regions the association is very steep and negative, while in others is flatter - or, in the case of "Middle East and North Africa" it is strong positive association.

This hints to an important limitation of correlations and simple bivariate regressions. It is often the case that third (and further) variables can have a very strong influence on the associations we observe, and we need to pay careful attention to disentangling the meaning behind our regression results.

## Fit the statistical model

So far we have only visualised the relationship between *inequality* and *trust*, but we haven't build a statistical model to obtain the coefficients describing the relationship. We already know from the scatterplots that a linear model may not be the ideal approach for our variables, but it is a very simple model that is worth starting with. In `R`, we can use the `lm()` function to run a **l**inear **m**odel.
Because we often want to have access to various components of the models we fit for further analysis, it's recommended to always save the model results to an object. Let's call out first model **m1.1**:

```{r}
m1.1 <- lm(trust_pct ~ s80s20, data = inequality)
```


::: {.codebox .code}

**Coding tip: Understand and simplify the formula**

<details>

<summary><i>Click to view</i></summary>

The `lm()` formula echoes the mathematical equation we are fitting:

\$\$

```{=tex}
\begin{aligned}
  \color{Blue}{\widehat{generalised\:trust}} &\color{Grey}{=} \color{Red}{a} \color{Grey}{+} \color{Red}{b}\color{Grey}{\times}\color{Blue}{inequality} \\
  
  \color{Blue}{trust_pct} &\color{Grey}{\sim} \color{Red}{1} \color{Grey}{+} \color{Red}{b}\color{Grey}{\times}\color{Blue}{\text{s80s20}}
\end{aligned}
```
\$\$

We have data on the blue components (our two variables) and we are searching for the corresponding coefficients for the red components. The $a$ is called the "**intercept**", referring to the point where the regression fit line we plotted earlier on the scatterplot *intercepts* the $y$ (trust) axis **when the** $x$ (inequality) axis equals **0**. The $b$ is called the *slope* (visually, it's the angle between a completely flat horizontal line and the regression fit line) and it tells us the observed difference between different countries' values of $generalised\;trust$ when their levels of $inequality$ also differ by one unit.

The *intercept* is included in the `lm()` function by default, so we don't need to explicitly write it. We also don't have to explicitly write `data =`, we can simply write just the name of the dataset. Our function call could be simplified to:

```{r eval=FALSE}
m1.1 <- lm(trust_pct ~ s80s20, inequality)
```

However, leaving out `data =` does not work for all formula-style functions, so until you become very used to writing these commands, it's better to spell out more than the strictly necessary. Also, remember that it's more important to be accurate and to make it easy for yourself and others to understand your code easily.

</details>
:::

We can see that the object "m1.1" now appears in the Environment pane, and if we expand it we see a list of different elements ("coefficients", "residuals", "effects", etc.). The "coefficients" are the most elementary and most important elements. We can print them by using the `coefficients()` function or simply `coef()`:

```{r}
coef(m1.1)
```

As noted in the *Coding tip* above, the two *coefficients* that we see here for `(Intercept)` and `s80s20` are the corresponding values for $a$ and $b$, respectively, in the $\hat{y} = a + bx$ equation. We can substitute them - rounding down to two decimal places for simplicity - to obtain the equation: $\widehat{trust} = 45.38 - 3.11\times inequality$. So what does this mean?

We said at the start that the aim of a linear regression model is to help us predict the value of *generalised trust* from that of *inequality*; i.e. what would our best guess concerning a country's population's level of *trust* be if the only information we had about that country was its level of inequality? Knowing a country's *inequality* score is already more information than ***not*** knowing anything. Without information on *inequality* our best guess would be simply the overall ***mean*** (i.e. ***average***) trust score for the whole dataset. If we remember from the descriptive statistics we've done earlier in the exercise, the **mean** of the `trust` variable was 25.73.

We can actually obtain the mean of a numeric variable also by using the `lm()` function without any predictors specified. Let's check the mean *trust* score using this method and compare it with what we got from the summary statistics earlier:

```{r}
lm(trust_pct ~ 1, data = inequality) |> coef()

```

The coefficient is exactly the same as the one we  got from the descriptive statistics we did in earlier steps. So, without knowledge of a country's *inequality* level, or any other further information, our safest guess of a national population's *trust* level would be 25.73. However, in knowledge of a country's *inequality*, the slope ($b$) coefficients we got from the regression tell us that the best guess is the *intercept* (45.38) **plus** -3.11 **times** the country's inequality score. For example, our best guess for the *generalised trust* score of a country with an inequality score of *5.4* in our dataset would be:

```{r}
45.38 + (-3.11 * 5.4)
```

More generally, the model tells us that a 1-unit difference in *inequality* is associated with a 3.11-unit difference in *generalised trust* as measured here.

::: {.importantbox .important}
Remember how we said earlier that the *Intercept* is the point where the regression fit line intercepts the $y$ axis when the $x$ axis is 0? This may sound okay visually, but note that in the case of our variables this means that the *Intercept* refers to the value of *trust* when the *inequality* variables is equal to **0**. Mathematically, there's nothing wrong with this, but in practice we don't have countries scoring "0" on *inequality* in our dataset - in fact, a S80/S20 income quintile share ratio of "0" would not make much sense at all. In other words: the interpretation of the *Intercept* value is meaningless in our case. That doesn't affect the *slope* coefficient, so our regression equation is useful for predicting from, but we should not interpret the *Intercept* as it is.

In order to make the *Intercept* more meaningful, one common approach is to **mean-centre** the predictor variable(s). In our case, we would "shift" the *inequality* scale so that the average (i.e. mean) inequality takes the value of "0". The mean `s80s20` inequality score in the dataset is 6.5, so using the mean-centred variable in the regression would make the *Intercept* refer to the value of *inequality* for the countries with an average income quintile share ratio; of course, there may not exist such a country in the dataset, but the value itself is theoretically possible within the context of our measurement.

:::

To request a more detailed summary of the model results, the base `R` function to use is `summary()`:

```{r}
summary(m1.1)
```

The `summary()` function prints out a lot of information, but it's not the best format if we wish to reuse the various statistical components for further analysis, and the presentation of the output could also be improved. The `model_parameters()` and the `model_performance()` functions from the `{parameters}` package part of `{easystats}` is a better option:

```{r}
model_parameters(m1.1) 

model_performance(m1.1)
```

We will learn more about the inferential statistics included in these model results (the *standard error*, the 95% *confidence interval*, the *t*-value and the *p*-value) later, once we are more confident with fitting and visualising regression models and interpreting the coefficients. For now, it's enough to note that they are all related statistics testing the level of confidence that we can have in the generalisability of our results given the characteristics of our sample.

The statistics obtained from the `model_performance()` function, on the other hand, describe the model overall and purport to aid with comparing the "performance" of different models. The R2 value (more precisely $R^2$, R-squared) is called the *coefficient of determination* and measures how well a linear model predicts the outcome, representing the proportion of variation in the *response* variable that is predicted by the *explanatory* variables; in our case that's 0.185 or 18.5%. That's a reasonable proportion, but not nearly close to explaining all the variation in levels of trust, highlighting that there are various other factors also at play apart from inequality, both at the national and the individual levels. 


## Standardizing the modelled variables

As mentioned earlier, *centring* our variables around the value "0" so that regardless of their measurement scale the value "0" becomes a meaningful number can be useful and help interpretation. Furthermore, if we constrain our *descriptor* variables to the same scale, then they become more easily comparable. Of course, we only have one *descriptor* variable at the moment, but that's always just an initial phase in a real analysis project. The way we can constrain variables to a comparable scale is through *standardization*. A standardized variable (sometimes called a *z-score* or a standard score) is a variable that has been rescaled to have a mean of zero and a standard deviation of one. 

To standardise a numeric variable, we can use the `standardize()` function. Let's create a new standardised version of the `s80s20` variable and save it as a new variable in the dataset:

```{r}
inequality <- inequality |> 
  data_modify(s80s20_std = standardize(s80s20))
```

We can plot the new variable:

```{r}
describe_distribution(inequality, s80s20_std)

gf_histogram( ~ s80s20_std, data = inequality)
```
Wee see that the *mean* of the new variable is very close to 0 (**9.13e-17** is just scientific notation for a very small and therefore very long number; it tells us to move the decimal point 17 places to the left to obtain the number), and the standard deviation is precisely 1. 

If we were to refit the regression model with this variable, we would get:

```{r}
lm(trust_pct ~ s80s20_std, data = inequality) |> 
  model_parameters()
```
In this case, we find that the average value of *trust* for a country with an *average* (0) level of *inequality* is 25.13 (Intercept), while countries with a 1 standard deviation higher inequality than the average are expected to have a *trust* score that is 8.09 units (percentage points) lower than the average. Although the units of measurement have changed, the actual proportions of the effects we observe have not changed. We have made the intercept value meaningful, but the units by which we measure *inequality* are no longer the same. A *standard deviation* is a much larger unit than the units of the original scale, so a 1-unit change in inequality now translates into a larger effect.

We could also standardize the *response* variable in the same way and refit the model with all the modelled variables in standardized format:

```{r}
inequality <- inequality |> 
  data_modify(trust_std = standardize(trust_pct))

m1.2 <- lm(trust_std ~ s80s20_std, data = inequality)

model_parameters(m1.2)

```
The interesting thing about this output is that the coefficient we get for `s80s20_std` is the same (within rounding error) as the *correlation coefficient* (r) between `s80s20` and `trust_pct`. To check the correlation between two numeric variables, we can either use the base-`R` function `cor.test()`:

```{r}
cor.test(inequality$trust_pct, inequality$s80s20)
```
Or the `cor_test()` function included in `{easystats}`:

```{r}
cor_test(inequality, "s80s20", "trust_pct")
```
The `{easystats}` ecosystem also has a convenience `plot()` function that creates quick plots from various summary functions, including `cor_test()`:

```{r}
plot(cor_test(inequality, "s80s20", "trust_pct"))
```

A correlation is nothing more (or less) than a bivariate regression with both variables standardized. Its coefficient is a measure of the strength and direction of the association between two numerical variables. In this case, we observe a negative correlation between *inequality* and *trust* of medium strength.

From the regression model we obtained a related statistics, the *coefficient of determination* ($R^2$), which is the square of the correlation coefficient ($-0.4304^2 = $`r (-0.430389)^2`).


# Exercise 2: Predicting *trust* from *inequality* and *urbanization*, while accounting for *region* effects

In this exercise we extend the model we fit earlier by introducing two new variables: `urban_pop_pct` (the percentage of the urban population), and `Region`, the world geographical region to which the country belongs, which, as we saw earlier in the scatter-plot, complicates the simple correlation between *inequality* and *trust*.


::: {.taskbox .task}

## On your own: Perform the appropriate descriptive statistics for the two new variables

```{r}
# Univariate descriptive statistics





```


## Plot the relationship between *urbanisation* and *trust*, and between *urbanisation* and *inequality*


```{r}
# Scatterplots





```

:::


## Plot the relationship between *Region* and the other numerical variables


For this task, we need to introduce a new plot type: the *boxplot*. Box-plots are useful for visualising the relationship between a *categorical* and a *numeric* variable. They describe the distribution of the numeric variables in each category of the categorical variable:

```{r}
gf_boxplot(trust_pct ~ Region, data = inequality)
```
To avoid the overlapping labels o the *x* axis, we can add some further specifications:

```{r}

gf_boxplot(trust_pct ~ Region, data = inequality) +
  scale_x_discrete(guide = guide_axis(n.dodge=3))
```
Box-plots contain a lot of useful summary information about variables, and the interpretation of the shapes is the following:

![box-plot](sheet_pics/1_boxplots.jpg)


::: {.taskbox .task}

**Create box-plots for the association between *region* and the other numeric variables**

```{r}
# Box plots


```


:::

To get the precise numeric values of the summary statistics captured in the box-plot, we can make a summary table using the `means_by_group()` function from the `{easystats}`:

```{r}
means_by_group(inequality, "trust_pct", "Region")
```

What the function provides is effectively a type of regression results, as the statistics at the bottom of the table allude. Specifically, besides the mean value of trust in countries within each geographical grouping, we see statistics about the generalisability of the distributions and of model fit. The model that is being summarised is an Anova model (or *analysis of variance*) and its results are very similar to what we would obtain from a linear regression using the `lm()` function:

```{r}
m1.3 <- lm(trust_pct ~ Region , data = inequality)

model_parameters(m1.3)
```
Look at the regression results closely and compare it with the results from the Anova model in th previous table. 

The logic of the linear regression model is different. As we know from the previous exercise, the coefficients represent a comparison between different values or levels of the *explanatory* variable. In the previous exercise, the comparison was to *a unit difference* in the level of *inequality*; here, `R` has recognised that `Region` is  a categorical variable and has broken it down into a series of *indicator*/*dummy*/*binary* variables, one for each category of the region variable. Then, these individual *binary* indicator variables were intered into the regression model, effectively turning it into a multiple regression model with six predictors. The first category of the *Region* variable ("East Asia & Pacific") appears to be missing. However, if we look closely, we notice that the mean value of "East Asia & Pacific" that we got in the previous summary table and the value of the *Intercept* in the regression model are the same. That's because in the linear regression model results the "East Asia & Pacific" category was "absorbed" into the intercept - it became the "reference category" to which all the other categories compare.
As in the models in the previous exercise, the *Intercept* is purely the *average* value of the *response* variable `trust_pct` when the value of the *explanatory* variables(s) is **0**. In our case, if the values of each of the six regions listed in the model is equal to **0** (i.e. the country does *not* belong to them), then the *Intercept* shows the average *trust* value of the only remaining region ("East Asia & Pacific").
The other coefficients in the table are just the difference between the average *trust* in the reference category ("East Asia & Pacific") and the other region.



```{r}
m1.3 <- lm(trust_pct ~ Region + 0 , data = inequality)

model_parameters(m1.3)
```





















# Exercise 1: Predicting *subjective wellbeing* from *age*

`About 60  minutes`

------------------------------------------------------------------------



## Step 0: Formulate your **"analysis research question"**

The "research question" in this context has a narrow meaning. It doesn't refer to a broad research question like the one you would come up with for a dissertation project, and which may require a combination of different methods and data, but a much more focused - smaller - question.

For example, in this exercise we want to model the relationship between a psychological concept called "subjective wellbeing" and "age". A simple analysis "research question" then is: "How does age affect subjective wellbeing?" The data that we can use to answer this quesiton is a matter for *Step 1*.

::: {.notebox .note}
**Note**

*Subjective wellbeing* has a large literature. Read [here about how the concept is commonly defined and measured](http://positivepsychology.org.uk/subjective-well-being/). Or check out this [gov.uk](https://www.gov.uk/government/publications/sources-of-wellbeing-data) page on how the concept has been used in UK public policy.
:::

## Step 1: **Find, describe and understand** your variables

In our case, we don't need to find a dataset, because in this module we are only using data from Wave 8 (2016-2017) of the [UK Household Longitudinal Study (UKHLS/Understanding Society) Main Survey](https://www.understandingsociety.ac.uk/documentation/mainstage). You have already loaded that dataset into your RStudio Environment in *Exercise 0* above. You can check the Environment pane in RStudio to make sure that the data is there (recall, we gave it the name "ukhls").

But within our dataset, we need to identify some variables that are appropriate for answering the research question. We know from *Lab6* that we can check the list of variables in our dataset on this page: [https://cgmoreh.github.io/SOC2069/Data/ukhls_w8](https://cgmoreh.github.io/SOC2069/Data/ukhls_w8){target="_blank"}. (**Tip**: you can search for keywords in the usual way: `ctrl+F` on Windows / `command+F` on Mac, and search for "wellbeing"). We can find in our dataset a variable labelled "Subjective wellbeing (GHQ): Likert" and named "**scghq1_dv**". We also have a variable named "**age_dv**" which codes respondents' age as a numeric variable.

We are already familiar with the **age_dv** variable from Lab6 Exercise 3. There, we already looked at basic descriptive statistics and a histogram to *describe* this variable. We should do it again here to remind ourselves. But the **scghq1_dv** is new to us. Let's run the usual descriptive statistics we've learnt in Lab6 to describe our variables.

::: {.taskbox .task}
### Task 1: Describe **scghq1_dv** and **age_dv** using summary statistics and histograms.

Write the required commands in the `Lab7.R` script file you created in Exercise 0 and run the commands in the script to get the results. You can check your `Lab6.R` script file for the commands to use. **Tip**: remember that we called our data object in RStudio as "ukhls" today, not "data" as we did in Lab6.
:::

::: {.codebox .code}
**Coding tip**

<details>

<summary><i>Click to view</i></summary>


</details>
:::



If you've managed Task 1, you should have produced the outputs below (or similar)(**Tip**: the command code is also shown so you can check against your own and copy it if you want):




In the TW11 lecture presentation you've also heard about another useful plot type that can be used to summarise numeric variables: the *boxplot*. We can make boxplots with the `bwplot` function from the `{mosaic}` package:

```{r}
bwplot( ~ scghq1_dv, data = ukhls)
```




## Step 2: **Modify** your variables (if needed)

If we decide that some variables would benefit from modifying in any way, we can do that. In our case, let's recode the values on the **scghq1_dv** variable so that *higher* values would equate *higher* subjective wellbeing.


In the above code, we first assign the dataset **ukhls** to an object of the same name (i.e. we are **overwriting** the dataset); then the create a new variable in the dataset called **wellbeing**, and we specify that the values of this variable should be calculated by subtracting the values of the existing **scghq1_dv** from the number **36**.

::: {.codebox .code}
**Coding tip: `max()` and `na.rm = TRUE`**

<details>

<summary><i>Click to view</i></summary>

The above code works well, but in order to make our commands more resilient and less error-prone, instead of typing by hand the *maximum* value of the scale, we could use a function to extract it from the data. There is a function for this purpose in both base `R` and the `{mosaic}` package and in both cases it's called `max()`. They both do the same thing, the only difference being that the base `R` function requires using the `$` selector to refer to a variable within a dataset (e.g. `ukhls**$**scghq1_dv`) whereas the `{mosaic}` function allows the formuala-style notation (e.g. `~ scghq1_dv, data = ukhls` ). So the commands below will produce equivalent results:

```{r}
base::max(ukhls$scghq1_dv)
mosaic::max( ~ scghq1_dv, data = ukhls)
```

While indeed the same, the result is not what we would expect. We know that the maximum value of the **scghq1_dv** scale is **36**, but here we are getting `NA`. Why? And how to fix it?

When this happens, we should make sure to exclude all the missing values first by adding the additional argument `na.rm = TRUE` to the command. The argument **r**e**m**oves **na** values (i.e. missing responses) in a variety of different functions, so it's good to know about. It's very useful because many functions fail if there are missing values in the data. In our case, without specifying `na.rm = TRUE` the `max` function takes values coded as `NA` to be above the highest numeric values and prints those as the *maximum* value in the variable. If we exclude the `NA` (i.e. missing) values, then we get the expected *maximum* value of the `scghq1_dv` variable:

```{r}
max(ukhls$scghq1_dv, na.rm = TRUE)
mosaic::max( ~ scghq1_dv, data = ukhls, na.rm = TRUE)
```

We can now rewrite the command creating the new `wellbeing` variable using the `max()` function to first extract the highest value to an object (let's call it "maxvalue" and print it to the console to check if it's correct) and use that as input in the `mutate()` command:

```{r}
maxvalue <- max(ukhls$scghq1_dv, na.rm = TRUE)    # extract the highest value

print(maxvalue)                                   # print `maxvalue` to the console to check it

ukhls <- ukhls %>% 
  mutate(wellbeing = maxvalue-scghq1_dv)          # use `maxvalue` instead of 36
```

</details>
:::

We should compare the `scghq1_dv` and `wellbeing` variables now to make sure we mutated it correctly:

```{r results='hold'}
favstats( ~ scghq1_dv, data = ukhls)
favstats( ~ wellbeing, data = ukhls)
```

::: {.codebox .code}
**Coding tip: `select()` and `summary()`**

<details>

<summary><i>Click to view</i></summary>

The `summary()` function allows for entire datasets to be summarised (i.e. all the columns/variables in the dataset). So we could select the variables we are interested in using the `select()` function from `{tidyverse}` and then do a `summary()` on this selected data. This has the advantage of allowing a pipeline workflow and the summary statistics are printed in columns, which may be easier to compare. The disadvantage is that we only have the descriptive statistics offered by `summary()` (e.f. we are missing the *standard deviation* (sd) that `favstats()` prints by default)

```{r}
ukhls %>% 
  select(scghq1_dv, wellbeing) %>% 
  summary()
```

</details>
:::

Does it look okay? We see that the *minimum*, *maximum* and *standard deviation* values haven't changed, which is what we would expect. The other values have changed as we would expect as well, given thet the scale was reversed (i.e. mirrored).





## Step 3: **Describe** the relationship between your variables

```{r include=FALSE}
##### Step 3: Bivariate relationships ----------
```

We can now start to explore the relationship between the two variables. The best plot type to represent the relationship between two numeric variables is a **scatterplot**. We will use the `xyplot()` function from the `{mosaic}` package for this. 




```{r}
xyplot(wellbeing ~ age_dv, data = ukhls)
```

The figure doesn't look too informative, but that's because we have too many cases (respondents) in the dataset - represented by the dots in this plot - and the correlation between the two variables doesn't seem to be very strong.

One addition to the plot that we can make is to add a *fit line* that expresses the linear relationship between the variables. We do this by adding the optional argument `type = c("p", "r")` to the function (here we are *c*ombining (using the `c()` function) two different plot types into one: a ***p**oint* plot and a ***r**egression line* plot):

```{r}
xyplot(wellbeing ~ age_dv, type = c("p", "r"), data = ukhls)
```








If we are interested in obtaining the Pearson correlation coefficient value (see **Agresti 2018: 260-263**) to assess the *direction* and *strength* of the correlation we observe on the plot, we can use the `cor.test()` function from `{mosaic}`:

```{r}
cor.test(wellbeing ~ age_dv, data = ukhls)
```

The last statistic ("cor") is the correlation coefficient *r*: 0.036. Knowing this figure, how would you answer the questions above? (**tip**: have a look in **Agresti 2018: 260-263** for the interpretation of the correlation coefficient).





## Step 4: **Model** the relationship between your variables





## Back to Step 2: *centring* the predictor variable

```{r include=FALSE}
##### Back to Step 2: Centring the predictor ----------
```

::: {.taskbox .task}
Following the example in Step 2, mutate the `age_dv` variable into a new variable called `age_median_centred` that is equal to the `age_dv` variable minus its median. **Tip**: you can get the median value of a variable using the `median()` function. Here's some code scaffolding to get you going:

```{r include=FALSE}
#### Student task ----------
```

```{r, eval=FALSE, purl=FALSE}
### Complete the code below

# First, extract the median age value from the data using the `median()` function:

medianAge <- median( ??? )

# Then `print` the median to check it and see if it's correct

???

# Then use the `mutate` function to create the new variables called "age_median_centred" by subtracting the median age from all values of `age_dv`

???
```

```{r include=F}

# First, extract the median age value from the data using the `median()` function:

medianAge <- median( ~ age_dv, data = ukhls, na.rm = TRUE)

# Then `print` the median to check it and see if it's correct

medianAge

# Then use the `mutate` function to create the new variables called "age_median_centred" by subtracting the median age from all values of `age_dv`

ukhls <- ukhls %>% 
  mutate(age_median_centred = age_dv - medianAge)
```

If you've done it correctly, the new variable should look like this:

```{r}
favstats( ~ age_median_centred, data = ukhls)
```

Now we can refit the model with the new centred *age* variable. Let's save the model results with the ingenious name `model2` and print the *coefficients* as we have done with *model 1*:

```{r eval=FALSE, purl=FALSE}
### Complete the code below

# Complete the lm() command

model2 <- ????
  
# Then print the coefficients from model2
  
  
```

If you've done it well, these are the coefficients that you should expect:

```{r echo=FALSE}
model2 <- lm(wellbeing ~ age_median_centred, ukhls)

coef(model2)
```
:::

::: {.questionbox .question}
**Questions**

-   How has the coefficient for the *Intercept* changed using the median-centred age variable?
-   How has the coefficient for the *age* variable change?
-   What is the expected *subjective wellbeing* score of someone aged 49?
-   What is the expected *subjective wellbeing* score of someone who has a value of -13 on the `age_median_centred` variable?
-   What is the real age in years of someone whose value on the `age_median_centred` variable is -29?
:::

## Step 5: **Present** and **interpret** your findings

```{r include=FALSE}
##### Step 5 ----------
```

The *coefficients* are only one part of the model output that we need to interpret. We can get a more comprehensive summary of the model using the same `summary()` function we are already familiar with:

```{r}
summary(model2)
```

There is a lot of information on this output. The "Call" block simply prints the function we called. The "Residuals" block shows descriptive statistics for the *residuals*, which are the distances between each individual score and the straight regression line (see the scatter-plot). The bottom block provides some statistics for the overall model - this is more important for more complex models where we have more than one predictor.

For instance, the **R-squared** value is a measure of overall model fit. It explains how much of the *variance* in the outcome variable can be explained by the predictor variable(s). In theory, the R-squared values can range from 0 to 1. An R-squared value of 0 means that the predictor variable(s) do not explain any (0%) of the variance of the outcome variable, and a value of 1 signifies that the independent variable(s) explain all (100%) the variance in the dependent variable. In our example, the R-squared value of 0.001 implies that the predictor variable (*age*) explains 0.1% (0.001 x 100) of the variance in the outcome variable (*subjective wellbeing*). So not a lot. Variation in *subjective wellbeing* is determined by various other factors. If we develop this basic model by including some other variables that we think potentially affect *subjective wellbeing*, we expect to see the R-squared statistic increase.

::: {.questionbox .question}
**Question**

Do you remember the *correlation coefficient* (Pearson's *r*) we computed in Step 3? What do you think the relationship is between that correlation coefficient (r) and the r-squared value? **Tip**: check the results below:

```{r}
summary(model2) %>% rsquared %>% sqrt()

cor.test(wellbeing ~ age_median_centred, data = ukhls)$estimate
```
:::

The adjusted R-squared can help us compare different models because it is a standardisation of the R-squared based on the number of cases and independent variables in the model. The best fitting model is always the model with the highest possible adjusted R-squared (not the model with the highest R-squared). But we won't be comparing models in this way on this module.

We can find the *coefficients* in the third block, where the *point estimates* we examined thus far are shown under the column titled "Estimate". The other three columns ("Std. Error", "t value" and "Pr(\>\|t\|)") refer to inferential statistics that allow us to assess the degree to which the associations we observe in our data can be taken to represent *real* associations in the wider population from which our representative sample was drawn (i.e. the UK population in this case). The most commonly cited statistic among these is the **p-value** (under the column "Pr(\>\|t\|)", i.e. the "probability" of the t-value under the null hypothesis). In the case of our model, because we only have one predictor variable, the *p-value* associated with the *age* predictor is the same as the *p_value* for the entire model shown at the very bottom of the output.

::: {.importantbox .important}
If the values are too small or two large, then `R` will display them in *scientific notation*. The *p-values* are often very small, with many decimals. In scientific notation, the letter *e* is used to mean "10 to the power of". The scientific notation tells us to move the decimal point to the left (if there's a minus sign after *e*) or to the right (if there's a plus sign after *e*) by the number of spaces shown after the -/+ sign. We don't need to know precise numbers here, so a quick glance at the number in scientific notation can tell us whether the value is really small or really big. In respect to *p-values*, we are interested in how close they are to 0 to about three decimals, so if we see something ending in e-4 or greater then we know that the value is very small.

You will eventually get used to scientific notation, but if you find it very frustrating, you can turn it off in `R` by running the following command in your script:

```{r eval=F}
options(scipen = 999)
```

Now, however, you'll need to bear with very long fractional numbers. We find out, for example, that the p-value for the *age* coefficient is 0.0000000000055. The three stars next to it are explained under the bloc in the line called "Signif. codes": values marked with \*\*\* are lower than 0.001, those marked with \*\* are loser than 0.01, and those marked with \* are under 0.05.
:::

As conclusion, try to write down your interpretation of the results. Check closely what each statistic means, guided by **Agresti (2018: 263-277)**.

Think about whether the results are *statistically significant* and what that means. Read and reflect on **Gelman et al. (2020: 57-63)** in particular for this.

# Exercise 2: On your own

------------------------------------------------------------------------

::: {.taskbox .task}
**Task**

Find some other numeric variables whose relationship can be modelled using a linear regression and undertake a complete analysis like the one in Exercise 1.

There aren't too many useful numeric variables in the UKHLS dataset, so you'll have to work with what you have to come up with interesting "research questions" to answer.

This is also what you'll be required to do in Assignment 2, so your time and effort will not be wasted!
:::

```{r eval=FALSE, include=F, purl=FALSE}
knitr::purl("Lab7.Rmd", "Lab7.R", documentation=0)
```
