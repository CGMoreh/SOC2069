---
title: "Worksheet 7"
output:
  html_document:
    theme: lumen
    toc: yes
    toc_float: yes
    toc_highlight: no
    tabset: yes
    number_sections: no
    anchor_sections: no
    self_contained: yes
    code_download: no
    css: ./../my_themes/labcss.css
---

```{r, include=FALSE}
### ---- Lab 7 script ---- ###
```

```{r setup, include = FALSE, purl=FALSE}
# knitr::knit_hooks$set(purl = knitr::hook_purl)

knitr::opts_chunk$set(error = FALSE, warning=FALSE, comment="", prompt=FALSE, message = F, strip.white = F, fig.width=6, fig.asp = 0.618, fig.show = 'hold', documentation = 0)
```

```{r klippy, echo=FALSE, include=TRUE, purl=FALSE}
klippy::klippy(position = c('right'))
```

------------------------------------------------------------------------

    Timetable week: 12
    Topic: "Modelling numeric outcomes 1: single predictors"

------------------------------------------------------------------------

# Intro

One of the most commonly encountered statistical methods in social science publications is **linear regression**. This week we will make some first steps towards understanding what this method involves and applying it in practice using the `R` programming language and data from Wave 8 (2016-2017) of the [UK Household Longitudinal Study (UKHLS/Understanding Society) Main Survey](https://www.understandingsociety.ac.uk/documentation/mainstage).

We'll focus on the simple (*but not trivial!* - as Gelman et al. (2020: 93) admonish) case where we model the relationship between two ***numeric*** (***continuous***) variables using a ***simple linear regression model***. The aim of a **simple linear regression model** is to predict the expected values of one ***continuous*** (***numeric***) variable (e.g. a child's *age)* from another ***continuous*** (***numeric***) variable (e.g. the child's *height)*. The variable we want to predict is usually called the *outcome* (or *dependent* variable) and the variable used for predicting this outcome is referred to as the *predictor* (or *independent* variable).

In actual applied sociological research the *simple linear regression* model is rarely used on its own because we understand that in the social world there are always many factors that influence an outcome at the same time - even in the case of modelling a child's *height*, we know that *age* is an important factor, but there is some considerable variation in height even among children of a similar age, so there must be other factors at work too. We also understand that assuming a strict ***linear*** relationship between two variables is too much of an oversimplification (staying with the *height \~ age* example, while the linear assumption may be realistic when applied to children, it is definitely not applicable if we extend the analysis to all ages: an increase in age is associated with an increase in height for children, but once the age of maturity is reached, we no longer get taller as we get older - in fact, in old age, as we get older we tend to get shorter!).

But the simple linear regression model contains many of the statistical components on which other statistical tests and more complex models are built, so understanding it is essential. We will also learn a few concepts and methods related to simple linear regression - such as *correlation* (also known as the *Pearson correlation,* after the mathematician [Karl Pearson](https://en.wikipedia.org/wiki/Karl_Pearson "Wikipedia")) - and we'll make some steps towards expanding the linear model to include non-continuous predictors. The main `R` function that we will learn is `lm()` (check `help(lm)` for technical information).

# Readings

Core readings:

-   Gelman et al. (2020): Chapters 4, 6 and 7 (pp. 49--67; 81--101)

Secondary readings:

-   Agresti (2018): Chapter 9 (pp. 247--277)
-   Byrne (2002): Chapter 7 (pp. 112--129)

Further readings:

-   Gelman et al. (2020): Chapters 8 and 9 (pp. 103--128)

`r fontawesome::fa("save")` [**Download the module reading pack from here (.zip 47MB)**](https://github.com/CGMoreh/SOC2069/raw/main/readings/soc2069-reading-pack.zip)

# Exercise 0: Setup

1.  **Open the `R Studio` interface** by clicking on the *SOC2069-Statistical-analysis**.Rproj*** file included in the **_SOC2069-Statistical-analysis_** project folder that you downloaded from Canvas in Lab6. The folder should be stored on your Newcastle University **OneDrive** and accessible from any computer

    `r fontawesome::fa("wrench")`   If you haven't yet downloaded the project folder in TW11 (Lab6), then [download it from Canvas](https://ncl.instructure.com/courses/48074/pages/data-access-and-description?module_item_id=2544656). **Tip:** If you haven't completed the Lab6 worksheet, it will be difficult to follow the next steps; please go through *Exercise 1* and *2* of [Lab6](https://cgmoreh.github.io/SOC2069/Worksheets/Lab6)).

2.  **Create a new blank `R` script** for this lab session and call it *Lab7.R*

    `r fontawesome::fa("wrench")` Check *Exercise 1, Task 2* of [*Lab6*](https://cgmoreh.github.io/SOC2069/Worksheets/Lab6)) if you need help with this. **Tip:**
```{r, echo=F, purl=FALSE}
#| out.width="40%",  fig.topcaption = TRUE, fig.align = "center", 

knitr::include_graphics("images/new_script_1.jpg")
```
<center>or</center> 
```{r, echo=F, purl=FALSE}
#| out.width="40%",  fig.topcaption = TRUE, fig.align = "center", 

knitr::include_graphics("images/new_script_2.jpg")
```


3. **Write a comment line** at the top of the new script to briefly detail what the script is about (e.g. *\# Script for Lab x, Week y*). **Tip:** in `R` scripts the hashtag (\#) denotes that the text following it on the same line is just a comment, not a programming command.


3.  **Load user-written `R` packages** we commonly use with the `library()` function

    `r fontawesome::fa("wrench")`   Check *Exercise 1, Task 4* of [*Lab6*](https://cgmoreh.github.io/SOC2069/Worksheets/Lab6) if you need help with this. **Tip:** You may need to first install the package with the `install.packages()` function if it's not yet installed
```{r include=FALSE}
##### Load packages ----------
```

    ```{r}
# install.packages(c("tidyverse", "mosaic", "sjmisc")) 
# If you need to first install the packages, remove the hashtag from in front of the `install.packages()` command to un-comment the command and make it executable in R
    
    library(tidyverse)
    library(mosaic)
    library(sjmisc)
    ```

4.  **Load the *ukhls_w8.rds* dataset** to an object in the session Environment; let's call that object "ukhls" this time

    `r fontawesome::fa("wrench")`   Check *Exercise 1, Task 4* of [*Lab6*](https://cgmoreh.github.io/SOC2069/Worksheets/Lab6)) if you need help with this. **Tip:** Specifying only the file name without a path assumes that you have followed Step 1 above correctly and your working directory is the RProject. If not, go back to Step 1.

```{r include=FALSE}
##### Read in data ----------
```
    ```{r eval=FALSE}
    ukhls <- readRDS("ukhls_w8.rds")
    ```

```{r include=FALSE, purl=FALSE}
ukhls <- readRDS(url("https://cgmoreh.github.io/SOC2069/SOC2069-Statistical-analysis/ukhls_w8.rds"))
```

# Exercise 1: Predicting *subjective wellbeing* from *age*

`About 60  minutes`

------------------------------------------------------------------------

For all data analysis tasks, we will follow a "0 + 5"-step workflow. First (**"Step 0"**, because it's not really a separate analytical step), state the research question you are attempting to answer; then (**Step 1**) find and understand variables (data) that help you answer that question; then (**Step 2**) modify (wrangle) any variables that need to be adjusted to help the analysis and interpretation; then (**Step 3**) describe how your *outcome* variables and main *predictor* variable(s) are related. This will help with the interpretation of the statistical results and with identifying any further changes to the variables that may be needed (returning to Step 2 again). You may also wish to check the relationship between your predictor variables if you have more than one (but we're not using multiple predictors yet); then (**Step 4**) apply the statistical model that is most appropriate to answer the "research question"; and finally (**Step 5**) summarise the results from your analysis using tables, figures and your own words. In each exercise, we'll go step-by-step.


## Step 0: Formulate your **"analysis research question"**

The "research question" in this context has a narrow meaning. It doesn't refer to a broad research question like the one you would come up with for a dissertation project, and which may require a combinaiton of different methods and data, but a much more focused - smaller - question.

For example, in this exercise we want to model the relationship between a psychological concept called "subjective wellbeing" and "age". A simple analysis "research question" then is: "How does age affect subjective wellbeing?" The data that we can use to answer this quesiton is a matter for *Step 1*.

:::: {.notebox .note}
**Note**

*Subjective wellbeing* has a large literature. Read [here about how the concept is commonly defined and measured](http://positivepsychology.org.uk/subjective-well-being/). Or check out this [gov.uk](https://www.gov.uk/government/publications/sources-of-wellbeing-data) page on how the concept has been used in UK public policy.
::::


## Step 1: **Find, describe and understand** your variables

In our case, we don't need to find a dataset, because in this module we are only using data from Wave 8 (2016-2017) of the [UK Household Longitudinal Study (UKHLS/Understanding Society) Main Survey](https://www.understandingsociety.ac.uk/documentation/mainstage). You have already loaded that dataset into your RStudio Environment in *Exercise 0* above. You can check the Environment pane in RStudio to make sure that the data is there (recall, we gave it the name "ukhls").

But within our dataset, we need to identify some variables that are appropriate for answering the research question. We know from *Lab6* that we can check the list of variables in our dataset on this page: [https://cgmoreh.github.io/SOC2069/Data/ukhls_w8](https://cgmoreh.github.io/SOC2069/Data/ukhls_w8){target="_blank"}. (**Tip**: you can search for keywords in the usual way: `ctrl+F` on Windows / `command+F` on Mac, and search for "wellbeing"). We can find in our dataset a variable labelled "Subjective wellbeing (GHQ): Likert" and named "**scghq1_dv**". We also have a variable named "**age_dv**" which codes respondents' age as a numeric variable.

We are already familiar with the **age_dv** variable from Lab6 Exercise 3. There, we already looked at basic descriptive statistics and a histogram to *describe* this variable. We should do it again here to remind ourselves. But the **scghq1_dv** is new to us. Let's run the usual descriptive statistics we've learnt in Lab6 to describe our variables.

:::: {.taskbox .task}
### Task 1: Describe **scghq1_dv** and **age_dv** using summary statistics and histograms. 

Write the required commands in the `Lab7.R` script file you created in Exercise 0 and run the commands in the script to get the results.
You can check your `Lab6.R` script file for the commands to use.
**Tip**: remember that we called our data object in RStudio as "ukhls" today, not "data" as we did in Lab6.
::::

:::: {.codebox .code}
**Coding tip**
<details>
  <summary><i>Click to view</i></summary>

In Lab6 we used the base `R` function `summary()` and the `descr()` function from the `{sjmisc}` package to obtain descriptive statistics. We saw that `sjmisc::descr()` allows the so-called 'piped' workflow by which we first state the *data* we are using and the then 'pipe' (` %>% `) it forward to a function. This can be useful as a way to avoid using the `$` operator to select a variable/column of a dataset and instead refer only to the variable/column in the function call; the more substantial benefits are that in the piped workflow we can then do additional procedures on the same data.

There is another workflow option that we can use, which uses a 'formula interface' of the kind we saw in the `histogram()` function from the `{mosaic}` package. This has the advantage that we can memorize the generic form **goal(y ~ x, data = my_data)** that is used in most of the modelling functions in `R` (such as the `lm()` function for **l**inear **m**odelling that we will use later).

The `favstats()` function from the `{mosaic}` package (we already loaded in Exercise 0) uses this form and provides similar descriptive statistics to `summary()`. To use it for describing a hypothetical numeric variable called `x` from a hypothetical dataset called `my_data` we would write:

```{r eval=F, purl=FALSE}
mosaic::favstats( ~ x, data = my_data)
```

</details>
::::

If you've managed Task 1, you should have produced the outputs below (or similar)(**Tip**: the command code is also shown so you can check against your own and copy it if you want):

```{r include=FALSE}
##### Descriptive stats ----------
```
```{r echo=TRUE, fig.show='asis'}
favstats( ~ age_dv, data = ukhls)
histogram( ~ age_dv, fit = "normal", data = ukhls)
favstats( ~ scghq1_dv, data = ukhls)
histogram( ~ scghq1_dv, fit = "normal", data = ukhls)
```

In the TW11 lecture presentation you've also heard about another useful plot type that can be used to summarise numeric variables: the *boxplot*. We can make boxplots with the `bwplot` function from the `{mosaic}` package:

```{r}
bwplot( ~ scghq1_dv, data = ukhls)
```

::: {.questionbox .question}
**Questions**

Examine all the descriptive results from the tables and charts you've produced and try to answer these questions:

- What is the average (mean) subjective wellbeing score of the respondents in the dataset?
- What about the median (md) subjective wellbeing score?
- How spread out are the subjective wellbeing scores of the respondents? (tip: the *standard deviation* (sd) is a good measure of "spread", or *variation* around the *mean*)
- What is the *minimum* and *maximum* subjective wellbeing score of the respondents? 
- Are there any missing values (NAs) on this variable? (i.e. What is the number of respondents for whom we don't have information on this variable for whatever reason?)
- Are there any extreme *outliers* on the subjective wellbeing variable? (i.e. Respondents whose score is either too low or too high. **Tip**: This can be seen most obviously on the boxplot; check the TW11 presentation slides for how to interpret the elements of the boxplot)
- **Finally, what exactly does the *scghq1_dv* variable measure?**
:::

The last question is tricky, because the dataset doesn't contain much useful information about the variable; all we know from the label is that it measures `Subjective wellbeing (GHQ): Likert`, but that's not enough information to accurately understand the variable. It's best if we check the variable directly on the UKHLS/Understanding Society website: [https://www.understandingsociety.ac.uk/documentation/mainstage/dataset-documentation/variable/**scghq1_dv**](https://www.understandingsociety.ac.uk/documentation/mainstage/dataset-documentation/variable/scghq1_dv){target="_blank"}. There, we find the following description about the variable:

> This measure converts valid answers to 12 questions of the General Health Questionnaire (GHQ) to a single scale by recoding so that the scale for individual variables runs from 0 to 3 instead of 1 to 4, and then summing, **giving a scale running from 0 (the least distressed) to 36 (the most distressed)**. See Cox, B.D et al, The Health and Lifestyle Survey. (London: Health Promotion Research Trust, 1987).

This is very useful. We know understand where the 0-36 scale (min-max values) we saw in the descriptive statistics comes from and, crucially, that the higher up one scores on this scale the **more distressed** that person is; in other words: higher scores on the **scghq1_dv** variable means lower levels of *subjective wellbeing*.

As long as we understand what the variable measure and how it measures it, we can used it in our analyses. However, it may make the interpretation of the results logically easier if we modified the **scghq1_dv** variable so that *higher* values mean *higher* subjective wellbeing. This is just to avoid any confusion in our minds about what the scale is telling us.

## Step 2: **Modify** your variables (if needed)

If we decide that some variables would benefit from modifying in any way, we can do that. In our case, let's recode the values on the **scghq1_dv** variable so that *higher* values would equate *higher* subjective wellbeing.

There are various ways to do that in `R`, but given that our variable is numeric and we don't need to worry about value labels, we can easily do it manually. We have all the information we need to do this. If the scale we want to inverse runs from `0` to `36`, then if we subtract each score from the maximum value on that scale (i.e. **36**), then we will have an inversed scale (because `36-0=36`, `36-1=35`, `36-36=0` and so on). It's also important not to overwrite the original variable in the dataset, in case we make a mistake; so we will create a new variable and give it an easy name to work with (**but make sure that the name doesn't already exist in the dataset, otherwise you are overwriting that variable!**). The `mutate` function (from `{tidyverse}`) makes it easy to create, modify and delete variables from a dataset. The code could be the following:
```{r include=FALSE}
##### Data transformation ----------
```
```{r}
ukhls <- ukhls %>% 
  mutate(wellbeing = 36-scghq1_dv)
```

In the above code, we first assign the dataset **ukhls** to an object of the same name (i.e. we are **overwriting** the dataset); then the create a new variable in the dataset called **wellbeing**, and we specify that the values of this variable should be calculated by subtracting the values of the existing **scghq1_dv** from the number **36**.

::: {.codebox .code}
**Coding tip: `max()` and `na.rm = TRUE`**
<details>
  <summary><i>Click to view</i></summary>

The above code works well, but in order to make our commands more resilient and less error-prone, instead of typing by hand the *maximum* value of the scale, we could use a function to extract it from the data. There is a function for this purpose in both base `R` and the `{mosaic}` package and in both cases it's called `max()`. They both do the same thing, the only difference being that the base `R` function requires using the `$` selector to refer to a variable within a dataset (e.g. `ukhls**$**scghq1_dv`) whereas the `{mosaic}` function allows the formuala-style notation (e.g. `~ scghq1_dv, data = ukhls` ). So the commands below will produce equivalent results:

```{r}
base::max(ukhls$scghq1_dv)
mosaic::max( ~ scghq1_dv, data = ukhls)
```

While indeed the same, the result is not what we would expect. We know that the maximum value of the **scghq1_dv** scale is **36**, but here we are getting `NA`. Why? And how to fix it?

When this happens, we should make sure to exclude all the missing values first by adding the additional argument `na.rm = TRUE` to the command. The argument **r**e**m**oves **na** values (i.e. missing responses) in a variety of different functions, so it's good to know about. It's very useful because many functions fail if there are missing values in the data. In our case, without specifying `na.rm = TRUE` the `max` function takes values coded as `NA` to be above the highest numeric values and prints those as the *maximum* value in the variable. If we exclude the `NA` (i.e. missing) values, then we get the expected *maximum* value of the `scghq1_dv` variable:

```{r}
max(ukhls$scghq1_dv, na.rm = TRUE)
mosaic::max( ~ scghq1_dv, data = ukhls, na.rm = TRUE)
```

We can now rewrite the command creating the new `wellbeing` variable using the `max()` function to first extract the highest value to an object (let's call it "maxvalue" and print it to the console to check if it's correct) and use that as input in the `mutate()` command:

```{r}
maxvalue <- max(ukhls$scghq1_dv, na.rm = TRUE)    # extract the highest value

print(maxvalue)                                   # print `maxvalue` to the console to check it

ukhls <- ukhls %>% 
  mutate(wellbeing = maxvalue-scghq1_dv)          # use `maxvalue` instead of 36
```

</details>
:::

We should compare the `scghq1_dv` and `wellbeing` variables now to make sure we mutated it correctly:

```{r results='hold'}
favstats( ~ scghq1_dv, data = ukhls)
favstats( ~ wellbeing, data = ukhls)
```

::: {.codebox .code}
**Coding tip: `select()` and `summary()`**
<details>
  <summary><i>Click to view</i></summary>

The `summary()` function allows for entire datasets to be summarised (i.e. all the columns/variables in the dataset). So we could select the variables we are interested in using the `select()` function from `{tidyverse}` and then do a `summary()` on this selected data. This has the advantage of allowing a pipeline workflow and the summary statistics are printed in columns, which may be easier to compare. The disadvantage is that we only have the descriptive statistics offered by `summary()` (e.f. we are missing the *standard deviation* (sd) that `favstats()` prints by default)

```{r}
ukhls %>% 
  select(scghq1_dv, wellbeing) %>% 
  summary()
```

</details>
:::

Does it look okay? We see that the *minimum*, *maximum* and *standard deviation* values haven't changed, which is what we would expect. The other values have changed as we would expect as well, given thet the scale was reversed (i.e. mirrored).

## Step 3: **Describe** the relationship between your variables
```{r include=FALSE}
##### Step 3: Bivariate relationships ----------
```

We can now start to explore the relationship between the two variables. The best plot type to represent the relationship between two numeric variables is a **scatterplot**. We will use the `xyplot()` function from the `{mosaic}` package for this. We keep in mind that our *outcome* variable is *subjective wellbeing* and the *predictor* is *age*. So it is customary to place the *outcome* on the *y* axis and the *predictor* on the *x* axis, in line with the general formula notation **_y_ ~ _x_**. The command is the following:

```{r}
xyplot(wellbeing ~ age_dv, data = ukhls)
```

The figure doesn't look too informative, but that's because we have too many cases (respondents) in the dataset - represented by the dots in this plot - and the correlation between the two variables doesn't seem to be very strong.

One addition to the plot that we can make is to add a *fit line* that expresses the linear relationship between the variables. We do this by adding the optional argument `type = c("p", "r")` to the function (here we are *c*ombining (using the `c()` function) two different plot types into one: a _**p**oint_ plot and a _**r**egression line_ plot):

```{r}
xyplot(wellbeing ~ age_dv, type = c("p", "r"), data = ukhls)
```

The straight line we plotted is a *regression line* that visualises the linear model $\hat{y} = a + bx$ (see **Agresti 2018: 250-252**; **Gelman et al. 2020: 37-38, 82-95**). In Step 4 we will fit this model statistically to obtain the values for the $a$ and $b$ coefficients so we can interpret the regression line more efficiently. For the moment, let's rely on our eyes.

If we find that the line blends in too much with the points, we can add another option to change the colour of the line to, say, red with `col.line = "red"`:

```{r}
xyplot(wellbeing ~ age_dv, 
       type = c("p", "r"), 
       col.line = "red", 
       data = ukhls)
```

There are various additional options to customize the plot, but we're only using it to get a sense of our variables, so we won't go into details. Charts prepared for presentaiton or publication should be more refined. For our purposes, this is enough for now. What's worth paying attention to is that when adding additional option arguments, we should not forget to separate them with a comma, and it's easier to read if we write them in separate lines, like in the example above; even if the command is spread across multiple lines, `R` will recognise that it's one command all the way until the first opening bracket is closed.

::: {.questionbox .question}
**Questions**

We can see that the line is not completely horizontal, but it's slightly increasing as *age* increases. What does this tell us?

- Does the line show a *negative* or *positive* correlation?
- Does the line show a *strong* or *weak* correlation?
:::

If we are interested in obtaining the Pearson correlation coefficient value (see **Agresti 2018: 260-263**) to assess the *direction* and *strength* of the correlation we observe on the plot, we can use the `cor.test()` function from `{mosaic}`:

```{r}
cor.test(wellbeing ~ age_dv, data = ukhls)
```

The last statistic ("cor") is the correlation coefficient *r*: 0.036. Knowing this figure, how would you answer the questions above? (**tip**: have a look in **Agresti 2018: 260-263** for the interpretation of the correlation coefficient).

## Step 4: **Model** the relationship between your variables

```{r include=FALSE}
##### Step 4: Linear modeling ----------
```

After getting a sense of the relationship between our two variables, we are ready to fit a linear regression model. We will use the `lm()` function for this. It is best to save the results to a new object that stores a variety of information from the model results that we can then use for further analysis. Let's give this object the name "model1" for simplicity:

```{r}
model1 <- lm(wellbeing ~ 1 + age_dv, data = ukhls)
```

::: {.codebox .code}
**Coding tip: Understand and simplify the formula**
<details>
  <summary><i>Click to view</i></summary>

The `lm()` formula echoes the mathematical equation we are fitting:

$$
\begin{aligned}
  \color{Blue}{\widehat{subjective\:wellbeing}} &\color{Grey}{=} \color{Red}{a} \color{Grey}{+} \color{Red}{b}\color{Grey}{\times}\color{Blue}{age} \\
  
  \color{Blue}{wellbeing} &\color{Grey}{\sim} \color{Red}{1} \color{Grey}{+} \color{Red}{b}\color{Grey}{\times}\color{Blue}{\text{age_dv}}
\end{aligned}
$$

We have data on the blue components (our two variables) and we are searching for the corresponding coefficients for the red components. The $a$ is called the "**intercept**", referring to the point where the regression fit line we plotted earlier on the scatterplot *intercepts* the $y$ (wellbeing) axis **when the $x$ (age) axis equals 0**. The $b$ is called the *slope* (visually, it's the angle between a completely flat horizontal line and the regression fit line) and it equals the change in $subjective\;wellbeing$ for a one-unit increase in $age$.

The *intercept* is included in the `lm()` function by default, so we don't need to explicitly write it. We also don't have to explicitly write `data =`, we can simply write just the name of the dataset. Our function call could be simplified to:

```{r eval=FALSE}
model1 <- lm(wellbeing ~ age_dv, ukhls)
```

However, leaving out `data =` does not work for all the other formula-style functions that we have used from the `{mosaic}` package, so until you become very used to writing these commands, it's better to spell out more than the strictly necessary. Also, remember that it's more important to be accurate and to make it easy for yourself and others to understand your code easily.

</details>
:::

We can see that the object "model1" now appears in the Environment pane, and if we expand it we see a list of different elements ("coefficients", "residuals", "effects", etc.). The "coefficients" are the most elementary and most imporatnt elements. We can print them by using the `coefficients()` function or simply `coef()`:

```{r}
coef(model1)
```

As noted in the *Coding tip* above, the two *coefficients* that we see here for `(Intercept)` and `age_dv` are the corresponding values for $a$ and $b$, respectively, in the $\hat{y} = a + bx$ equation. We can substitute them - rounding down to two decimal places for simplicity - to obtain the equation: $\widehat{wellbeing} = 24.35 + 0.0109\times age$. So what does this mean? 

We said at the start that the aim of a linear regression model is to help us predict the value of *subjective wellbeing* from one's *age*; i.e. what would our best guess concerning someone's *subjective wellbeing* score be if the only information we had about that person was their age? Knowing someone's *age* is already more information than **_not_** knowing anything. Without knowing one's *age* our best guess would be simply the overall **_mean_** (i.e. **_average_**) subjective wellbeing score for the entire dataset. If we remember from the descriptive statistics we've done earlier in the exercise, the **mean** of the `wellbeing` variable was 24.87867. 

We can actually obtain the mean of a numeric variable also by using the `lm()` function without any predictors specified. Let's check the mean wellbeing score using this method and compare it with what we get from the `mean()` function of the `{mosaic}` package:

```{r}
lm(wellbeing ~ 1, data = ukhls) %>% coef()

mean( ~ wellbeing, data = ukhls, na.rm = TRUE)
```
The two numbers are exactly the same, and they are the same as the mean we know from the descriptive statistics we did in earlier steps. 
So, without knowledge of one's age or any other further information, our safest guess for one's *subjective wellbeing* score, were we to randomly select respondents from our dataset, would be 24.88 (on the 0-36 scale). 
However, in knowledge of one's *age*, the slope ($b$) coefficients we got from the regression tell us that the best guess is the *intercept* (24.3470) **plus** 0.0109 **times** one's age. For example, our best gues for the *subjective wellbeing* score of someone aged 21 in our dataset would be:
```{r}
24.3470 + (0.0109 * 21)
```

More generally, the model tells us that a 1-unit (i.e. one year) difference in *age* is associated with a 0.01-unit difference in *subjective wellbeing* as measured here. 

::: {.importantbox .important}

Remember how we said earlier that the *Intercept* is the point where the regression fit line intercepts the $y$ axis when the $x$ axis is 0? This may sound okay visually, but noe that in the case of our variables this means that the *Intercept* refers to the value of *wellbeing* when the *age* variables is equal to **0**. Mathematically, there's nothing wrong with this, but in practice we don't have people aged "0" in our dataset (the youngest respondents are aged 16), and at any rate, it would be impossible to measure the *subjective wellbeing* of someone aged "0" using our scale instrument. In other words: the interpretation of the *Intercept* value is meaningless in our case. That doesn't affect the *slope* coefficient, so our regression equation is useful for predicting from, but we should not interpret the *Intercept* as it is.

In order to make the *Intercept* more meaningful, one common approach is to **mean-centre** the predictor variable(s). In our case, we would shift the *age* scale so that the average (i.e. mean) age takes the value of 0. The mean age in the dataset is 48.77 years, so using the mean-centred age variable in the regression would make the *Intercept* refer to the value of *wellbeing* for those of average age (i.e. those aged 48.77). One remainin issue would be that the `age_dv` variable measures age on a *discreet* scale recording only full years; so a fraction such as 48.77 is a fiction.

Another approach would be to centre age around the *median*, which is 49. That's an actually existing value in the dataset, so it makes the interpretation of the *Intercept* more realistic. At this point, we would go back to Step 2 and modify the *age* variable, then refit the model. Creating the new centred age variable follows a similar logic to the recoding that we've already done to `wellbeing`, but in this case, what we want is to subtract the *median* age from the values of `age_dv`, making the new *median* equal to 0. 
:::

## Back to Step 2: *centring* the predictor variable

```{r include=FALSE}
##### Back to Step 2: Centring the predictor ----------
```

::: {.taskbox .task}

Following the example in Step 2, mutate the `age_dv` variable into a new variable called `age_median_centred` that is equal to the `age_dv` variable minus its median. **Tip**: you can get the median value of a variable using the `median()` function. Here's some code scaffolding to get you going:

```{r include=FALSE}
#### Student task ----------
```

```{r, eval=FALSE, purl=FALSE}
### Complete the code below

# First, extract the median age value from the data using the `median()` function:

medianAge <- median( ??? )

# Then `print` the median to check it and see if it's correct

???

# Then use the `mutate` function to create the new variables called "age_median_centred" by subtracting the median age from all values of `age_dv`

???
```
```{r include=F}

# First, extract the median age value from the data using the `median()` function:

medianAge <- median( ~ age_dv, data = ukhls, na.rm = TRUE)

# Then `print` the median to check it and see if it's correct

medianAge

# Then use the `mutate` function to create the new variables called "age_median_centred" by subtracting the median age from all values of `age_dv`

ukhls <- ukhls %>% 
  mutate(age_median_centred = age_dv - medianAge)
```

If you've done it correctly, the new variable should look like this:

```{r}
favstats( ~ age_median_centred, data = ukhls)
```

Now we can refit the model with the new centred *age* variable. Let's save the model results with the ingenious name `model2` and print the *coefficients* as we have done with *model 1*:

```{r eval=FALSE, purl=FALSE}
### Complete the code below

# Complete the lm() command

model2 <- ????
  
# Then print the coefficients from model2
  
  
```

If you've done it well, these are the coefficients that you should expect:

```{r echo=FALSE}
model2 <- lm(wellbeing ~ age_median_centred, ukhls)

coef(model2)
```
:::

::: {.questionbox .question}
**Questions**

- How has the coefficient for the *Intercept* changed using the median-centred age variable?
- How has the coefficient for the *age* variable change?
- What is the expected *subjective wellbeing* score of someone aged 49?
- What is the expected *subjective wellbeing* score of someone who has a value of -13 on the `age_median_centred` variable?
- What is the real age in years of someone whose value on the `age_median_centred` variable is -29?

:::


## Step 5: **Present** and **interpret** your findings

```{r include=FALSE}
##### Step 5 ----------
```

The *coefficients* are only one part of the model output that we need to interpret. We can get a more comprehensive summary of the model using the same `summary()` function we are already familiar with:

```{r}
summary(model2)
```
There is a lot of information on this output. The "Call" block simply prints the function we called. The "Residuals" block shows descriptive statistics for the *residuals*, which are the distances between each individual score and the straight regression line (see the scatter-plot). The bottom block provides some statistics for the overall model - this is more important for more complex models where we have more than one predictor. 

For instance, the **R-squared** value is a measure of overall model fit. It explains how much of the *variance* in the outcome variable can be explained by the predictor variable(s). In theory, the R-squared values can range from 0 to 1. An R-squared value of 0 means that the predictor variable(s) do not explain any (0%) of the variance of the outcome variable, and a value of 1 signifies that the independent variable(s) explain all (100%) the variance in the dependent variable. In our example, the R-squared value of 0.001 implies that the predictor variable (*age*) explains 0.1% (0.001 x 100) of the variance in the outcome variable (*subjective wellbeing*). So not a lot. Variation in *subjective wellbeing* is determined by various other factors. If we develop this basic model by including some other variables that we think potentially affect *subjective wellbeing*, we expect to see the R-squared statistic increase.

::: {.questionbox .question}
**Question**

Do you remember the *correlation coefficient* (Pearson's *r*) we computed in Step 3? What do you think the relationship is between that correlation coefficient (r) and the r-squared value? **Tip**: check the results below:
```{r}
summary(model2) %>% rsquared %>% sqrt()

cor.test(wellbeing ~ age_median_centred, data = ukhls)$estimate
```
:::

The adjusted R-squared can help us compare different models because it is a standardisation of the R-squared based on the number of cases and independent variables in the model. The best fitting model is always the model with the highest possible adjusted R-squared (not the model with the highest R-squared). But we won't be comparing models in this way on this module.

We can find the *coefficients* in the third block, where the *point estimates* we examined thus far are shown under the column titled "Estimate". The other three columns ("Std. Error", "t value" and "Pr(>|t|)") refer to inferential statistics that allow us to assess the degree to which the associations we observe in our data can be taken to represent *real* associations in the wider population from which our representative sample was drawn (i.e. the UK population in this case). The most commonly cited statistic among these is the **p-value** (under the column "Pr(>|t|)", i.e. the "probability" of the t-value under the null hypothesis). In the case of our model, because we only have one predictor variable, the *p-value* associated with the *age* predictor is the same as the *p_value* for the entire model shown at the very bottom of the output.

:::{.importantbox .important}

If the values are too small or two large, then `R` will display them in *scientific notation*. The *p-values* are often very small, with many decimals. In scientific notation, the letter *e* is used to mean "10 to the power of". The scientific notation tells us to move the decimal point to the left (if there's a minus sign after *e*) or to the right (if there's a plus sign after *e*) by the number of spaces shown after the -/+ sign. We don't need to know precise numbers here, so a quick glance at the number in scientific notation can tell us whether the value is really small or really big. In respect to *p-values*, we are interested in how close they are to 0 to about three decimals, so if we see something ending in e-4 or greater then we know that the value is very small.

You will eventually get used to scientific notation, but if you find it very frustrating, you can turn it off in `R` by running the following command in your script:
```{r eval=F}
options(scipen = 999)
```

Now, however, you'll need to bear with very long fractional numbers. We find out, for example, that the p-value for the *age* coefficient is 0.0000000000055. The three stars next to it are explained under the bloc in the line called "Signif. codes": values marked with *** are lower than 0.001, those marked with ** are loser than 0.01, and those marked with * are under 0.05.

:::

As conclusion, try to write down your interpretation of the results. Check closely what each statistic means, guided by **Agresti (2018: 263-277)**.


Think about whether the results are *statistically significant* and what that means. Read and reflect on **Gelman et al. (2020: 57-63)** in particular for this.


# Exercise 2: On your own

------------------------------------------------------------------------

::: {.taskbox .task}
**Task**

Find some other numeric variables whose relationship can be modelled using a linear regression and undertake a complete analysis like the one in Exercise 1. 

There aren't too many useful numeric variables in the UKHLS dataset, so you'll have to work with what you have to come up with interesting "research questions" to answer. 

This is also what you'll be required to do in Assignment 2, so your time and effort will not be wasted!

:::


```{r eval=FALSE, include=F, purl=FALSE}
knitr::purl("Lab7.Rmd", "Lab7.R", documentation=0)
```

