{
  "hash": "d9681adfd4398e456d1d8e8aa2bac269",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Worksheet 2b\"\nsubtitle: \"Importing, exploring and describing data\"\nformat: \n  # docx: default # error becasue of embedded mp4\n  html: \n    code-link: true\n    embed-resources: true\n    fig-align: center\n    fig-width: 5\n    fig-asp: .8\n    fig-responsive: true\n    code-overflow: wrap\n    code-tools:\n      source: false # add URL to script document to be downloaded\n      toggle: false\n      caption: none\nexecute: \n  warning: false\n  error: false\n  echo: true\n  purl: false\nknitr: \n  opts_chunk: \n    comment: \"\"\n    prompt: false\n    message: false\n    strip.white: false\ncss: labcss.css\nbibliography: references.bib\n---\n\n\n------------------------------------------------------------------------\n\n```         \nTimetable week: 5\nTopic: \"Measurement and Description\"\n```\n\n------------------------------------------------------------------------\n\n# Learning outcomes\n\nBy thee end of the session you will know how to:\n\n-   Install and load *packages* in R\n-   Import data into your R environment\n-   Explore variables of different types using descriptive statistics\n-   Create basic descriptive graphs to visualise your data\n\n# Introduction\n\nIn *Worksheet 2a* you have had a look at the RStudio environment and have created an R script (`script.R` or `Lab_2.R`) and a Quarto markdown document (`Lab_2.qmd`).\nIn *Worksheet 1* you have also explored some survey data sources and read through survey documentation to gain an understanding of how sociological concepts - such as \"trust\" - are being measured and operationalised in empirical research.\nThe exercises on this worksheet will begin bringing these two activities together by exploring the original survey data using `R`.\n\nOpen the `Lab_2.R` script file and start working there.\nAt the end, once you have completed all the exercises, the final task will be to transfer some of the code from the `R` script into the `Lab_2.qmd` markdown document, add some more description to what the code is supposed to achieve, and test if you have done it all correctly by rendering it to HTML or Microsoft Word.\n\n# Exercise B1: `R` functions and user-written packages\n\n`About 15  minutes`\n\n------------------------------------------------------------------------\n\nMost work in `R` is done using *functions*.\nThe most common operations involving a function take the following generic form (think of an analogy of baking a loaf of bread):\n\n![](sheet_pics/bake_function.png){fig-align=\"center\" width=\"400\"}\n\nIt's possible to **create your own functions**.\nThis makes `R` extremely powerful and extendible.\nBut instead of programming our own functions, we can rely on functions written by other people and bundled together into **packages** designed to perform some specific (or sometimes many very general) tasks.\n\nThere are a large number of reliable, tested and oft-used packages containing functions that are particularly useful for social scientists.\nIn this module, we will rely on several such user-written packages that extend the basic packages already bundled in with our `R` software (the so-called *base-R* packages and functions).\n\nMost mature packages are available from the *Comprehensive R Archive Network* (CRAN) or private repositories such as *Bioconductor* and *GitHub*.\nPackages made available on CRAN can be installed using the command `install.packages(\"packagename\")`.\nOnce the package/library is installed (i.e. it is sitting somewhere on your computer), we then need to *load* it to the current R session using the command `library(packagename)`.\n\n::: {.taskbox .task}\n**Install the `tidyverse` collection of packages and read about the packages and functions they contain**\n\nType the following commands into your R script and execute each line of code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"tidyverse\")\n\nlibrary(\"tidyverse\")\n\n```\n:::\n\n\nHere's a one-minute video crash-course on what you are expected to do:\n\n![](sheet_pics/first-code-in-script.mp4)\n:::\n\nWe can check the list of packages that are loaded with the `tidyverse` library using a command from the `tidyverse` itself:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidyverse_packages()\n```\n:::\n\n\nWith the `library()` function, we are loading the entire \"library\" of functions and data from the given package (or suite of packages).\nIf we know that we will only be using one or two functions just once or twice from a package in our session, we could alternatively just use the function we need without loading the entire library, prepending the function name with the package name and two consecutive colons (`packagename::functionname()`):\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidyverse::tidyverse_packages()\n```\n:::\n\n\nI will sometimes use this form in the worksheets to clarify what package a function originates from, even if the package is loaded in the library.\n\n::: {.taskbox .task}\nNow that you've installed a package, write the required functions in your R scripts to **install** and **load** the following packages that we will also be using a lot:\n\n-   the `{easystats}` collection of packages\n-   the `{gtsummary}` package\n-   the `{ggformula}` package\n:::\n\n\n\n\n\nYou can read more about the packages we have just installed here:\n\n-   [`tidyverse`](https://www.tidyverse.org/)\n-   [`easystats`](https://easystats.github.io/easystats/)\n-   [`gtsummary`](https://www.danieldsjoberg.com/gtsummary/index.html)\n-   [`ggformula`](https://www.mosaic-web.org/ggformula/index.html)\n\n# Exercise B2: Importing data\n\n`About 15  minutes`\n\n------------------------------------------------------------------------\n\nSo far we have learnt about some useful functions for installing and loading R packages.\nWe can now look at functions that can be used to load a dataset and make it available for analysis.\n`R`'s native data format carries the extension `.rds`.\nBut we can import data stored in other formats too, such as the generic *comma separated values* (`.csv`) format or the various formats used by other proprietary statistical analysis packages (e.g. SPSS, Stata, SAS).\n\nThe typical survey data that we use most commonly in the social sciences is usually distributed in one of the proprietary formats mentioned, because those have been designed specifically to manage \"labelled\" data (i.e. variables that need to have longer descriptive *labels* and consist of many categorical variables whose categories/levels only make sense if they are themselves meaningfully \"labelled\").\n\nWe saw from the exercises in *Worksheet 1* that the [The World Values Survey (WVS)](https://www.worldvaluessurvey.org/WVSContents.jsp) provides the greatest number of options for data download formats, including `.rds`.\nAll other surveys provide their data either in `.csv`, `.sav` (SPSS) or `.dta` (Stata).\nMy advice is to download the `.sav` (SPSS) version of the data whenever possible, just because SPSS allows the longest variable and value labels and therefore that data labelling may be the most complete.\n\nTraditionally, one of the most severe shortcomings of `R` compared to other proprietary statistical analysis packages has been it's very limited and cumbersome support of labelling.\nThis has changed significantly over the past few years, and currently a suite of packages developed or contributed to by sociologist [Daniel Lüdecke](https://github.com/strengejacke) of the University of Hamburg are among the best available tools for this purpose - including the packages making up the `{easystats}` ecosystem, which we have just installed in the previous exercise.\n\n::: {.notebox .note}\n**Access to survey data**\n\nIn these lab sessions we will be using real-world data from some of the surveys that you learnt about in Lab 1.\nHowever, real-life data - even structured survey data - can be messy and too large.\n\nTo make it easier for you to focus on the more substantive aspects of methodological work in this module, you will be able to access lightly pre-formatted and reduced versions of those data-sets.\nYou will be able to download the SOC2069-version of the data from Canvas for the purpose of working with them for this module.\n\nHowever, if you will want to use survey data in the future, you will need to download the original raw versions of the datasets form the original sources following a free registration process.\n\n**You can read about the data that you have available here: <https://soc2069.chrismoreh.com/data/data_main>**\n:::\n\nLet's load the *World Values Survey, Wave 7* dataset.\n\nFirst, [**download the dataset from Canvas**](https://ncl.instructure.com/files/7506140/download?download_frd=1) and save it in the \"Data\" subfolder of the R Project folder you created in *Worksheet 2a*.\n\nOnce the dataset is downloaded, you can import it into `R`.\nBecause the downloaded data file is in the `.rds` format, you can navigate to the file either from within RStudio's Files pane, or manually in Windows Explorer, and double-click the file to open.\nHowever, this is not a recommended way of opening a file, because you want to have a record of the action in your R script for the future.\n\nYou can load datasets saved in the `.rds` format using the `readRDS()` command.\nIf your dataset is in another format, you can check out the `{readr}` package (for rectangular data such as `.csv` (comma separated values) or `.tsv` (tab separated values)), `{readxl}` (for Excel data), and `{sjlabelled}` (for SPSS, Stata and SAS formats).\n\nThe more generic function `data_read()` from `{easystats}`'s `{datawizard}` package loads data from various formats based on the source files extension, including files from internet sources or compressed files.\nIt relies on the `{rio}` package, which provides similar functionality.\n\n::: {.notebox .note}\n**It's important** to note that by default the `data_read()` function assumes that numeric variables where all values have a value label are categorical and will convert them into *factors*.\nThat means that all the numeric \"values\" will be replaced by the \"value labels\" (e.g. 1=\"Yes\"/ 2=\"No\" will become simply \"Yes\"/\"No\").\nThis is usually a reasonable default behaviour, because the majority of functions in `R` do not handle \"value labels\" and working with textual (character string) values can be more explicit.\nHowever, this may be less appropriate when the dataset contains many long ordered variables (such as 0-10 scale items), as we will most likely want to treat such variables as *numeric* in statistical models.\nTo cancel this default behaviour, we can add the additional argument `convert_factors = FALSE`.\nThis is the format of the variables as listed in the Codebook on the module's website (https://soc2069.chrismoreh.com/data/data_main), with both \"Values\" and \"Value labels\" kept as in the original survey questionnaire documentation.\nThis is also the format that gets imported when using `readRDS()`.\nHowever, most of the common tabulation and graphing functions will not show the category \"labels\" in the output either, and for that purpose having variables \"converted to factors\" (with the original \"Values\" overwritten by the \"Value labels\") may be a better option.\nBelow we will import the data with factors converted, as we will focus on tabulation and plotting.\n:::\n\nOn my computer, I can import the WVS7 dataset from the location (path) where I saved the file like so:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Change the file path to the one on your computer!\n\n# wvs7data <- readRDS(\"D:/GitHub/SOC2069/Data/for_analysis/wvs7.rds\")\n\n# or #\n\nwvs7data <- data_read(\"D:/GitHub/SOC2069/Data/for_analysis/wvs7.rds\")\n\n```\n:::\n\n\nIn the code above, I am assigning (with the assignment operator `<-`) the data to an **object** I called \"wvs7data\", but I could have given it any other name as long as it follows `R`'s naming conventions (no spaces, avoid special characters; [see also recommended naming strategies in the tidyverse style guide](https://style.tidyverse.org/syntax.html?q=object#object-names)).\nThe object will appear in the Environment tab (on the bottom right pane).\n\n::: {.importantbox .important}\n**Working with folders and paths**\n\nIf the data file is in the same folder as the R script used to import it, it's enough to specify the data file's name to import it; e.g.:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwvs7data <- data_read(wvs7.rds)\n\n```\n:::\n\n\nHowever, if the data is in a different directory, you will need to specify either a relative path from the script to the data file, or an absolute path from the home/working directory.\n\n***Copy/pasting paths on Windows computers***\n\nAn easy way to get the path to the data file is to navigate there on your computer, copy the path, and paste it into your R script.\nHowever, there are some complications with this procedure when using a Windows PC. My path above would have copied as `D:\\GitHub\\SOC2069\\Data\\for_analysis`, but `R` does not recognise back-slashes as path elements because in `R` the backslash has a special meaning.\nYou must manually replace backslashes with either forward slashes or double-backslashes (`D:\\\\GitHub\\\\SOC2069\\\\Data\\\\for_analysis`).\n\nAnother option is to use the function `readClipboard()`, which pastes into R whatever text you have on your clipboard (after copying it) using double backslashes.\nFor example, you can copy a path to the folder where the data is located, then add the filename to the end of the text string using the `paste0()` function to get a working path to the file you want to load.\nYou can check:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# First, copy the path to your data folder on your computer\n# Check the output:\n\npaste0(readClipboard(), \"/wvs7.rds\")\n\n```\n:::\n\n\nThe file in the path can then be imported as normal:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwvs7data <- data_read(paste0(readClipboard(), \"/wvs7.rds\"))\n```\n:::\n\n\n***Using the `{here}` package***\n\nWorking with paths, folders, directories in a sustainable and robust way can be a challenge.\nThe `{here}` package provides some useful options if you are working within R project directories.\nBy loading the `here` library, you establish the working directory to be the root folder of your R project (i.e. the folder where the `.Rproj` file is located), and you can easily construct paths to any file within that project by listing the folders that contain it relative to the project root.\nThis works similarly on all operating systems, and it's as easy as:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\nlibrary(here)\n\nwvs7data <- data_read(here(\"Data\", \"for_analysis\", \"wvs7.rds\"))\n```\n:::\n\n\nYou can read more about the package in the [\"R for Social Scientists\"](https://datacarpentry.org/r-socialsci) course.\n:::\n\nWe can look at the dataset object in the Environment pane.\nIf we click on the blue button with the white arrow before the name of the object, a list of variables and other information about them will roll down.\nIf we click on the object's name or info, the dataset will open in the Sources pane, just next to the R script file.\nThis is equivalent to having run the following command:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nView(wvs7data)    \n```\n:::\n\n\nNote the capital \"V\"; `R` is case-sensitive, so always pay attention; `view(wvs7data)` won't work.\n\nYou can explore the dataset a bit.\nOnly the first 50 columns (i.e. variables) are displayed, to see the next 50 you can click on arrow (\\>) in the dataset window's toolbar.\nOnce you've had a quick look, you can close that view or return to the R script.\n\nWe have already read about the WVS7 survey in Lab 1 and have had a look at the survey website.\nThe codebook for the dataset is available at: <https://soc2069.chrismoreh.com/data/data_main#world-values-survey-wave-7-wvs7>\n\nHave a look through the dataset description and variable list to get a sense of the data.\nAs you did last week, locate the variables that refer to \"trust\" and make a note of the variable names.\n\n# Exercise B3: Basic descriptive statistics\n\n`About 30 minutes`\n\n------------------------------------------------------------------------\n\nLet's learn a few functions that can help us explore variables through basic descriptive statistics.\n\n## Tabulate categorical variables\n\nMost of the variables in the dataset are categorical, so tabulating the frequency distributions of their categories can come handy.\nThere are various ways for doing this in R, but one of the most convenient options is to use the `data_tabulate()` function from the `{datawizard}` package.\n\nLet's look, for instance, at the \"generalised social trust\" variable, *Q57*:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwvs7data |> data_tabulate(Q57)\n\n```\n:::\n\n\nLet's decipher the code above:\n\n-   first, we assume that there is an object called 'wvs7data' in our Environment that contains the *WVS7* dataset; unless we restarted RStudio or deleted it, the object should still be there from the previous exercise;\n\n-   the `|>` (or `%>%`) operator (called a *pipe*) allows objects and functions to be fed (i.e. piped) forward to other functions.\n    The `%>%` \"pipe\" originates from the user-written package `{magrittr}` and was widely adopted in the `{tidyverse}`.\n    The `|>` version is a new native operator introduced in recent versions of `R`.\n    It looks a bit simpler than the original pipe, and it behaves somewhat differently (we won't get into that here).\n    The Windows Rstudio keyboard shortcut to insert the pipe character is *Ctrl + Shift + M*, which by default inserts the original version.\n    You can change the RStudio settings to insert the native version instead.\n    If you get to use RStudio more, you may want to do this (in the RStudio Menu bar, go *Tools* \\> *Global Options...*, click *Code* and then tick the box *\"Use native pipe operator\"*).\n    Getting into the habit of using the \"pipe\" workflow is particularly useful as it makes combining a series of operations/commands easier to read and follow.\n    Here, we take our dataset and pipe it forward so that we can easily refer to variables from that dataset (such as *Q57*);\n\n-   the remaining code applies the `data_tabulate` function to the variable *Q57*\n\nWe could have written the command in these ways, with the same result:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_tabulate(wvs7data, Q57)\n\ndata_tabulate(wvs7data$Q57)\n```\n:::\n\n\nThe first approach above takes the general form `function_name(dataframe_name, variable_name)`, which some `tidyverse`-based functions allow.\nThe second one follows base-R's general form of `function_name(dataframe_name**$**variable_name)`, using the `$` operator.\nHere, `wvs7data**$**Q57` extracts the *Q57* variable/column from the *wvs7data* dataframe object.\n\nWe will aim to follow the `|>`/`%>%` \"piped\" workflow whenever possible, because it's more flexible to expand and reads more logically to humans.\nHowever, the `$` \"extract\" notation is ubiquitous and many functions that depend of base-R require it, so it's worth being aware of it.\n\nThe resulting frequency table will be printed in the Console, and will look something like this:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nMost people can be trusted (Q57) <categorical>\n# total N=94278 valid N=93005\n\nValue                      |     N | Raw % | Valid % | Cumulative %\n---------------------------+-------+-------+---------+-------------\nMost people can be trusted | 22552 | 23.92 |   24.25 |        24.25\nNeed to be very careful    | 70453 | 74.73 |   75.75 |       100.00\n<NA>                       |  1273 |  1.35 |    <NA> |         <NA>\n```\n\n\n:::\n:::\n\n::: {.cell type='questionblock'}\n<div class=\"questionblock\">\n<p><strong>Question</strong></p>\n<p>Interpret the table:</p>\n<ul>\n<li>How many of the respondents in the data think that “most people can\nbe trusted”?</li>\n<li>What is the percentage of those who are <em>less</em> trusting of\nothers?</li>\n<li>Are there any missing values (NA) on this variable?</li>\n</ul>\n</div>\n:::\n\n\n## Summarize numeric variables\n\nThere are much fewer *numeric* variables in this dataset (and in sociological datasets in general).\nOne that we have is *age* (*Q262*).\nThe base-R function `summary` is enough to get a basic summary of a quantitative/numeric type variable.\nBase R functions, however, don't always work with a pipe ( `|>` ) workflow; `summary` doesn't, so we must use the `$` operator:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(wvs7data$Q262)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  16.00   30.00   41.00   43.41   56.00  103.00     510 \n```\n\n\n:::\n:::\n\n\nThere are various other options available too in different packages.\nFor example, the below is the default output from the `describe_distribution()` function from the `{datawizard}` package which is part of the `{easystats}` ecosystem that we already installed and loaded:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwvs7data |> describe_distribution(Q262)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nVariable |  Mean |    SD | IQR |           Range | Skewness | Kurtosis |     n | n_Missing\n------------------------------------------------------------------------------------------\nQ262     | 43.41 | 16.58 |  26 | [16.00, 103.00] |     0.40 |    -0.71 | 93768 |       510\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\nThe `describe_distribution()` function also allows us to change the measure of \"central tendency\" that we want reported.\nThe easiest is to request \"all\" available measures of centrality available for the given variable type:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwvs7data |> describe_distribution(Q262, centrality = \"all\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nVariable | Median |   MAD |  Mean |    SD |   MAP | IQR |           Range | Skewness | Kurtosis |     n | n_Missing\n-------------------------------------------------------------------------------------------------------------------\nQ262     |     41 | 19.27 | 43.41 | 16.58 | 30.03 |  26 | [16.00, 103.00] |     0.40 |    -0.71 | 93768 |       510\n```\n\n\n:::\n:::\n\n::: {.cell type='questionblock'}\n<div class=\"questionblock\">\n<p><strong>Question</strong></p>\n<p>Interpret the outputs:</p>\n<ul>\n<li>What is the average (mean) age of the respondents in the\ndataset?</li>\n<li>What about the median age?</li>\n<li>How spread out is the age of the respondents?</li>\n<li>Is the variable skewed in any direction?</li>\n<li>What is the <em>minimum</em> and <em>maximum</em> age of the\nrespondents?</li>\n<li>Are there any missing values on this variable?</li>\n</ul>\n</div>\n:::\n\n\nThere are a few statistics in the outputs above that may be less straightforward:\n\n-   the *1st Qu.* and *3rd Qu.* refer to the 1st and 3rd quartiles (i.e. the 25th and 75th percentiles), respectively. Half (50%, i.e. 75%-25%) of the values in the variable fall between these two values. The range of values between the two is also known as the *interquartile range* (IQR), and it's a similar measure of spread/dispersion of the data as the *standard deviation* (SD) or the *variance* (the square of the SD), but more robust against outliers.\n-   the *MAD* refers to the *median absolute deviation*, another measure of data dispersion. It is the *median* of the absolute values of the deviations of each data point from the data's median.\n-   the *MAP* (for *Maximum A Posteriori* probability estimate) is effectively an equivalent of the *mode* for numeric/continuous variables.\n-   the *skewness* and *kurtosis* of a distribution is easiest to evaluate visually, but there are rules of thumb for evaluating the statistics; if the skewness value is roughly between -0.5 and 0.5, the distribution is approximately symmetric; values between -1 and -0.5 or between 0.5 and 1 reflect moderate negative or positive skewness, respectively; if the skewness is lower than -1 (negative skewed) or greater than 1 (positive skewed), the data are extremely skewed. Similarly, kurtosis values greater than 3 denote a *leptokurtic* shape (peaked, with thick tails), values below 3 a *platykurtic* (flatter) shape, and values around 3 denote a roughly normal distribution. The values of -0.71 thus means a strongly *platykurtic* (flat) distribution. We can get a better sense of what this means by looking at a visual description of the variable's distribution.\n\n## Visualising distributions\n\nIt is often useful to make a graph to visualise a variable, especially a *numeric* variable.\nA figure can often convey information in a much more efficient way that a statistic/number.\nOne of the most useful graph types for visualising numeric variables is a *histogram*.\nThere are many functions available in R for producing graphs.\nOnce you become more proficient and use R more often, it is very useful to learn the graphing workflow of the `{ggplot2}` package (included in the `{tidyverse}`), which builds up a graph canvas step by step using various elements.\nThat allows the creation of highly customized figures of publishable quality.\n\nFor quick graphs, base R also has good plotting functionality with simple commands.\nA histogram of age could be called with:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(wvs7data$Q262)\n```\n\n::: {.cell-output-display}\n![](Worksheet_2b_files/figure-html/unnamed-chunk-20-1.png){width=480}\n:::\n:::\n\n\nAn equivalent chart using `{ggplot2}` functions requires more steps:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(wvs7data) +         # start a blank canvas with the data\n  aes(x = Q262) +          # add an \"aesthetic\", i.e. the variable(s) we want to visualise\n  geom_histogram()         # add a \"geometric object\", i.e. the type of graph we want \n```\n\n::: {.cell-output-display}\n![](Worksheet_2b_files/figure-html/unnamed-chunk-21-1.png){width=480}\n:::\n:::\n\n\nIf you see a cryptic warning message that a number of \"rows containing non-finite values\" have been removed, in this case that refers to the missing values (NA) on the *age* variable (compare to the summary statistics you produced earlier).\n\nThe `{ggformula}` package can be a useful choice if you want to combine the ease of expanding the graphs in more complex ways with a command structure that will become more familiar once we start modelling relationships between variables.\n\nAll the plot calls in the `{ggformula}` package start with `gf_`, and they require a \"formula\" style input.\nThe command would be:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngf_histogram( ~ Q262, data = wvs7data)\n```\n\n::: {.cell-output-display}\n![](Worksheet_2b_files/figure-html/unnamed-chunk-22-1.png){width=480}\n:::\n\n```{.r .cell-code}\n\n# or #\n\nwvs7data |> gf_histogram( ~ Q262)\n```\n\n::: {.cell-output-display}\n![](Worksheet_2b_files/figure-html/unnamed-chunk-22-2.png){width=480}\n:::\n:::\n\n\nThis function has a similar structure to the functions that we will use for statistical modelling, generically of the form **goal(y \\~ x, data = my_data)**.\nIn our case, we are only \"modelling\" one variable here, so the first (y-axis) object before the tilde (\\~) is missing.\nThe data can be specified with the option `data = ...`, or in a \"piped\" workflow.\n\n*Density plots* are also useful for this purpose:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwvs7data |> gf_density( ~ Q262)\n```\n\n::: {.cell-output-display}\n![](Worksheet_2b_files/figure-html/unnamed-chunk-23-1.png){width=480}\n:::\n:::\n\n\n*Categorical* variables are best visualised using *bar charts*.\nFor example, a bar chart of the \"generalised trust\" variable:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwvs7data |> gf_bar( ~ Q57)\n```\n\n::: {.cell-output-display}\n![](Worksheet_2b_files/figure-html/unnamed-chunk-24-1.png){width=480}\n:::\n:::\n\n\nIf we are not interested in the missing (NA) values, we can eliminate them as part of the plotting function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwvs7data |> \n  drop_na(Q57) |> \n  gf_bar( ~ Q57)\n```\n\n::: {.cell-output-display}\n![](Worksheet_2b_files/figure-html/unnamed-chunk-25-1.png){width=480}\n:::\n:::\n\n\nFinally, we could combine the two graphs that we have created so far, faceting the histogram of *age* by the categories of the \"generalised trust\" variable.\nTo do this, we add the categorical variable after a vertical bar (\\|):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwvs7data |> \n  drop_na(Q57, Q262) |> \n  gf_density( ~ Q262 | Q57)\n```\n\n::: {.cell-output-display}\n![](Worksheet_2b_files/figure-html/unnamed-chunk-26-1.png){width=480}\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n## Exercise B4: On your own\n\n-   Make sure that you have copied all the commands from this page to your R script and that you have saved the script so you can access the commands at a later date.\n\n-   In your own time, go on and select a few more variables from the dataset and explore them using the appropriate descriptive statistics using the functions we have practiced above.\n\n-   Think about what the results tell you.\n    Finally, transfer the commands that you practiced here into th `Lab_2.qmd` document, setting up code chunks appropriately, organising the code blocks into relevant sections under sub-titles, and adding some explanatory text in between.\n    When you're done, click on the \"Render\" button and check the resulting output.\n    Will you manage to generate an output without errors?\n",
    "supporting": [
      "Worksheet_2b_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}