---
title: "Intro to R/RStudio.Rmd"
output:
  html_document:
    theme: lumen
    toc: yes
    toc_float: yes
    toc_highlight: no
    tabset: yes
    number_sections: no
    anchor_sections: no
    self_contained: yes
    css: ./../my_themes/labcss.css
    code_download: true
---

```{r knitr options, include=FALSE}
knitr::opts_chunk$set(fig.width=6, fig.asp = 0.618, fig.show = 'hold', eval = F, warning=T, comment=NA, prompt=FALSE, message = F, strip.white = F, class.source = "bc-b", attr.source='.numberLines', results = 'hold')
```


# What is R? What is RStudio?

The term "`R`" is used to refer to both the programming language and the software that interprets the scripts written using it.

[RStudio](https://rstudio.com) is currently a very popular way to not only write your R scripts but also to interact with the R software. To function correctly, RStudio needs R and therefore both need to be installed on your computer.

To make it easier to interact with R, we will use RStudio. RStudio is the most popular IDE (Integrated Development Environment) for R. An IDE is a piece of software that provides tools to make programming easier.

# Why learn R?

- R does not involve lots of pointing and clicking, and that's a good thing - it forces us to work with *scripts*, which makes the steps you used in your analysis clear, and the code you write can be inspected by someone else who can give you feedback and spot mistakes
- R code is great for reproducibility
- R is interdisciplinary and extensible
- R produces high-quality graphics
- R is free, open-source and cross-platform

# Getting R and RStudio

**R** and **RStudio** are separate downloads and installations. R is the
underlying statistical computing environment, but using R alone is no
fun. RStudio is a graphical integrated development environment (IDE) that makes
using R much easier and more interactive. You need to install R before you
install RStudio. Once installed, because RStudio is an IDE, RStudio will run R in 
the background.  You do not need to run it separately, but we'll check how it looks like just to get an idea.

## On Windows

* Download R from
  the [CRAN website](http://cran.r-project.org/bin/windows/base/release.htm).
* Run the `.exe` file that was just downloaded.
* Go to the [RStudio download page](https://www.rstudio.com/products/rstudio/download/#download).
* Under *Installers* select **RStudio x.yy.zzz - Windows.
  Vista/7/8/10** (where x, y, and z represent version numbers).
* Double click the file to install it.
* Once it's installed, open RStudio to make sure it works and you don't get any
  error messages.
  
## On Mac OS

* Download R from
  the [CRAN website](http://cran.r-project.org/bin/macosx/).
* Select the `.pkg` file for the latest R version.
* Double click on the downloaded file to install R.
* It is also a good idea to install [XQuartz](https://www.xquartz.org/) (needed
  by some packages).
* Go to the [RStudio download page](https://www.rstudio.com/products/rstudio/download/#download).
* Under *Installers* select **RStudio x.yy.zzz - Mac OS X 10.6+ (64-bit)**
  (where x, y, and z represent version numbers).
* Double click the file to install RStudio.
* Once it's installed, open RStudio to make sure it works and you don't get any
  error messages.
  
## Updating R and RStudio

If you already have R and RStudio installed and would like to update them to the newest version:

* Open RStudio, and click on "Help" > "Check for updates". If a new version is
	available, quit RStudio, and download the latest version for RStudio.
* To check which version of R you are using, start RStudio and the first thing
  that appears in the console indicates the version of R you are
  running. Alternatively, you can type `sessionInfo()`, which will also display
  which version of R you are running. Go on
  the [CRAN website](https://cran.r-project.org/) and check
  whether a more recent version is available. If so, you can update R using
  the `installr` package and the `updateR()` function:
  
```{r eval=FALSE, include=TRUE, eval=F}
installr::updateR()
```


# R and the RStudio interface

1. Once installed, let's have a look at how `R` looks like, just so we know
2. But we'll only be using `R` via `RStudio` in this course and in actual research, so let's explore `RStudio` a bit more. Let's open the `RStudio` programme that we have just installed.

# RStudio (main) Panes

RStudio shows you four **main** panes:

![](images/console_etc.png)

## The R Console Pane  
The R Console, by default the left or lower-left pane in R Studio, is the home of the R "engine". This is where the commands are actually run and non-graphic outputs and error/warning messages appear. The **Console** is the direct interface to the `R` software itself; it's what we get if instead of `RStudio` we open the `R` software: a direct interface to the `R` programming language, where we can type commands and **where results/messages are printed**.

You can directly enter and run commands in the R Console, but realize that these commands are not saved as they are when running commands from a script. For this reason, we should not use the **Console** pane directly too much. For typing commands that we want `R` to execute, we should instead use an `R` script file, where everything we type can be saved for later and complex analyses can be built up. 


## The Source Pane  
This pane, by default in the upper-left, is a space to edit and run your scripts. This pane can also display datasets (data frames) for viewing.  

:::: {.notebox .note}
**Note**

If your RStudio displays only one left pane it is because you have no scripts open yet. We can open an existing one or create a new one. We' ll do that a bit later.
::::

## The Environment Pane  
This pane, by default the upper-right, is most often used to see brief summaries of objects in the R Environment in the current session. These objects could include imported, modified, or created datasets, parameters you have defined, or vectors or lists you have defined during analysis. You can click on the arrow next to a dataframe name to see its variables.

:::: {.notebox .note}
**Note**

If your Environment pane is empty, it means that you don't have any "objects" loaded or created yet. We will be creating some objects later and we will also import an example dataset.
::::


## Files, Plots, Packages, Help, etc  
The lower-right pane includes several tabs including plots (display of graphics including maps), help, a file library, and available R packages (including installation/update options).  

**You can arrange the panes in different ways, depending on your preferences, using *Tools > Global Options in the top menu.* So the arrangement of panes may look different on different computers.**


# File directories and R _**Projects**_

First, with the `getwd()` function (i.e. "**Get** the **w**orking **d**irectory") we can check which file directory we are in at the moment. If we opened the `RStudio` program directly, then our *working directory* should the the one that was pre-defined during the installation of `RStudio`. It's best to set as *working directory* the folder that we are planning to use for a specific session/data analysis project. The *working directory* tells R where to look for and save files.

## Setting a directory manually (not recommended, but useful to know about)

Within `RStudio` go to *Session >> Set Working Directory > Choose Directory...*. There you can select some options for the working directory of your preference.

Or you can use the function: `setwd("path/to/directory")`

:::: {.notebox .note}
**Note**

Unfortunately, if you are on a Windows machine you will need to change all backslashes (\\\) to forward slashes (/). This is because R follows UNIX conventions which are native to Linux and Mac computers.
::::

However, there is a big problem with this approach. If you send your `.R` script file to someone else who is working on a different machine, it is most likely that your path will be pointing to a location that doesn't exist on that other computer, and it will need to be changed manually, introducing potential errors.

## Using an **R Project** (recommended)

The best way to set the working directory is to create a self-contained **R Project** in `RStudio`. When you do this, the R Project stores all files and data objects which were opened in the last session, and also keeps track of where the files are stored, so there is no need to set the directory. **Students in class should save everything, all their work, in an RStudio project. They will be able to download an initial folder containing data and some template files, and should save that to their OneDrive and use it throughout the semester.**

To create a new R Project select *File > New Project* from the top RStudio menu.  

Creating a new R project will create:  

* A new project directory (folder)  
* A R project file (.Rproj) in the project folder that serves as a shortcut to open the project via RStudio  

Note:  

* You can make sub-folders for data, scripts, etc.  
* All files produced by R scripts saved to the project will also save into the project folder  


:::: {.taskbox .task}

Let's have a go at setting up an R Project from scratch. Let's try out the following tasks live:

1. Create a new folder set up as an R project
2. create a new R script (`.R`) where we will copy and store the code we use below; let's call it simply "script"
3. download the source document for this html page, which is a so-called Rmarkdown file (`.Rmd`), a type of text file that combines written text and R code, together with outputs. This is the format that we'll ask students to use for Assignment 2, so we'll have a look a bit later.

Once we have done this, we have a folder on our computers that will be recognised by RStudio as an R Project, and within that folder we will have an R Markdown file (`.Rmd`) and an R script (`.R`).

We will be using these two different file types in this course, so it's worth getting familiar a little bit with them. There are some good trainings and students will be required to go through the following in Week 10:

- Component 6 of "R for Social Scientists" (“Getting started with R Markdown”, around 1 hour): https://datacarpentry.org/r-socialsci

- Official "R Markdown" tutorial from RStudio: https://rmarkdown.rstudio.com/lesson-1.html

- Andy Field's "Getting started in R and RStudio" (see on the software tutorial page too), especially the section on 'R Markdown' (http://milton-the-cat.rocks/learnr/r/r_getting_started/#section-r-markdown
::::


# Basic arithmetic operations and functions

The first thing new users are shown how to do in `R` are basic arithmetic operations. This is useful get a first experience **doing** things in the `R` language, but we won't be doing much of it in the module itself.

Let's have a look at a few operations using the **Console** directly. Let's say we want to know the result of adding up three numbers: 1, 3 and 5. In the Console pane, type the following command:

```{r, eval=F}
1 + 3 + 5
```

This will print out the result (9) in the Console:

```{r eval=T, echo=FALSE}
1 + 3 + 5
```

The `[1]` in the result is only the line number; in this case, our result only consists of a single line.

We can also save the result of this operation in an object, so we can use it for other operations. For example, let's save the result in an object called "nine". We create objects by using the `<-` characters:

```{r}
nine <- 1 + 3 + 5
```

Notice that there is no output printed in the Console this time. But there are also no error messages, so the operation should have run without any problems. Instead, if we look at the Environment pane, we notice that it is no longer empty, but contains an object called "nine" that stores the value "9" in it. We can now use this object for other operations, such as:

```{r}
nine - 3
nine + 15
nine / 3
nine * 9
```

We see the results of these operations printed out in the Console.

We can also check results of so-called **logical** operations. For example, let's check whether 9 is greater than 5, and whether it is lower than 8:

```{r}
9 > 5
9 < 8
```

We get "TRUE" and "FALSE", respectively, as we would expect. Let's now check whether our objecyt "nine" is equal to the number 9. This is a bit of a tricky task, maybe; let's see why. What happens if we type:

```{r}
nine = 9
```

Did we get the result we expected? Nothing was printed in the output, so seemingly nothing happened... That's becasue the "=" sign is also used as an assignment operator in `R`, just like "<-". So we basically assigned the value "9" to the object "nine" again. To use the *equal* sign as a logical operator we must type it twice (==). Let's see:

```{r}
nine == 9
```

Now we get the expected result. 

This distinction between "=" and "==" is important to keep in mind. What would have happened if we had tried to test whether our object "nine" equals value "5" or not, and instead of the logical operator (==) we used the assignment operator (=)? Let's see:

```{r}
nine = 5
```

In the Console we again see no results printed, but if we check our Environment, we see that the value of the object "nine" was changed to 5. So it can be a dangerous business. We'll be using the "<-" as assignment operator instead of "=" to avoid any confusion.

So, try out the following commands in turn now and check if the results are what you'd expect:

```{r}
nine == 9
nine == 5
five <- 9
nine == five
five=nine
nine == five
nine + five <= 10 # lower than or equal to ...
```

The text following the hashtag (#) in the last line is a comment. If you'd like to comment on any code you write (i.e. you do not wish R to try to 'run' this code) just add a hash (`#`) or series of hashes in front of it. This is particularly useful in R scripts that you save to use later.

# Functions

Staying with basic arithmetic operations still, let's try to check the following:

```{r eval=TRUE}
sqrt(9)
```
The result we get is "3", the *square root* of 9. The `sqrt()` command is what we call a **function**. This particular function takes the square root of a number/numeric object specified in the brackets.

Let's see if we get what we expect if we run the following commands:

```{r}
nine <- 9
sqrt(nine)
nine <- nine
sqrt(nine)
nine <- "nine"
sqrt(nine)
```

The last command results in an error message telling us that we attempted to pass a "non-numeric argument to mathematical function". That's because in the previous step we created an object called "nine" that stores not a numeric value but a textual value (i.e. "nine"). We can apply functions that operate on text (i.e. "strings") using this object as target, but not mathematical functions that operate on "numerical" objects. For example:

```{r}
some_text <- "is a number"   # Create a new object storing a text string
paste(nine, some_text)       # use a function called "paste()" to concatanate two objects 
```

And for some more fun:

```{r}
eight <- 8
other_text <- "is also a number"
paste(nine, some_text, "and", eight, other_text)
```
So, what we learnt is that the `paste()` function, as opposed to the `sqrt()`, can take both next and numbers as arguments, and we can even feed in text directly.

There are many useful functions for manipulating text in `R`, but we won't use them much on this module. It's useful, however, to be aware that almost everything is possible and there are many functions that already exist for various tasks. For fun, check out the below functions called `gsub` and `grepl`. What do you think they do? Clue: if you're stuck, search the help file using `?`

```{r}

gsub("R-studio", "Rstudio", "R-studio is a great piece of software")

grepl("chocolate", "Mary likes chocolate cookies")

```


Most work in R is done using _Functions_. These take the following generic form: _function(argument(s))_. So, putting together what we have done so far, any piece of analysis that we will do in `R` can be thought of as analogous to baking a loaf of bread

![](images/bake_function.png)

It's possible to **create your own functions**. This makes R extremely powerful and extendible. We're not going to cover making your own functions in this course, but it's important to be aware of this capability. There are plenty of good resources online for learning how to do this, including [this one](https://www.statmethods.net/management/userfunctions.html)

# Getting help

As we have seen above, to find out about a particular function just type `?` and the name of the function into the console, e.g. `?grepl`. This accesses the help files on your computer. If you'd like to search more broadly type `??grepl` and your computer will look online for relevant materials on CRAN (the main R website)

Help files in R are quite densely written and not particularly aimed at beginners. Fortunately there are loads of excellent resources on the internet. Here are some really good sites:

(a) [https://www.tidyverse.org/](https://www.tidyverse.org/) - A brilliant set of of resources on all things related to the tidyverse, Hadley Wickham's brilliant suite of packages
(b) [https://www.statmethods.net/index.html](https://www.statmethods.net/index.html) - a quick way of looking up basic R techniques
(c) [https://stats.idre.ucla.edu/r/modules/](https://stats.idre.ucla.edu/r/modules/)
(d) [https://rseek.org/](https://rseek.org/) - a search engine for all things related to R (because the word 'R' brings up a whole load of irrelevant stuff in Google)
(e) [http://www.cookbook-r.com/](http://www.cookbook-r.com/) - this has lots of tips on how to do graphics.


# Packages

Instead of programming your own functions in the ***R*** language, you can rely on functions written by other people and bundled within a package that performs some set task. There are a large number of reliable, tested and oft-used packages containing functions that are particularly useful for social scientists.  

Some particularly useful packages:
  - the ```tidyverse``` bundle of packages, which includes the ```dplyr``` package (for data manipulation) and additional R packages for reading in (`readr`), transforming (`tidyr`) and visualizing (`ggplot2`) datasets. It also ports the `%>%` so-called pipe operator that helps write more human-readable code
  - to import datasets in non-native formats and to manage attached labels (a concept familiar from other statistical packages but foreign to ***R***), load the `sjlabelled` package (an alternative to `haven` and `labelled`, which work in a similar way but provide less functionality) 
  - the `summarytools` package can be useful for making simple summary tables and crosstabulations
  - the `sjmisc` package contains very useful functions for undertaking data transformations on labelled variables (recoding, grouping, missing values, etc); also has some useful tabulation functions
  - the `sjPlot` package contains functions for graphing and tabulating results from regression models
  - the `mosaic` and `ggformula` packages provide functions to write data visualisation commands using "formula syntax", which is particularly useful for introductory-level teaching/learning as it unifies data description and statistical modelling around a shared logical structure (read about it [here](http://www.mosaic-web.org/ggformula/articles/pkgdown/ggformula-blog.html) and [here](http://www.mosaic-web.org/mosaic/articles/web-only/LessVolume-MoreCreativity.html))

Packages are often available from the *Comprehensive R Archive Network* (CRAN) or private repositories such as *Bioconductor*, *GitHub* etc. Packages made available on CRAN can be installed using the command `install.packages("packagename")`. Once the package/library is installed (i.e. it is sitting somewhere on your computer), we then need to _load_ it to the current R session using the command `library(packagename)`.

So using a package/library is a two-stage process. We:

1. `Install` the package/library onto your computer (from the internet)
2. `Load` the package/library into your current session using the library command.

Let's start by installing the 'tidyverse' package, and then load it:

```{r, eval=FALSE}
install.packages("tidyverse")  ## this command installs packages from CRAN; note the quotation marks around the package name
```

You can check the suite of packages that are loaded when you load the `Tidyverse` library using a comand from the `tidyverse` itself:

```{r, eval=FALSE}
tidyverse_packages()
```

:::: {.questionbox .question}
**Question**

Why do you think we got an error message when we tried to run the above command?
::::

Because `tidyverse_packages()` is itself a function from the `tidyverse`, in order to use that function we need not only to install the `tidyverse` but also to make its functions available. In other words, we did not yet *load* the `tidyverse` for use in our R session, we only just installed it on our computers.

If we don't want to load a package that we have downloaded - because maybe we only want to use a single function once and we don't want to burden our computer's memory, we can state explicitly which package the function is from in the following way:

```{r, eval=FALSE}
tidyverse::tidyverse_packages()        # Here we state the package followed by two colons, then followed by the function we want
```


But in many cases we do want to use several functions at various points in an analysis session, so it is usually useful to *load* the entire package or set of packages:

```{r, eval=FALSE}
library(tidyverse)             ## this command loads the packages in our session so we can use functions from it without having to refer to the package name first
```

Now we can use functions from that pakcage without having to explicitly state the name of the package. We can still state the name explicitly, and that may be useful for readers of our code to understand what package a function come from. Also, it may happen that different packages have similarly named functions, and if all those packages are loaded, then the functions from a package loaded later will override that in the package loaded earlier. `R` will note in a comment whether any functions from a package are **masked** by another, so it's worth paying attention to the comments and warnings printed by `R` when we load packages.

There are also convenience tools - e.g. the `pacman` package - that make it easier to load several packages at once, while at the same time downloading the package if it has not yet been downloaded on our computer.

For example, we can download a number of packages with the command below:

```{r eval=TRUE}

# Install 'pacman' if not yet installed:

if (!require("pacman")) install.packages("pacman") 

# Then load/install other packages using 'pacman':

pacman::p_load(
  tidyverse,    # general data management tools ('dplyr', etc.)
  sjlabelled,   # data import from other software (alternative to 'haven') and labels management
  sjmisc       # data transformation on variables (recoding,grouping, missing values, etc)
  )
```

# About the `Tidyverse`

## Data frames and 'tibbles'

The `Tidyverse` is built around the basic concept that data in a table should have one observation per row, one variable per column, and only one value per cell. Once data is in this 'tidy' format, it can be transformed, visualized and modelled for an analysis.

When using functions in the `Tidyverse` ecosystem, most data is returned as a `tibble` object. `Tibbles` are very similar to the `data.frames` (which are the basic types of object storing datasets in base ***R***) and it is perfectly fine to use `Tidyverse` functions on a `data.frame` object. Just be aware that in most cases, the `Tidyverse` function will transform your data into a `tibble.` If you are unobservant, you won't even notice a difference. However, there are a few differences between the two data types, most of which are just designed to make your life easier. For more info, check [R for Data Science](https://r4ds.had.co.nz/tibbles.html) and [R for Social Scientists](https://datacarpentry.org/r-socialsci/02-starting-with-data/index.html)

## Selected `dplyr` functions

The `dplyr` package is designed to make it easier to manipulate flat (2-D) data (i.e. the type of datasets we are most likely to use, which are laid out as in a standard spreadsheet, with rows referring to cases (observations; respondents) and columns referring to variables. `dplyr` provides simple "verbs", functions that correspond to the most common data manipulation tasks, to help you translate your thoughts into code. Here are some of the most common functions in `dplyr`:

  * `filter()` chooses rows based on column values.
  * `arrange()` changes the order of the rows.
  * `select()` changes whether or not a column is included.
  * `rename()` changes the name of columns.
  * `mutate()`/`transmute()` changes the values of columns and creates new columns (variables)
  * `summarise()` compute statistical summaries (e.g., computing the mean or the sum)
  * `group_by()` group data into rows with the same values
  * `ungroup()` remove grouping information from data frame.
  * `distinct()` remove duplicate rows. 

All these functions work similarly as follow:

- The first argument is a data frame/tibble 
- The subsequent arguments are comma separated list of unquoted variable names and the specification of what you want to do
- The result is a new data frame

For more info, check [R for Social Scientists](https://datacarpentry.org/r-socialsci/03-dplyr-tidyr/index.html)


## The forward-pipe (`%>%`) workflow

All of the `dplyr` functions take a data frame or tibble as the first argument. Rather than forcing the user to either save intermediate objects or nest functions, `dplyr` provides the forward-pipe operator `%>%` from the  `magrittr` package. This operator allows us to combine multiple operations into a single sequential chain of actions.

Let’s start with a hypothetical example. Say you would like to perform a sequence of operations on data frame `x` using hypothetical functions `f()`, `g()`, and `h()`:

1.  Take x *then*
2.  Use x as an input to a function f() *then*
3.  Use the output of f(x) as an input to a function g() *then*
4.  Use the output of g(f(x)) as an input to a function h()

One way to achieve this sequence of operations is by using nesting parentheses as follows:

```
h(g(f(x)))
```

This code isn’t so hard to read since we are applying only three functions: `f()`, then `g()`, then `h()` and each of the functions is short in its name. Further, each of these functions also only has one argument. However, you can imagine that this will get progressively harder to read as the number of functions applied in your sequence increases and the arguments in each function increase as well. This is where the pipe operator `%>%` comes in handy. `%>%` takes the output of one function and then “pipes” it to be the input of the next function. Furthermore, a helpful trick is to read `%>%` as “then” or “and then.” For example, you can obtain the same output as the hypothetical sequence of functions as follows:

```
x %>% 
  f() %>% 
  g() %>% 
  h()
```

You would read this sequence as:

1. Take x *then*
2. Use this output as the input to the next function f() *then*
3. Use this output as the input to the next function g() *then*
4. Use this output as the input to the next function h()

So while both approaches achieve the same goal, the latter is much more human-readable because you can clearly read the sequence of operations line-by-line. Instead of typing out the three strange characters of the operator, one can use the keyboard shortcut *Ctrl + Shift + M* (Windows) or *Cmd + Shift + M* (MacOS) to paste the operator.

Note that since `R 4.1.0` there is also a [native pipe operator](https://www.r-bloggers.com/2021/05/the-new-r-pipe/) in ***R*** (|>), and in RStudio one can set the shortcut to paste the new pipe operator instead.

# Some useful data management functions and workflows

In each data analysis project we encounter specific data manipulation challenges. Some data transformation tasks are very common in a majority of projects using large survey-based social science data, and we can think about performing these tasks in terms of a workflow, a logical step-by-step order in which to manipulate the data to end up with a dataset that can easily be submitted to statistical analysis. 

Let's follow such a hypothetical workflow to cover some of these most common requirements. Our hypothetical data analysis aim will be to estimate and report on a model predicting `trust in strangers` from `age`, `sex`, `religiosity`, and `political ideology` in the UK population:

$$
Trust\_in\_strangers = \beta_0 + \beta_1(Age) + \beta_2(Sex) + \beta_3(Religiosity) + \beta_4(Political\_ideology)
$$
For this, we first need some data with measurements on the variables that we need. One useful source of data for answering this question is the European Values Study. Let's use data from Wave 5 (2017-2020) of the EVS. 
## Importing datasets from other software packages

Many datasets available from secondary data sources such as the *UK Data Service* are made available in formats used by other software (e.g. Excel, SPSS, Stata). The *Comma Separated Values* (.csv) format is a more generic format, which can be imported using the base ***R*** function `read.csv()`, which is a bit slow but okay for smaller datasets. For larger datasets the `read_csv()` function from the `readr` package (in `tidyverse`) is available. See also [R for Social Scientists](https://datacarpentry.org/r-socialsci/02-starting-with-data/index.html).

`R`'s native data format is RDS (`.rds`). It's useful to convert imported datasets to this format because it compresses the data to a much smaller size.

Getting survey data in SPSS or Stata formats has the advantage that additional information about the variables are also included (e.g. variable labels, value labels). Various packages - such as `sjlabelled` or `haven` - have designed functions that are useful for importing such datasets (e.g. the `read_stata()` function from the `sjlabelled` package or its almost equivalent `read_dta()`/`read_stata()` from the `haven` package). These functions have small particularities and specificities that it's worth knowing about if we use them often, but we won't be going into details at this point.

We'll use the `sjlabelled` package to import the EVS Wave 5 dataset stored in an online repository. We can do that with the following command:

```{r eval=TRUE}
evs <- sjlabelled::read_stata("https://cgmoreh.github.io/SSC7001M/data/evs5.dta")
```

If a dataset you want to import is in another format, you can use the appropriate function to `read_*format type*`. You can check a list of the functions available in a package using the `ls()` listing command; for example:

```{r}
# List all functions in the 'sjlabelled' package that contain the word "read":

ls("package:sjlabelled", pattern = "read") 
```

Once a dataset is loaded, you can have a quick look at some of its characteristics (which you can also check manually in the Environment panel of RStudio or by opening the dataset in the Viewer). 

```{r}
# Number of rows (as first element), and the number of columns (as the second element); the 'dimensions' of the object:
dim(evs)

# Or separately, the number of rows (cases, observations):
nrow(evs)

# And number of columns (variables):
ncol(evs)
```

We see that the loaded **evs** dataset contains `r format(nrow(evs), big.mark=",")` observations and `r ncol(evs)` variables. That's a very large dataset that would benefit from some data reduction and transformations before being used for the actual statistical analysis we want to undertake.

Below, let's look in turn at some data management issues that come up very often, especially when we use data imported form other software packages that have various attributes (e.g. variable and value labels).

## There are too many variables (columns) in the dataset

Sometimes (often? always?) it's useful to select only a few variables that we are interested in using for analysis. With externally imported data we probably have access to the survey documentation and a data codebook (e.g. https://europeanvaluesstudy.eu/methodology-data-documentation/survey-2017/pre-release-evs-2017/documentation-survey-2017/). We can identify variables of interest in that documentation and select them either when importing the data or as a later step, using the `select()` function from `dplyr`. We can either mention the location number of the variables we want to keep or, more usefully, the names of the variables. We separate the variable names using a comma, or we can specify a range of variables using "**:**", as in the example below, where we keep all the variables between <tt>v35</tt> and <tt>v37</tt> from the **evs** dataset as well as other variables that are useful for addressing our hypothetical data analysis aim^[Note that in a real statistical analysis scenario you would make sure to import any sampling design variables that are required to apply the necessary weighting to your analysis; for example, [read here about weighting data in the European Values Study (2017)](https://nbn-resolving.org/urn:nbn:de:0168-ssoar-70113-4)]: 

```{r}
evs %>% select("country", "v31", "v33", "v35":"v37", "v54", "v55", "v102", "v225", "age")
```

There are also several special functions that can be used inside `select()`, such as `starts_with()`, `ends_with()`, `contains()`, `matches()`, `one_of()`, etc. (read more [here](https://rdrr.io/cran/dplyr/man/select.html)).

To actually collapse the loaded dataset to the selected variables, we have to assign (with "<-") an object name. If we assign the same name as an already loaded dataset object, the data will be overwritten:

```{r, eval=TRUE}
# We can save the reduced dataset to a new data frame object called 'evs.slim'(because we have reduced the number of its columns)
evs.slim <- evs %>% select("country", "v31", "v33", "v35":"v37", "v54", "v55", "v102", "v225", "age")
```

```{r}
# Or we can overwrite the already existing data frame object called 'evs'
evs <- evs %>% select("country", "v31", "v33", "v35":"v37", "v54", "v55", "v102", "v225", "age")

```

## I don't need all the observations (rows)

We can also select a subset of cases/observations for analysis. We may be interested in analysing only observations that satisfy certain criteria. For example, our hypothetical data analysis aim relates only to observations from the United Kingdom, whereas the **evs** dataset contains observations from many different countries. The `filter()` function (from `dplyr`) can be used to filter rows that meet some logical criteria. For setting our criteria we can use ***R***'s logical comparisons and operators:

1. Logical comparisons:
 - **<**   for less than
 - **>**   for greater than
 - **<=**   for less than or equal to
 - **>=**   for greater than or equal to
 - **==**   for equal to each other
 - **!=**   for not equal to each other
 - **%in%**   group membership; for example, “age %in% c(16, 17)” specifies those aged 16 or 17 only.
 - **is.na()**   is NA (i.e. missing)
 - **!is.na()**   is not NA (i.e. missing)

2. Logical operators:
 - **|**   means **or** (e.g. age == 16|17 ("age %in% c(16, 17)" is a shortcut with the same meaning)
 - **&**   means **and** (e.g. sex == “female” & age > 25)

One frequent mistake is to use **=** instead of **==** when testing for equality. When you are testing for equality, you should always use **==** (not **=**).

For example let's select from the already slimmed dataset only cases that come from respondents in the United Kingdom. The <tt>country</tt> variable codes the country of data collection. We can get a list of variable labels using `get_labels()` from `sjlabelled`, which has the option to also request that labels be prefixed with their values, which is useful to see for recoding purposes (read more about the `sjlabelled` package and its functions [here](https://rdrr.io/cran/sjlabelled/)): 

```{r}

# Get a list of variable labels with values prefixed (i.e. ="p" or ="as.prefix")

sjlabelled::get_labels(evs.slim$country, values ="as.prefix")

# Or, equivalently, first selecting the variable(s); this allows listing several variables within the select() function
# evs %>% select(country) %>% 
#  sjlabelled::get_labels(values = "as.prefix")
```

We can see in the list that the country label is 'Great Britain' and it is coded as 826, so we can filter the data using the following command (assigning the reduced dataset to a new object in this case, but we could also overwrite the existing one:

```{r}

# Notice the use of the double-equal == operator to compare equality (select if country code is equal to 826)

evs.slim.gb <- evs.slim %>% filter(country==826)
```

We could also make the selection using the variable label instead of the numeric code, but because the 'country' variable is currently stored as a labelled **numeric** vector, that wouldn't work straight away because operations are performed on the *values* rather than the *labels* (read all there is to know about vector types in ***R*** in [R for Data Science](https://r4ds.had.co.nz/vectors.html?q=typeof#vector-basics)). Instead, we need to convert the variable to another type first, such as **character** (i.e. text) or labelled **factor** (i.e. categorical) variable (the `sjlabelled` package has functions to replace values of a variable with their associated value labels; read more about [`as_factor()`](https://rdrr.io/cran/sjlabelled/man/as_factor.html) and [`as_label()`](https://rdrr.io/cran/sjlabelled/man/as_label.html). 

Setting the appropriate variable types for statistical analysis is a separate and complex data wrangling step and we'll loot at it in more detail below. But it's often useful to constrain variables to a given type as part of other data manipulation tasks; for example, here we can constrain the <tt>country</tt> variable to be of type **character**/**label** for the purpose of `filter()`ing, so that we can refer to its levels by their label, without actually changing its type in the dataset:

```{r eval=TRUE}

evs.slim.gb <- evs.slim %>%
  mutate(country = as_numeric(country)) %>% 
  mutate(country = as_character(country)) %>%  # alternatively: "as_label()"
  filter(country=="Great Britain")

```

In the above code chunk lines, we:
- assign a new object name by taking an existing object and piping it worward
- then transform the "country" variable from a Factor (categorical) format to a numeric format first; this is an important step in this particular context, because we want the order of the labels to become fixed first to the numeric values;
- then we transform the "country" variable from a Numeric format to Character/Label format, so that we can refer to its levels using the label texts rather than the numeric codes
- and finally we select only the cases where the country is equal to "Great Britain".

To combine more than one item, such as selecting not one but several countries (e.g. Great Britain and Germany), we can use the `%in%` operator and the `c(...)` function:

```{r}

# Select if country label/name is equal to one of the labels/names listed inside c(...)

evs.slim.gb_de <- evs.slim %>% 
  mutate(country = as_numeric(country)) %>% 
  mutate(country = as_character(country)) %>%
 filter(country %in% c("Great Britain", "Germany"))
```

The 'filter' command is highly flexible and also allows to combine multiple criteria if needed, say only British respondents aged between 25 and 35:

```{r}

evs.slim.gb_de <- evs.slim %>% 
  mutate(country = as_numeric(country)) %>% 
  mutate(country = as_character(country)) %>%
  filter(country == "Great Britain",
         age >= 25 & age <= 35
         )
```

## I don't need all the objects in my Environment

Saving different versions of the transformed **evs** dataset under different names in the working memory is useful for keeping track of intermediary transformations and cross-checking the changes we have made. But after a while the Environment becomes cluttered with objects that we no longer need. We can remove redundant objects from the Environment using the `rm()`/`remove()` function. Within the function we can list the objects we want to remove (e.g. `rm(evs.slim.gb_de, evs.slim)`; or we can remove all objects using `rm(list = ls())` (here the `ls()` function writes a list of all the objects in the Environment, then `rm()` is applied to that list); or we can remove all objects apart from one/few selected ones that we keep:

```{r}

# Keep only the 'evs.slim.gb' data frame object

rm(list = setdiff(ls(), "evs.slim.gb"))
```

## Quick variable tabulations

The `count()` and `summarise()` functions (from `dplyr`) come handy for quick tabulations of single variables. For example:

```{r eval=TRUE}

evs.slim.gb %>% count(v31)
```

The same frequency table can be produced with a combination of `group_by()` and `summarise()`:

```{r}

evs.slim.gb %>% group_by(v31) %>% summarise(n = n())
```

This latter option is a bit of an overkill compared to `count()`, but has the advantage that it is very flexible and can be expanded to achieve complex summaries by grouping variables, as we'll see later.

In either case, to see value labels instead of numeric values, we could coerce the variable type to be treated for the purpose of this operation as a labelled 'factor' variable:

```{r eval=TRUE}

evs.slim.gb %>% 
  mutate(v31 = as_numeric(v31)) %>% 
  mutate(v31 = as_label(v31)) %>% 
  count(v31)

# or equivalently:
# evs.slim.gb %>% count(as_label(as_numeric(v31)))
```

Functions such as `frq` (from `sjmisc`) can produce more complex frequency tables that handle labelled variables better for exploratory purposes.

Let's look at the output below for example:

```{r eval=TRUE}

evs.slim.gb %>% frq(v31) ## or "sjmisc::frq" if the package is lot loaded in the library

```

We see more clearly in this table, for instance, that levels (categories) coded with negative values (-1 to -10) can be excluded from the analysis by setting them to NA (missing). Two missing (negative) values have a few cases, but the majority of the other negative-coded optional labels do not have any cases on this variable. 

Recoding variables involves a lot of subjective decision-making; for example, in some contexts it may make sense to include "don't know" answers into the analysis because *not knowing* something may have a substantive interpretation relevant to the research question. In the case above, however, the small number of "not knowers" can safely be excluded from the analysis. We'll look at how to recode variables in a moment, but let's move from general to specific and first address the issue of having very uninformative variable names.

## I need to rename my variables

It's always a good idea to rename the variables you want to use to names that are more intuitive to you. Often, even when variables have apparently more human-readable names, they may not accurately reflect what the variable entails.

There are several convenient options for renaming variables (columns), such as the `rename()` and `rename_with()` functions in `dplyr` or the `var_rename()`/`rename_variables()` function in `sjmisc`. The main difference is that in case of `rename()` the command is *new variable name* = *old variable name*, whereas in `var_rename()` it is the other way around, *old* = *new*.

Let's rename the **v31** variable we tabulated earlier (labelled: "people can be trusted/can't be too careful (Q7)" ) to **ppl.trust.yn** (to my mind some shorthand for "people can be trusted, yes or no"). The new name will be longer to type, so it's a trade-off between brevity and clarity. Unless you are very intimately familiar with a survey and its variable naming conventions, I recommend erring on the side of clarity:

```{r eval=TRUE}

evs.slim.gb <- evs.slim.gb %>%
  rename(ppl.trust.yn = v31)

# Or using the "var_rename" function form the already loaded "sjmisc" package:
# ... %>% sjmisc::var_rename(v31 = ppl.trust.yn)
```

We can rename several variables in one go. For example, let's rename **<tt>v54</tt>** to **<tt>rel.now</tt>**, **<tt>v55</tt>** to **<tt>rel.as.child</tt>**, **<tt>v102</tt>** to **<tt>pol.lr</tt>**, and **<tt>v225</tt>** to **<tt>sex</tt>**:

```{r eval=TRUE}

evs.slim.gb <- evs.slim.gb %>% 
  rename(
    rel.now = v54,
    rel.as.child = v55,
    pol.lr = v102,
    sex = v225
    )

# Note: Break down command chains into separate lines to make complex code easier to read and troubleshoot
```

We now have more intuitive variable names, but as we transform/recode our variables it may come useful to rename some of them again later to better reflect their actual meaning. Also, note that we haven't renamed all the variables - <tt>v33</tt>, <tt>v35</tt>, <tt>v36</tt> and <tt>v37</tt> remain unchanged - and that's for a reason, as we'll see in a moment.

## I need to assign missing (NA) values

We've seen in the tables above that negative values need recoding to missing values, so that they are not included in our statistical analyses. Missing values in ***R*** are denoted with *NA*. Let's recode the variable we have just renamed to **<tt>ppl.trust.yn</tt>**, and which we have tabulated earlier [Code bloc 21]. The `set_na()` function from `sjlabelled` is very useful for this purpose:

```{r eval=TRUE}

evs.slim.gb <- evs.slim.gb %>% 
  set_na(ppl.trust.yn, na = c(-1:-10))

# Adding option 'as.tag = TRUE' let's us store the labels of the missing values, just in case we'll need them again

```

The result is:
```{r eval=TRUE}
evs.slim.gb %>% frq(ppl.trust.yn)
```


Instead of recoding variables individually, we can recode all the negative values in a dataset to missing:

```{r eval=TRUE}

evs.slim.gb <- evs.slim.gb %>% 
  set_na(na = c(-1:-99))
```

## I need to recode variables

Recoding variables can cover many practical needs. Here, let's cover four very common requirements that often emerge in the manipulation of survey-based social science data: (1) reordering the values of ordinal variables; (2) collapsing categories; (3) shifting the range of values; and (4) recoding numeric scales into equal-rnaged groups.

In order to maintain and manage value and variable labels, we will use some functions from the `{sjmisc}` package, although similar operations can be done using `{dplyr}` and base ***R*** operations.   

### (1) reordering the values of ordinal variables

We can very easily reverse the value order in ordered variables using the `rec()` function. In our data, we have several variables that fall into this category (<tt>v33</tt> to <tt>v37</tt>, <tt>rel.now</tt> and <tt>rel.as.child</tt>). Additionally, we can also reverse the order of values of binary variables (e.g. <tt>ppl.trust.yn</tt>). Before actually altering our data, we can check what the results will be:  

```{r eval=TRUE}

evs.slim.gb %>% 
  sjmisc::rec(v33, rec = "rev") %>% 
  frq(v33, v33_r)
```


```{r eval=TRUE}

evs.slim.gb %>% 
  sjmisc::rec(rel.now, rec = "rev") %>%  
  frq(rel.now, rel.now_r)
```

Once we know that the results are what we wish, we can reverse all variables at once, and setting the 'suffix' to empty ("") overwrites the existing variables (by default, new variables suffixed by "_r" would be created, which is often the safer option; we can also set our own suffixes):

```{r eval=TRUE}

evs.slim.gb <- evs.slim.gb %>% 
  sjmisc::rec(ppl.trust.yn, v33:v37, rel.now, rel.as.child, rec = "rev", suffix = "")
```

### (2) collapsing categories

The `sjmisc::rec()` function from `{sjmisc}` allows specifying in detail how values should be recoded. There are many options appropriate for different purposes, so [read more about the function here](https://rdrr.io/cran/sjmisc/man/rec.html). In this example, let's recode the two 'religiosity' variables so that we collapse their 7 categories to three: rarely (1, 2, 3), moderately (4, 5), often (6, 7):

```{r eval=TRUE}

evs.slim.gb <- evs.slim.gb %>% 
  sjmisc::rec(rel.now, 
              rel.as.child, 
              rec = "1:3=1 [Rarely]; 4,5=2 [Moderately]; 6:max=3 [Often]")

# Note that:
# each recode pair has to be separated by a ;
# multiple old values that should be recoded into a new single value may be separated with comma
# minimum and maximum values are indicated by min (or lo) and max (or hi)
# a value range is indicated by a colon
# value labels for new values can be assigned inside the recode pattern by writing the value label in square brackets after defining the new value in a recode pair; otherwise they can be done separateky after the recode too
# In the code below I'm using a combination of these on purpose for demonstration
```

This is the result, comparing the old variable with the recoded one:

```{r eval=TRUE}

evs.slim.gb %>% 
  frq(rel.now, rel.now_r, rel.as.child, rel.as.child_r)
```


### (3) shifting the range of values

The `recode_to` function from `{sjmisc}` is a very comfortable option for recoding binary variables to 0 and 1, while keeping the original value labels. In fact, what it does is to shift the category values to start form any "lowest" value we supply, or to end with any "highest" value, so it is not restricted to binary variables. 

For example, we can recode to 0/1 dummies our <tt>*ppl.trust.yn*</tt> and <tt>*sex*</tt> variables, making it easy to include them in a regression model as indicator variables:

```{r}

evs.slim.gb <- evs.slim.gb %>% 
  sjmisc::recode_to(ppl.trust.yn, sex, lowest = 0, suffix = "")

# Note: by default, new variables are created with the "_r0" suffix; to overwrite the original variables, we can specify the option 'suffix = ""'
# The setting 'lowest = ' is by default set to 0 so it could be excluded

```

The result, compared to the original variables, would be:
```{r eval=TRUE, echo=FALSE}

evs.slim.gb %>% frq(ppl.trust.yn, sex)

evs.slim.gb <- evs.slim.gb %>% 
  sjmisc::recode_to(ppl.trust.yn, sex, lowest = 0, suffix = "")

evs.slim.gb %>% frq(ppl.trust.yn, sex)
```

Recoded to 0/1 values, these variables are now more clearly 'indicator' variables, where the value 1 indicates the meaning of the variable. To make it more explicit for ourselves, we could then rename them to something like <tt>*Trusting*</tt> and <tt>*Female*</tt>, respectively:

```{r eval=TRUE}

evs.slim.gb <- evs.slim.gb %>% 
  sjmisc::var_rename(
    ppl.trust.yn = Trusting,
    sex = Female
    )

# Or: dplyr::rename(newname = oldname)
```


## I need to compute a new variable

We often need to create new variables based on the values of other variables. In our case, we would find it useful to create a numeric scale measuring the theoretical construct <tt>*trust in strangers*</tt>, which is our variable of interest in this hypothetical analysis. We have in the dataset four variables that can be taken to measure trust in strangers (<tt>*v33*</tt>, <tt>*v35*</tt>, <tt>*v36*</tt> and <tt>*v37*</tt>, which we haven't renamed on purpose, because we will not use them individually, and which are measured on 4-point ordinal scales running from 1 to 4), and the <tt>*Trusting*</tt> variable, which is binary. 

There are many ways to go about this and it is ultimately a theoretical and substantive decision. For instance, one can decide to compute a summative scale, where the values of individual items are simply added up, or an averaging scale, where the mean across the items are taken. Missing data also needs to be thought about when computing scales. A participant may be missing data on all items or on just a subset. A decision needs to be made about what to do in both cases.

For our purposes, we'll create a variable that sums up the values on the five variables we want to use to measure <tt>*trust in strangers*</tt>; to give more weight to the general dichotomous social trust variable (<tt>*Trusting*</tt>), we'll make it's indicator value (1) equal to the highest value of the other component variables. We'll also set the scale to start from 0 and exclude cases with missing answers (NA) on three or more of the component variables.

Let's do the following operations in one pipeline:
  1. Shift the coding range of all component variables to start from 0 (as in the previous example); the scales will then be 0 to 3;
  2. Keep only observations that have missing response (NA) on fewer than three out of the five variables;
  3. Create a new numeric variable (`as.numeric`) called "trust_in_strangers" by ...
    3.1. multiplying the value of <tt>*Trusting*</tt> by 3 (0/1 will become 0/3) and ..
    3.2. adding up the values of the five variables

```{r eval=TRUE}

evs.slim.gb <- evs.slim.gb %>%                                        # Steps:
  sjmisc::recode_to(v33:v37, lowest = 0, suffix = "") %>%             # 1 
  filter(rowSums(across(c(Trusting, v33:v37), ~ is.na(.))) < 3) %>%   # 2 
  mutate(trust_in_strangers = as.numeric(                             # 3 
                (Trusting * 3) +                                      # 3.1
                  v33 + v35 + v36 + v37))                             # 3.2
```  

Step (2) is a bit convoluted, using the `rowSums` base ***R*** function to count the missing values (is.na) `across()` the specified variables, and `filter()` out observations where this count is higher than or equal to 3.

The newly created variable <tt>*trust_in_strangers*</tt> looks like this:

```{r eval=TRUE}

# Type of variable:
class(evs.slim.gb$trust_in_strangers)

# Number of observations:
length(evs.slim.gb$trust_in_strangers)

# Summary statistics:
summary(evs.slim.gb$trust_in_strangers)
```

The `summary` function from base R is useful for getting a quick summary of numeric variables with information such as the minimum and maximum values, the mean, median, number of NAs.

# All ready for analysis

The data is now ready to undertake the regression that we planned. The `lm` function is a base-R function for linear modeling, and the `summary` function lets us summarise the model we fit. We would usually save the model results as an object so that we can perform further analysis on it, so we would write something like:

```{r eval=TRUE}
my_linear_model <- lm(trust_in_strangers ~ age + Female + rel.now + rel.as.child + pol.lr, data = evs.slim.gb)

summary(my_linear_model)
```


Our results can then be written as:

```{r eval=T, echo=FALSE}
lm(trust_in_strangers ~ age + Female + rel.now + rel.as.child + pol.lr, data = evs.slim.gb) %>% 
  equatiomatic::extract_eq(wrap = TRUE, use_coefs = TRUE)
```

