[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html#aims",
    "href": "index.html#aims",
    "title": "HOME",
    "section": "Aims",
    "text": "Aims\nSociology is based on systematic knowledge about the social world that we inhabit. ‘Researching Social Life I’ introduces the range of ways that sociologists do research by collecting empirical information so that they can develop their sociological ideas. Its core theme is the importance of evidence: the way we collect and analyse information has a huge effect on our research findings. Data analysis is a practical activity, and therefore the module is distinctive in involving a series of hands-on computer labs and seminars, as well as being focused on practical assignments. It is in these applied sessions that you will convert the principles explained in the lectures into skills, so that you too can carry out research. Although concerned to convey the systematic application of appropriate professional standards, the module also tries to communicate some of the excitement and fun of doing research. The module makes four main contributions to the degree program:\n\nIt lays a foundation of knowledge and critical awareness about how research gets done, which helps to appreciate and make sense of the other sociological sources used in the rest of the degree’s modules.\nIt enables the best choice of research methods to be made for doing your research for the Final Year Dissertation in sociology.\nMore generally, the informed and critical thinking learned in the module can also be applied to analyse what really lies behind media news-stories, politicians’ speeches, lobby groups’ reports on social problems – and even gossip – that we all encounter as citizens.\nFinally, because Research Social Life I delivers a substantial part of “what every sociology graduate can be expected to know”, having studied the subject, it offers the opportunity to acquire transferable skills for later employment in a range of professions. These include interviewing, analysing social behaviour, using computers to process quantitative information, where to locate data on public issues and how to apply them, and how to make sense of social surveys."
  },
  {
    "objectID": "index.html#teaching-methods-2022-2023",
    "href": "index.html#teaching-methods-2022-2023",
    "title": "HOME",
    "section": "Teaching methods (2022-2023)",
    "text": "Teaching methods (2022-2023)\nThe main practical content will be delivered in IT labs (seminars). These will be supplemented with 10 recorded lectures and 5 timetabled drop-in on-campus Q&A sessions focusing particularly on the assessments. Students are required to complete some self-study online training sessions as well."
  },
  {
    "objectID": "Materials/index.html",
    "href": "Materials/index.html",
    "title": "Materials",
    "section": "",
    "text": "Materials for each week are available from the side menu.\n\n\n\n Back to top"
  },
  {
    "objectID": "Materials/Labs/index.html",
    "href": "Materials/Labs/index.html",
    "title": "Worksheets",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "Materials/Labs/Worksheet_1.html",
    "href": "Materials/Labs/Worksheet_1.html",
    "title": "Worksheet 1",
    "section": "",
    "text": "Timetable week: 5\nTopic: \"Taming information\"\n\n\nIntro\nWe begin our exploration of sociological research methods by looking at some real data that social scientists have used in empirical research. Before starting to think in a more structured way about research questions, how to design a sociological research project, and how to create useful instruments to capture and collect relevant information about the social world, let’s look at some ‘semi-tamed’ information: data collected by others for academic and/or policy analysis purposes.\n\n\n\nExercise 1: Exploring the UK Data Service\nAbout 25 minutes + 5 minutes discussion\n\nThe UK Data Service (UKDS) is the country’s largest data repository. It makes a variety of data available to UK researchers for secondary analysis, and we will be using data accessible through this repository throughout this module (and for the assignments!).\nIt is also useful to become acquainted with the variety of secondary data available there and the basic functionalities of the site, as you may decide to use secondary data for your final third-year dissertation.\nThe UKDS is available at this website: https://ukdataservice.ac.uk/\nBegin by exploring the options available under the Find data tab of the UKDS website. The Find data page contains a short video on How to use the UK Data Service catalogue search tool, which you can watch outside class. As we are just exploring the data offerings of the service, navigate to the Browse and access data page, where you can browse data by theme or type, among some other options. Let’s choose to Browse by data type and select UK Surveys.\nThis takes us to the main data catalogue, where we can see that the search is filtered down to “UK Survey data” in the Data Type field of the menu on the left. We can perform various selections using that menu, refining the search by date or setting other filters.\n\n\n\nQuestion 1\n\n\nLook at the information on the UKDA Data catalogue page and answer:\n\n\n\nHow many studies are available on the UKDA in total?\n\n\nHow many data series are available? (Series are large surveys that are taken regularly; they can be longitudinal or time series)\n\n\nHow many studies categorised as UK Survey data are there?\n\n\n\nTo check whether you’ve found the correct information, you can check the answers below.\n\n\n\nClick to view answer\n\n\nThe numbers shown in the main tab refer to the overall number of studies available through the UKDA, so at the time of writing 8937 studies and 79 series.\n\n\nThe number of results in the light blue box above the results list refer to the number of studies based on our set filter, so in this case 4891 in UK Survey data.\n\n\n\n\nTake some time to explore and become familiar with the page. Try to:\n\nFind the newest studies based on their date of collection (tip: check out Sort by)\nRemove the filter on data type and scroll through some of the most recently released datasets\nSearch a keyword of your own interest in the search field (Search our data catalogue) (e.g. ‘migration’, ‘gender violence’, or any topic you are currently interested in). How many studies did you find?\nChoose one study that you find most interesting and read through the details. Write down a few notes on the aims, methodology, etc. employed:\n\nWhat was the main aim of the study? (usually stated somewhere in the abstract)\nWhen was the study conducted and where?\nWhat kind of data was collected? (e.g. qualitative, quantitative, mixed, etc.)\nWhat was the sampling methodology?\nWhat were the methods of data collection?\nUnder what conditions are the data available? (tip: click on Access data) \\(\\rightarrow\\) read more about data access conditions here\n\nAre there any Series that matched your keyword? This is useful information because the Series contain data from large-scale publicly funded surveys with publicly available data that can be used for secondary analysis.\n\n\n\n\nDiscussion\n\n\nShare what you have found out about your chosen study with the group.\n\n\n\n\n\nExercise 2: Find qualitative and mixed data\nAbout 30 minutes + 10 minutes discussion\n\nMost of the secondary data available is quantitative. But we can find some qualitative and mixed methods studies too, some of which have made their data (e.g. transcribed and anonymised interview data) available. We can find such studies if we filter by data type. Give it a go - but remember to clear your previous search to find all qualitative and mixed methods studies, not only the ones relevant to your keyword.\n\n\n\nQuestion 2\n\n\nHow many qualitative and mixed methods studies are available on the UKDA in total?\n\n\n\nClick to view answer\n\n\nAt the time of writing there were 1585\n\n\n\n\n\n\n\nDiscussion\n\n\nWhy is there less qualitative data made publicly available?\n\n\n\nYou may have found some studies that you find interesting based on their titles. You can have a closer look at them outside class, but because not all datasets are equally well documented and made available, let’s look at a few studies that I find particularly interesting:\n\nStetka, Vaclav and Mihelj, Sabina and Toth, Fanni and Kondor, Katherine (2022) “The Illiberal Turn? News Consumption, Polarization and Democracy in Central and Eastern Europe, 2019-2020” https://doi.org/10.5255/UKDA-SN-855088\nWrighton, Sam J (2021) “The Annexation of Populations: A New State Strategy, 2017-2021” https://doi.org/10.5255/UKDA-SN-854598\nEinarsdottir, Anna and Mumford, Karen and Birks, Yvonne and Lockyer, Bridget and Sayli, Melisa and Jeep, Sudthasiri (2021) “LGBT+ Networks, 2017-2020” https://doi.org/10.5255/UKDA-SN-855322\nPearce, Sioned and Fox, Stuart (2020) “Young people and Brexit 2017” https://doi.org/10.5255/UKDA-SN-854031\n\nWrite down a few notes on the aims, methodology, etc. employed, as you have done in the previous exercise.\n\n\n\nDiscussion\n\n\nWhat are the most interesting aspects of these studies? (i.e. things that really caught your attention)\n\n\n\nIf one of the studies has caught your attention, make a note of it, as we will use some data from them later for the purposes of the first assessment.\n\n\nExercise 3: Explore the Understanding Society panel study\nAbout 20 minutes + 5 minutes discussion\n\nAfter looking at some characteristics of qualitative and mixed-methods studies, let’s have a quick look at a highly complex survey: The UK Household Longitudinal Study (UKHLS), a.k.a. Understanding Society. This survey is the UK’s largest panel study, offering sociological insight into continuous developments in British society.\nFind the study on the UKDS site among the Series (tip: the study’s code number is SN 2000053) and read through some general information about it in the FAQ’s. The study has it’s own separate website that contains a lot more information and is worth having a look at (www.understandingsociety.ac.uk/). It’s useful to start familiarising yourself with the study because we will be using data from it later on in the module and for the second assessment.\nThere is a lot of information on the survey website! Eventually, you’ll become a bit more familiar with the questionnaires used and the resulting variables; but for now, it may be interesting to scroll through some of the publications that have used data from Understanding Society to get a feel for what researchers using this type of data are interested in: https://www.understandingsociety.ac.uk/research/publications. Find a few publications that sound interesting and check their summaries for some more information.\n\n\n\nDiscussion\n\n\nWhat was/were the most interesting publication/s you have found, and why do you find them interesting?\n\n\nWhat are your initial thoughts on the differences between the types of analysis that such survey data provides compared to the data from the qualitative/mixed studies you read about earlier?\n\n\n\n\n\nExercise 4: Explore some cross-national surveys\nIf there is time left, in class; if not, outside class\n\nThe Understanding Society survey allows us to study changes in British society over time. There are other large-scale social surveys that are conducted internationally and allow researchers to compare different societies. There is a Wikipedia article that lists many such surveys. Some of the most generally relevant to social scientists are:\n\nThe European Social Survey (ESS)\nThe European Values Study (EVS) and its twin sister The World Values Survey (WVS)\nThe International Social Survey Programme(ISSP)\n\nHave a quick look at the description pages linked above to gain a sense of what these surveys are about.\n\n\nOutro\nWe have now seen some examples of social science data and have learnt how to find out more about them and how to access them. Thinking ahead to your third-year dissertation, you may want to register for a free user account on the UKDS or some of the international comparative surveys. The UKDS provides some great resources and advice on using their data for undergraduate dissertations and there’s also an annual Dissertation Award! So, keep this great option in mind.\nNext week we will be thinking more closely about designing a research project starting from a relevant research question (another step in the process of “taming information”), sampling considerations, case selection, data collection methods and so on.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Materials/Labs/Worksheet_2.html#task-1-open-an-rstudio-project",
    "href": "Materials/Labs/Worksheet_2.html#task-1-open-an-rstudio-project",
    "title": "Worksheet 6",
    "section": "Task 1: Open an RStudio project",
    "text": "Task 1: Open an RStudio project\nAn RStudio Project stores all the files and data objects and performs all the operations relative to the location of the project’s root file on the computer.\nTo create a new R Project select File &gt; New Project from the top RStudio menu.\nCreating a new R project will create:\n\nA new project directory (folder)\n\nA R project file (.Rproj) in the project folder that serves as a shortcut to open the project via RStudio\n\nNote:\n\nYou can make sub-folders for data, scripts, etc.\n\nAll files produced by R scripts saved to the project will also save into the project folder\n\nYou can try creating a new R Project, but for simplicity, you can download a folder already set up as an R Project from here: https://cgmoreh.github.io/SOC2069/SOC2069-Statistical-analysis.zip\n\nDownload the compressed folder into a suitable location on your OneDrive - Newcastle University drive. This will allow you to access your files from any computer by logging into your university OneDrive account.\nOnce downloaded, use the Compressed Folders Tools to extract the contents of the compressed folder.\n\nThe contents of the downloaded folder should look like this:\n\nIf you double-click on SOC2069-Statistical-analysis.Rproj, the project will open in a new RStudio window. The advantage of having an .Rproj file in the folder that you are using is that you don’t have to worry about setting manually any paths to files that you will be using or producing; everything is stored relative to where the .Rproj file is located on your computer."
  },
  {
    "objectID": "Materials/Labs/Worksheet_2.html#task-2-using-r-scripts",
    "href": "Materials/Labs/Worksheet_2.html#task-2-using-r-scripts",
    "title": "Worksheet 6",
    "section": "Task 2: Using R scripts",
    "text": "Task 2: Using R scripts\nR scripts are special text documents where you can write and execute commands in the R programming language. You can create an R Script from within RStudio by going to File &gt; New File &gt; R Script (also CNTRL + SHIFT + N). We already have an example R script in the folder you have just downloaded: Lab6.R. You can click on that file to open it. It will load in the upper left panel of RStudio. This example R script contains mainly comments and a single command, a simple arithmetic operation (1 + 3 - 5 + 7). You can use this script file to copy/paste commands from this window into it so you can run the commands in R."
  },
  {
    "objectID": "Materials/Labs/Worksheet_2.html#task-3-explore-the-rstudio-main-panes",
    "href": "Materials/Labs/Worksheet_2.html#task-3-explore-the-rstudio-main-panes",
    "title": "Worksheet 6",
    "section": "Task 3: Explore the RStudio (main) Panes",
    "text": "Task 3: Explore the RStudio (main) Panes\nLet’s have a closer look at the four (main) panes of RStudio:\n\n\nThe R Console Pane\nThe R Console, by default the left or lower-left pane in R Studio, is the home of the R “engine”. This is where the commands are actually run and non-graphic outputs and error/warning messages appear. The Console is the direct interface to the R software itself; it’s what we get if instead of RStudio we open the R software: a direct interface to the R programming language, where we can type commands and where results/messages are printed.\nYou can directly enter and run commands in the R Console, but realize that these commands are not saved as they are when running commands from a script. For this reason, we should not use the Console pane directly too much. For typing commands that we want R to execute, we should instead use an R script file, where everything we type can be saved for later and complex analyses can be built up.\n\n\nThe Source Pane\nThis pane, by default in the upper-left, is a space to edit and run your scripts. This pane can also display datasets (data frames) for viewing.\n\n\nThe Environment Pane\nThis pane, by default the upper-right, is most often used to see brief summaries of objects in the R Environment in the current session. These objects could include imported, modified, or created datasets, parameters you have defined, or vectors or lists you have defined during analysis. You can click on the arrow next to a dataframe name to see its variables.\n\nNote\nIf your Environment pane is empty, it means that you don’t have any “objects” loaded or created yet. We will be creating some objects later and we will also import an example dataset.\n\n\n\nFiles, Plots, Packages, Help, etc\nThe lower-right pane includes several tabs including plots (display of graphics including maps), help, a file library, and available R packages (including installation/update options).\nYou can arrange the panes in different ways, depending on your preferences, using Tools &gt; Global Options in the top menu. So the arrangement of panes may look different on different computers."
  },
  {
    "objectID": "Materials/Labs/Worksheet_2.html#task-4-start-using-functions-and-user-written-packages",
    "href": "Materials/Labs/Worksheet_2.html#task-4-start-using-functions-and-user-written-packages",
    "title": "Worksheet 6",
    "section": "Task 4: Start using functions and user-written packages",
    "text": "Task 4: Start using functions and user-written packages\nMost work in R is done using Functions. The most common operations involving a function take the following generic form (think of an analogy of baking a loaf of bread):\n\nIt’s possible to create your own functions. This makes R extremely powerful and extendable. But instead of programming our own functions, we can rely on functions written by other people and bundled within packages. There are a large number of reliable, tested and oft-used packages containing functions that are particularly useful for social scientists. In this module, we will rely on several such user-written packages that extend the basic packages already bundled in with our R software (the so-called base-R packages and functions).\nPackages are often available from the Comprehensive R Archive Network (CRAN) or private repositories such as Bioconductor, GitHub etc. Packages made available on CRAN can be installed using the command install.packages(\"packagename\"). Once the package/library is installed (i.e. it is sitting somewhere on your computer), we then need to load it to the current R session using the command library(packagename).\n\nInstalling and loading the tidyverse\nLet’s begin by installing and loading one of the most useful suite of packages called the ‘tidyverse’ (you can read more about the tidyverse here). Type or copy the command below into the Console window and click Enter (or write it in the Lab6.R script file and run it from there; in fact, this command is already there in the script):\n\ninstall.packages(\"tidyverse\")\n\nlibrary(tidyverse)\n\nWe now have access to functions contained in the ‘tidyverse’ bundle of packages. We can check the suite of packages that are loaded with the tidyverse library using a command from the tidyverse itself:\n\ntidyverse_packages()\n\nIf we don’t want to load a package that we have downloaded - because maybe we only want to use a single function once and we don’t want to burden our computer’s memory - we can state explicitly which package the function is from by using the form package::function, like this:\n\ntidyverse::tidyverse_packages()\n\nI will sometimes use this form in the worksheets to clarify what package a function originates from, even if the package is loaded in the library.\n\n\nInstalling some other packages\n\nNow that you’ve installed a package, write the required functions in your R scripts to install and load the following two packages that we will be using later in this session:\n\nmosaic\nsjmisc\n\n\n\n\nLoad a dataset\nSo far we have learnt about some useful functions for installing and loading R packages. We can now look at functions that can be used to load a dataset into the Environment pane. We will load a dataset stored in R’s native format: .rds. There are other functions that are useful for loading data stored in other formats, particularly the most commonly used generic comma separated values (.csv) format, but we won’t use them now.\nLet’s load the dataset called ukhls_w8.rds that is included in the SOC2069-Statistical-analysis project folder you downloaded earlier. We’ll use the readRDS function and the assignment operator (&lt;-) to create an object called “data” that will store the dataset (we can give any name to the object, but it’s useful to have something short that is easy to type because we will be typing it a lot when writing commands targetted to that dataset). Copy the following command into your R script and run it from there:\n\ndata &lt;- readRDS(\"ukhls_w8.rds\")\n\nThis will only work if you are within an R Project; otherwise you need to provide the complete file path to the dataset.\nYou can now see that an object called “data” was created in the Environment pane, and next to it we have some information about the number of observations and variables in the dataset. If we click on the blue button with the white arrow before the name of the object, a list of variables and other information about them will roll down. If we click on the object’s name or info, the dataset will open in the Sources pane, just next to the R script file. This is equivalent to having run the following command:\n\nView(data)    \n\n# Note the capital \"V\"; R is case-sensitive, so always pay attention; view(data) won't work\n\nYou can explore the dataset a bit. Only the first 50 columns (i.e. variables) are displayed, to see the next 50 you can click on arrow (&gt;) in the dataset window’s toolbar. Once you’ve had a quick look, you can close that view or return to the R script."
  },
  {
    "objectID": "Materials/Labs/Worksheet_2.html#task-3.1-tabulate-categorical-variables",
    "href": "Materials/Labs/Worksheet_2.html#task-3.1-tabulate-categorical-variables",
    "title": "Worksheet 6",
    "section": "Task 3.1: Tabulate categorical variables",
    "text": "Task 3.1: Tabulate categorical variables\nMost of the variables in the dataset are categorical, so tabulating the frequency distributions of their categories can come handy. There are various ways for doing this in R, but one of the most convenient options is to use the frq function from the {sjmisc} package.\nLet’s look, for instance, at the variable sex:\n\ndata %&gt;% sjmisc::frq(sex)\n\nLet’s decipher the code above:\n\nfirst, we assume that there is an object called ‘data’ in our Environment that contains the ukhls_w8.rds; unless we restarted RStudio or deleted it, the object should still be there from the first exercise;\nthe %&gt;% operator (called a pipe) is a function loaded with the tidyverse and allows objects and functions to be fed (i.e. piped) forward to other functions. Getting into the habit of using the “pipe” workflow is particularly useful as it makes combining a series of operations/commands easier to read and follow. Here, we take our dataset and pipe it forward so that we can easily refer to variables from that dataset (such as sex);\nthe remaining code applies the frq function from the {sjmisc} package to the variable sex\n\nWe could have written the following command, with the same result:\n\nsjmisc::frq(data$sex)\n\nOr if we already have the {sjmisc} package loaded with the library command, then:\n\nfrq(data$sex)\n\nWhat we learn here is that another way of referring to variables within datasets is by using the $ operator. Here, data**$**sex extracts the sex variable/column from the data object.\nThe resulting frequency table will be printed in the Console, and will look something like this:\n\n\nsex (x) &lt;categorical&gt; \n# total N=39293 valid N=39293 mean=1.54 sd=0.50\n\nValue  |     N | Raw % | Valid % | Cum. %\n-----------------------------------------\nmale   | 17925 | 45.62 |   45.62 |  45.62\nfemale | 21368 | 54.38 |   54.38 | 100.00\n&lt;NA&gt;   |     0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\n\n\n\nQuestion\n\n\nInterpret the table:\n\n\n\nHow many of the respondents in the data are female?\n\n\nWhat is the percentage of men in the dataset?\n\n\nAre there any missing values (NAs) on this variable?"
  },
  {
    "objectID": "Materials/Labs/Worksheet_2.html#task-3.2-summarize-numeric-variables",
    "href": "Materials/Labs/Worksheet_2.html#task-3.2-summarize-numeric-variables",
    "title": "Worksheet 6",
    "section": "Task 3.2: Summarize numeric variables",
    "text": "Task 3.2: Summarize numeric variables\nThere are much fewer numeric variables in this dataset (an in sociological datasets in general). One that we have is the age_dv variable that codes respondents’ age. The base-R function summary is enough to get a basic summary of a quantitative/numeric type variable:\n\nsummary(data$age_dv)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  16.00   34.00   49.00   48.77   63.00  102.00 \n\n# Base R functions don't always work with a pipe ( %&gt;% ) workflow; summary doesn't, so we use the `$` operator\n\nWe can also use the descr function from {sjmisc}:\n\ndata %&gt;% sjmisc::descr(age_dv)\n\n\n## Basic descriptive statistics\n\n    var    type                                  label     n NA.prc  mean    sd\n age_dv numeric Age, derived from dob_dv and intdat_dv 39293      0 48.77 18.73\n   se md trimmed       range iqr skew\n 0.09 49   48.53 86 (16-102)  29 0.09\n\n\nThis function allows more control, so we can fine-tune which statistics we want to print and ad some other options. For example:\n\ndata %&gt;% sjmisc::descr(age_dv, show = c(\"n\", \"NA.prc\", \"mean\", \"sd\", \"md\", \"range\"))\n\n\n## Basic descriptive statistics\n\n    var     n NA.prc  mean    sd md       range\n age_dv 39293      0 48.77 18.73 49 86 (16-102)\n\n\nIn this output, we selected some of the more useful statistics only and excluded the variable label. We could also use the max.length option in addition to show to set the maximum length of variable labels in the output, making it shorter if we wanted to. We can also include more than one variable in the function, which is useful for comparing several numeric variables.\nThe c(...) function used in the example above is used to combine values into vectors or lists. It is used very often in various contexts, so it is worth knowing about. Here, we used it to combine the names of the statistics we wanted to include within the show function.\n\n\n\nQuestion\n\n\nInterpret the table:\n\n\n\nWhat is the average (mean) age of the respondents in the dataset?\n\n\nWhat about the median (md) age?\n\n\nHow spread out is the age of the respondents? (tip: the standard deviation (sd) is a good measure of spread)\n\n\nWhat is the minimum and maximum age of the respondents? (tip: the range statistic contains this information)\n\n\nAre there any missing values (NAs) on this variable?"
  },
  {
    "objectID": "Materials/Labs/Worksheet_2.html#task-3.3-visualising-distributions",
    "href": "Materials/Labs/Worksheet_2.html#task-3.3-visualising-distributions",
    "title": "Worksheet 6",
    "section": "Task 3.3: Visualising distributions",
    "text": "Task 3.3: Visualising distributions\nIt is often useful to make a graph to visualise a variable, especially a numeric variable. A figure can often convey information in a much more efficient way that a statistic/number. One of the most useful graph types for visualising numeric variables is a histogram. Again, there are many functions available in R for producing graphs. Once you become more proficient and use R more often, it is very useful to learn the graphing workflow of the ggplot2 package (included in the tidyverse), which builds up a graph canvas step by step using various elements. That allows the creation of highly customized figures of publishable quality. But for very basic graphs that we use only to get a general impression, we can use some more basic functions. The plotting functions included in the mosaic package can be useful for this purpose. For example, the histogram function from {mosaic} produces the following figure:\n\nhistogram( ~ age_dv, data = data)\n\n\n\n\nThis function has a similar structure to the functions that we will use for statistical modelling, generically of the form goal(y ~ x, data = my_data). In our case, we are only “modelling” one variable here, so the first (y-axis) object before the tilde (~) is missing. The data can be specified with the option data = .... In our case, the actual name that we gave to our data object is data, so we have data = data.\nWe can also easily superimpose a normal distribution curve over the histogram to check whether the numeric variable is more or less normally distributed or not:\n\nhistogram( ~ age_dv, data = data, fit = \"normal\")\n\n\n\n\nDensity plots are also useful for this purpose:\n\ndensityplot( ~ age_dv, data = data)\n\n\n\n\nWe can also add a categorical variable to break down the graph into groups. For instance, we can use the sex variable that we are already familiar with to visualise the age distribution of men next to that of women. To do this, we add the categorical variable after a vertical bar (|):\n\nhistogram( ~ age_dv | sex, data = data, fit = \"normal\")\n\n\n\n\nWhat we can learn from all these figures is that the age distribution of our respondents is not perfectly normal, with a thicker left tail showing a higher numbers of under-30 year-olds."
  },
  {
    "objectID": "Materials/Labs/Worksheet_4.html#step-0-formulate-your-research-question",
    "href": "Materials/Labs/Worksheet_4.html#step-0-formulate-your-research-question",
    "title": "Worksheet 8",
    "section": "Step 0: Formulate your “research question”",
    "text": "Step 0: Formulate your “research question”\nThe title of the exercise is almost a reasonable sounding research question. We want to understand: _“Does gender have an effect on subjective wellbeing”? Or, formulated differently: “Are there any gender-based differences in subjective wellbeing? Or:”Are women happier than men?“.\nAll of these are similar in their aim and the methods required to answer then, but choosing one over the other means that we need to pay attention to how we interpret our results and formulate our answers later on."
  },
  {
    "objectID": "Materials/Labs/Worksheet_4.html#step-1-find-describe-and-understand-your-variables",
    "href": "Materials/Labs/Worksheet_4.html#step-1-find-describe-and-understand-your-variables",
    "title": "Worksheet 8",
    "section": "Step 1: Find, describe and understand your variables",
    "text": "Step 1: Find, describe and understand your variables\nLast week (see Lab7) we got to know how subjective wellbeing is defined and operationalised, and which variable in our UKHLS dataset measures it: scghq1_dv.\nThe other variable we need to answer our question is one that measures respondents’ gender. We have a variable in the dataset called sex that best approximates this (we’ll disregard conceptual differences between sex and gender here; we could have used sex instead of gender in our research question to be more precise, but that would have sounded a bit awkward and probably changed its meaning completely ).\nWe have already looked at the sex variable in Lab6, Task 3.1. So there’s nothing new that we need to do here, but let’s look again at some descriptive statistics for these two variables:\n\nNote\nYou have already loaded the UKHLS dataset into your RStudio Environment in Exercise 0 above. You can check the Environment pane in RStudio to make sure that the data is there (recall, we gave it the name “ukhls”).\nRemember, you can check the list of variables in the dataset on this page: https://cgmoreh.github.io/SOC2069/Data/ukhls_w8. (Tip: you can search for keywords in the usual way: ctrl+F on Windows / command+F on Mac, and search for “wellbeing”).\n\n\n\n### Complete the code below\n\n# Descriptive statistics for 'scghq1_dv':\n\n...\n\n# Descriptive statistics for 'sex':\n\n...\n\n\n\nQuestions\nExamine the descriptive results from the tables and charts you’ve produced and answer:\n\nWhat is the average (mean) subjective wellbeing score of the respondents in the dataset?\nWhat does it mean to have a score of 32 on the scghq1_dv scale? Tip: In Lab7 we checked what exactly the scghq1_dv variable measures by cross-checking on the UKHLS/Understanding Society website.\nAre there more women or men in the dataset?\nWhat is the percentage of women in the dataset?\nDo we need to transform/recode any of the variables? If so, why and how?"
  },
  {
    "objectID": "Materials/Labs/Worksheet_4.html#step-2-modify-your-variables-if-needed",
    "href": "Materials/Labs/Worksheet_4.html#step-2-modify-your-variables-if-needed",
    "title": "Worksheet 8",
    "section": "Step 2: Modify your variables (if needed)",
    "text": "Step 2: Modify your variables (if needed)\nIn Lab7 we recoded the values on the scghq1_dv variable so that higher values would equate higher subjective wellbeing. This just makes the interpretation of the variable more logical. We also renamed it to wellbeing just because the original variable name is ugly and a bother to type. We can quickly do that again here (tip: check Lab7, Ex1, Step2 for more explanation):\n\n# extract the highest value (check in the Environment if the extracted value is correct)\nmaxvalue &lt;- max(ukhls$scghq1_dv, na.rm = TRUE)    \n\n# reverse the scale\nukhls &lt;- ukhls %&gt;% \n  mutate(wellbeing = maxvalue - scghq1_dv)  \n\n# quickly compare the two variables to make sure the recoding was correct\nukhls %&gt;% \n  select(scghq1_dv, wellbeing) %&gt;% \n  summary() \n\n# We should get this output:\n\n   scghq1_dv       wellbeing    \n Min.   : 0.00   Min.   : 0.00  \n 1st Qu.: 7.00   1st Qu.:23.00  \n Median :10.00   Median :26.00  \n Mean   :11.12   Mean   :24.88  \n 3rd Qu.:13.00   3rd Qu.:29.00  \n Max.   :36.00   Max.   :36.00  \n NA's   :3351    NA's   :3351"
  },
  {
    "objectID": "Materials/Labs/Worksheet_4.html#step-3-describe-the-relationship-between-your-variables",
    "href": "Materials/Labs/Worksheet_4.html#step-3-describe-the-relationship-between-your-variables",
    "title": "Worksheet 8",
    "section": "Step 3: Describe the relationship between your variables",
    "text": "Step 3: Describe the relationship between your variables\nIn this step we’ll learn something (relatively) new. When we described the relationship between two numeric variables (wellbeing and age) in Lab7, we did it using a scatterplot to which we added a linear regression fit line (a.k.a. a line of best fit, least squares line; see Agresti (2018: 250-255), Gelman et al.(2020: 86-87)).\nUsing a scatterplot to visualise the linear relationship between a dichotomous(binary) variable (sex) and the numeric variable wellbeing is not the best approach, but it can be done.\n\nScatterplots with fit line\n\nxyplot(wellbeing ~ sex, type = c(\"p\", \"r\"), data = ukhls)\n\n\n\n\n\n\n\n\nAs with the wellbeing by age scatterplot in Lab7, it doesn’t look great because the wellbeing scale is rather small (0-36 discrete points) and the dispersion within genders is broad; with other variables it may look a bit better and more useful. But the scatter is as we would expect: since we only have two categories/groups in the variable sex (males and females), the scatter dots are vertically aligned.\nNonetheless, with a linear regression line fitted over the scatterplot we can sense that the average wellbeing score of women is somewhat lower than that of the men.\n\n\nBoxplots to compare group medians\nIt’s more appropriate to use boxplots when plotting numeric variables against categorical variables, and this is what you should use in the assignment to demonstrate that you know how to pick the most appropriate visualisation method.\nWe’ve already met the boxplot in Lab7, Ex1, Step 1, where we used it to visualise the scghq1_dv by itself, as a counterpart to a histogram. But boxplots are more useful when comparing the distributions of different categories within a categorical variable against a numeric scale (such as the wellbeing score of males vs. females within the sex variable).\nFunctions such as boxplot in base R or the bwplot from {mosaic} that we are already familiar with can be used to make boxplots. They make the following plots:\n\n\n\n# with base R `boxplot`\nboxplot(wellbeing ~ sex, data = ukhls)\n\n\n\n\n\n\n\n# with `bwplot`\nbwplot(wellbeing ~ sex, data = ukhls)\n\n\n\n\n\n\n\n# with `bwplot`, adding the setting ' pch = \"|\" ' which transforms the median dot into a line, like in `boxplot`, making the comparison easier\nbwplot(wellbeing ~ sex, data = ukhls, pch = \"|\")\n\n\n\n\n\n\n\n\n\n\nWe can also plot the boxplots horizontally, which may be more appropriate with some variables and can further ease visual comparisons. The additional settings required for each approach are:\n\n# with `boxplot`, add \"horizontal = TRUE\"\nboxplot(wellbeing ~ sex, data = ukhls, horizontal = TRUE)\n\n# with `bwplot` we can simply reverse the order of variables and place our categorical (factor, dichotomous) variable first. It's as if we'd say that we're modelling the sex variable on wellbeing, rather than the other way around.\nbwplot(sex ~ wellbeing, data = ukhls, pch = \"|\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can choose whichever option you prefer and find most useful for the variables you are modelling.\nWe can now see more clearly that there is some sex-based difference in wellbeing scores, with women reporting slightly lower wellbeing than men but also showing slightly higher standard variation in their scores.\n\n\nDescriptive statistics by group\nVisual assessment is always useful because we are visual creatures after all; but our eyes can deceive. We need to look at and compare some numbers too if we want precision.\nWe already know how to produce descriptive statistics for numeric variables, and we can easily produce them by categories of categorical variables. Let’s use the favstats command from the {mosaic} package again:\n\nmosaic::favstats(wellbeing ~ sex, data = ukhls)\n\n     sex min Q1 median Q3 max     mean       sd     n missing\n1   male   0 24     27 29  36 25.52269 5.155753 16017    1908\n2 female   0 22     25 28  36 24.36095 5.796800 19925    1443\n\n\nNote how compared to our previous use of favstats for single variables, when we use it to compare categories of categorical variables against a numeric scale, we place our variables on either side of the ~ operator, just as we would do in the linear regresison model command lm().\nThe table lists the usual descriptive statistics for wellbeing for men and women separately.\n\nQuestion\nSo what does the table tell us about differences in wellbeing between sexes? For brief definitions of each statistic in the table and the boxplots, see the Descriptive statistics lecture slides"
  },
  {
    "objectID": "Materials/Labs/Worksheet_4.html#step-4-model-the-relationship-between-your-variables",
    "href": "Materials/Labs/Worksheet_4.html#step-4-model-the-relationship-between-your-variables",
    "title": "Worksheet 8",
    "section": "Step 4: Model the relationship between your variables",
    "text": "Step 4: Model the relationship between your variables\nNow we have some sense of gender-based differences in subjective wellbeing (or, whether women are happier than men) in our data, but if our aim is (a) to quantify these differences and be able to make predictions of wellbeing-scores knowing one’s gender, and (b) to check the probability that these differences do not only appear in our data by pure chance (measurement error) but they actually exist in the wider UK population from which our sample data was drawn, then we need to apply a statistical model.\nIn this exercise, we assume that there is a linear relationship between sex and wellbeing and thus model it using the same linear regression formula that we are familiar with. Let’s fit the model and save the results to an object called model_ex1 (to stand for our Exercise 1 model), then ask for a summary() of the model results:\n\nmodel_ex1 &lt;- lm(wellbeing ~ sex, data = ukhls)\n\nsummary(model_ex1)\n\n\nCall:\nlm(formula = wellbeing ~ sex, data = ukhls)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-25.523  -1.523   0.639   3.639  11.639 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 25.52269    0.04362  585.13   &lt;2e-16 ***\nsexfemale   -1.16174    0.05858  -19.83   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.52 on 35940 degrees of freedom\n  (3351 observations deleted due to missingness)\nMultiple R-squared:  0.01082,   Adjusted R-squared:  0.0108 \nF-statistic: 393.2 on 1 and 35940 DF,  p-value: &lt; 2.2e-16\n\n\nWe are already familiar with this output from Lab7, where we modelled wellbeing as a function of age. One difference that you might have spotted if you looked carefully is the name of the predictor variable shown in the output: sexfemale. What does this represent? The women? And if so, where are the men?\n\nFactors and other variable types in R\nR detects that the sex variable is saved in the dataset as being a categorical variable (i.e. a factor in the language of R). See below for more information about data/variable types in R:\n\n\nClick to View\n\n\n\n\n\n\n\n\n\n\nWe can check the type of each variable using the command class(); with the str() command we query the “structure” of the variable, which prints some more details about the variable; if we believe that a variable is of a certain type but we want to check, we can use functions such as is.factor, is.numeric, is.character etc., which will return a logical value (TRUE or FALSE):\n\n# Check a variable's type/class:\nclass(ukhls$sex)\n\n[1] \"factor\"\n\n# Check the structure of the variable:\nstr(ukhls$sex)\n\n Factor w/ 2 levels \"male\",\"female\": 2 2 2 1 2 2 1 1 2 1 ...\n - attr(*, \"label\")= chr \"sex\"\n\n# Check if a variable is a 'factor':\nis.factor(ukhls$sex)\n\n[1] TRUE\n\n\nThe dataset we are using has been prepared in advance so that most of the variables are assigned the appropriate type, and we can check their types in the Variable column of the data dictionary page too. But we may need to change a variable’s type at some point; we’ll learn how to do it when the problem emerges.\n\n\n\nAutomatic releveling of factor variables in regression models\nBecause R detects that the sex variable is as a factor, it treats it as such. When lm() encounters a factor variable with two levels/categories, it creates a new variable based on the second level. In our example, the second level is female, and ‘sexfemale’ was created automatically. It is a binary variable that takes the value 1 if the value of ‘gender’ is female, and 0 if the value of ‘gender’ is not female (i.e. is male).\nThe level coded as 0 is called the reference level or base level and is “absorbed” by the intercept, which is why it is not showing as a separate level/category in the list of coefficients. Just like in the wellbeing-by-age model in Lab7, where the intercept referred to age=0 (or to median age after we median-centred the age variable to give the intercept more meaning), here the intercept value refers to sexfemale = 0 = male.\nThe same automatic releveling is done in the background when the factor variable has more than two levels/categories, as we’ll see later.\n\nTo avoid mistakes, it is often safer to coerce a variable to a given measurement type within the model itself, which has the added advantage that the labels in the summary output will be more easily legible. We can coerce a variable to factor type with the as.factor() function. Compare the output from the command below to what we got previously:\n\n# ! This will overwrite the previous model object\nmodel_ex1 &lt;- lm(wellbeing ~ as.factor(sex), data = ukhls)\n\nsummary(model_ex1)\n\nYou will find that nothing has changed apart from the label for the female level; all the coefficients are the same.\nIf we preferred to get the regression (slope) coefficient for males instead of females, we could first relevel the sex variable by hand and then refit the model. In case of dichotomous factors (with only two levels) there is no substantive reason to do this, because the two coefficients mirror each other: if females are expected to score -1.16 points lower on subjective wellbeing compared to the males, then the males logically are expected to score 1.16 points higher than the females.\nSetting the reference category manually will be more useful with polytomous factors (those with more than two levels), as we’ll see in Exercise 2."
  },
  {
    "objectID": "Materials/Labs/Worksheet_4.html#step-5-present-and-interpret-your-findings",
    "href": "Materials/Labs/Worksheet_4.html#step-5-present-and-interpret-your-findings",
    "title": "Worksheet 8",
    "section": "Step 5: Present and interpret your findings",
    "text": "Step 5: Present and interpret your findings\nWe already gave a partial interpretation of the results in the previous paragraph. As we noted, the intercept here represents the males, or more specifically the average subjective wellbeing score for male respondents, and the slope coefficient for females tells us that women score 1.16-point lower than men. The regression equation is thus:\n\\[\\widehat{wellbeing} = 25.52 - 1.16 \\times 1(for\\;female)\\]\n\nor\n\n\\[\\widehat{wellbeing} = 25.52 + 1.16 \\times 0(for\\;male)\\]\nThis is the same information that we got from the summary statistics by group that we did in Step 1. There we saw that the average score for males was 25.52 and the average for females was 24.36; if we ask R to calculate 24.36 - 25.52 for us we get -1.16, which is the regression coefficient for females from the model output.\nBut the regression modelling framework allows us to answer some deeper questions. Our aim is always to extrapolate from our sample data to the wider population from which the representative sample was drawn (here, the UK population). The Standard Error, the t value, the F-statistic and their associated p-values all refer to this step of drawing inferences about our population of interest from the actual small data that we collected.\n\nInterpret the inferential statistics\nAgresti (2018: 85-89, 114-120, 140-151, 187-190 and 321-325) and Gelman et al. (2020: 50-55, 57-62 and 147) provide good brief introductions and examples of these inferential statistics. Take some time to look over those pages again and examine the results from our model.\n\nIn this very simple bivariate example with only two variables, the t and F statistics and their p-values correspond to those we would obtain from a two-sample t-test and a one-way analysis of variance (ANOVA), respectively.\nThe t-test tests whether differences between two groups measured against a quantitative variable are statistically significant or not. ANOVA tests whether two or more population variances are statistically significantly different. In the simple case where there are only two groups, the t-test and the ANOVA measure the same thing and produce the same results, and the two are related by the simple formula: \\(t^2 = F\\). Our linear regression output contains both statistics, so we can check this out in R:\n\n# t-value for the factor variable: -/+ 19.83\nt &lt;- -19.83\n\n# F-statistic: 393.2\nF &lt;- 393.2\n\n# Is the statement \"t squared (and rounded to one decimal point) equals F\" true?\nround(t^2, 1) == F\n\n[1] TRUE\n\n\nWe can also run a t-test or an ANOVA with our data using the commands below:\n\n# A t-test; to get exactly the same test statistic as in the regression, we must assume that the variances in the two groups are equal ('var.equal=TRUE')\nt.test(wellbeing ~ sex, data = ukhls, var.equal=TRUE)\n\n\n    Two Sample t-test\n\ndata:  wellbeing by sex\nt = 19.83, df = 35940, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group male and group female is not equal to 0\n95 percent confidence interval:\n 1.046915 1.276567\nsample estimates:\n  mean in group male mean in group female \n            25.52269             24.36095 \n\n\n# An ANOVA model; we can view its summary() just as with the lm() models\naov(wellbeing ~ sex, data = ukhls) %&gt;% summary()\n\n               Df  Sum Sq Mean Sq F value Pr(&gt;F)    \nsex             1   11984   11984   393.2 &lt;2e-16 ***\nResiduals   35940 1095238      30                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n3351 observations deleted due to missingness\n\n\nTake a minute to compare the outputs and the statistics.\nT-tests and ANOVAs are very commonly used in disciplines where data from randomized experiments is prevalent. In sociology, this is less often the case, as sociological data are usually observational (non-experimental), complex, large, and most of the variables we are interested in are measured on categorical and ordinal scales rather than continuous numeric scales with statistically well-defined distributional characteristics. We are more likely to encounter t-values and F statistics as part of broader regression models rather than as the maim methods of analysis. But we have seen in the example above how these statistics relate to each other. As we will see in later exercises, regression models are particularly useful because they can be extended significantly to cases and questions that simple statistical tests cannot tackle. But in terms of the aim of drawing inferences about a wider unobserved population from a small observed set of data, the logic these tests underpins more complex models.\nIn the classical null-hypothesis significance-testing framework, both the t-test and the F test set up the “null hypothesis” (\\(H_0\\)) that differences between the groups (here males and females) are nil or very close to zero; in other words, that the groups are equal in respect to subjective wellbeing. The counterpart to this is the “alternative hypothesis” (\\(H_a\\) or \\(H_1\\)) that the differences are not equal to 0. In a hypothesis test, the deviation of the data from the null hypothesis is summarised by the p-value: the probability of observing something at least as extreme as the observed test statistic (i.e. the t or the F here). The possible outcomes of a hypothesis test are “reject” or “not reject.” It is never possible to “accept” a statistical hypothesis, only to find that the data are not sufficient to reject it. This wording may feel cumbersome but we need to be careful, as it is a common mistake for researchers to act as if an effect is negligible or zero, just because this hypothesis cannot be rejected from data at hand.\nIn common practice, the null hypothesis is said to be “rejected” if the p-value is less than 0.05 — that is, if the 95% confidence interval for the parameter excludes zero. Put differently, statistical significance is conventionally defined as a p-value less than 0.05, relative to some null hypothesis or prespecified value that would indicate no effect present. For fitted regressions, this roughly corresponds to coefficient estimates being considered as statistically significant if they are at least two standard errors from zero, or not statistically significant otherwise.\nSpeaking more generally, an estimate is said to be not statistically significant if the observed value could reasonably be explained by simple chance variation, much in the way that a sequence of 20 coin tosses might happen to come up 8 heads and 12 tails; we would say that this result is not statistically significantly different from chance. In that example, the observed proportion of heads is 0.40 but with a standard error of 0.11 — thus, the data are less than two standard errors away from the null hypothesis of 50%.\nIn our model the p-value associated with “sexfemale” is \\(&lt; 2e-16\\) in scientific notation, which we know from last week is a very small number that can be calculated as \\(2\\times10^{-16}\\) (in R: (2*10)^-16 %&gt;% format(scientific = FALSE) = 0.000000000000000000001525879). It is definitely well below the conventional value of 0.05, and as the regression summary output indicates with the *** notation, it is lower than 0.001. This p-value tells us the probability that the true test statistic in the wider population is at least as extreme as the one we found in our data, if the \\(H_0\\) were true. In other words, if there really was no difference at all in the average “subjective wellbeing” between men and women in the UK population, then the probability of us observing the t-value we observed in our sample data would be lower than 0.001, or 0.1%. So we reject the assumption that there really is no difference between men and women in the population and provisionally accept that the evdence we have points to a real diference in the population. A very common but not too informative way of expressing this result is to say that we found a statistically significant association between gender and subjective wellbeing, or that gender has a statistically significant effect on subjective wellbeing.\nIn scientific publications if you manage to find such a *** statistical significance for your main hypothesis - the one that directly answers one of your research questions - you’ve hit the jackpot. Yet, most of our simple regressions and tests using the UKHLS to address a sociologically relevant research question are likely to be highly “statistically significant”. The reason is that our dataset is extremely large in the context of the simple tests for which null-hypothesis significance testing had been designed. We will also notice that while our p-value is very low (and therefore statistically significant), thea actual size of the effect that we measured was very moderate: the difference between the male and female scores on the subjective wellbeing scale was really not all that impressive. In other words, the difference may truly exist in the whole population of the UK, not only in our data. but the difference is so small as to be completely uninteresting from a practical point of view. One could ask: so what if women score 1.16 points lower on a 37-point self-reported scale? Does that really have any social implications? And, importantly, can we really be sure that the fact of being a woman is what truly drives this difference, or might there be other factors at stake? For example, what if income is the factor that really makes a difference, and being a women merely reflects the fact that on average they earn less than the men?\nThese are the questions that a multiple regression model can try to answer by accounting for the joint effect of several predictors. Before starting to develop our simple model into a multiple linear regression, reflect on what Gelman et al. say about p-values and over-reliance on statistical significance:\n\nIn the fields in which we work, we do not generally think null hypotheses can be true: in social science and public health, just about every treatment one might consider will have some effect, and no comparisons or regression coefficient of interest will be exactly zero. We do not find it particularly helpful to formulate and test null hypotheses that we know ahead of time cannot be true. Testing null hypotheses is just a matter of data collection: with sufficient sample size, any hypothesis can be rejected, and there is no real point to gathering a mountain of data just to reject a hypothesis that we did not believe in the first place. That said, not all effects and comparisons are detectable from any given study. So, even though we do not ever have the research goal of rejecting a null hypothesis, we do see the value of checking the consistency of a particular dataset with a specified null model. The idea is that non-rejection tells us that there is not enough information in the data to move beyond the null hypothesis. … Conversely, the point of rejection is not to disprove the null … but rather to indicate that there is information in the data to allow a more complex model to be fit (Gelman et at. 2020: 59-60).\n\n\nCoding tip: Model summary using jtools::summ()\nThe base R summary() function is useful, but there are other functions that print model summary statistics in much nicer default formats. One that I like is the summ() function from the {jtools} package.\n\n\nClick to View\n\nWe have installed and loaded the {jtools} package in Exercise 0, so we should have access to its functions. The basic model summary command is the following:\n\njtools::summ(model_ex1)\n\nBy default, the output reports statistics with two decimal points precision. We can easily change that with the digits= option if needed (in most cases, three decimals are the most useful); another very useful option is to request confidence intervals instead of standard errors in the output with the confint option. Confidence intervals are much more straightforward to interpret and use than the standard error. The command with these options included and the output look like this:\n\njtools::summ(model_ex1, digits = 3, confint = TRUE)\n\n\n\nMODEL INFO:\nObservations: 35942 (3351 missing obs. deleted)\nDependent Variable: wellbeing\nType: OLS linear regression \n\nMODEL FIT:\nF(1,35940) = 393.247, p = 0.000\nR² = 0.011\nAdj. R² = 0.011 \n\nStandard errors: OLS\n--------------------------------------------------------------\n                      Est.     2.5%    97.5%    t val.       p\n----------------- -------- -------- -------- --------- -------\n(Intercept)         25.523   25.437   25.608   585.129   0.000\nsexfemale           -1.162   -1.277   -1.047   -19.830   0.000\n--------------------------------------------------------------\n\n\nThe package also contains a number of very useful functions for plotting the regression results to graphs and making it easier to assess them visually. We will explore that in the context of slightly more complex models later."
  },
  {
    "objectID": "Materials/Labs/Worksheet_5.html#step-0-formulate-your-research-question",
    "href": "Materials/Labs/Worksheet_5.html#step-0-formulate-your-research-question",
    "title": "Worksheet 9",
    "section": "Step 0: Formulate your “research question”",
    "text": "Step 0: Formulate your “research question”\nSay we want to find out what are the factors that influence smoking behaviour (i.e. smoking or not smoking). If this is our research question, then we need a dependent variable relating to smoking behaviour and some independent variables relating to possible influencing factors.\nWe know that in the UKHLS (Understanding Society) dataset we have a useful variable relating to smoking, which could serve as the dependent variable (if we search the data catalogue we find the following variable: smoker, a factor variable with two levels (“yes” and “no”) with relatively few missing values (4.4%). We already see there how the variable is distributed, with the majority of the respondents (85%) being non-smokers.\nNow that we know that the variable we want to predict (“smoker”) is a categorical variable with two categories, we know that a linear regression would not be the appropriate method. Instead, the statistical model we should use is the logistic regression.\nNow we need to ask ourselves about what independent variables we want to include. These are the variables that can potentially influence smoking behaviour. This is primarily a theoretical question (i.e. thinking logically, or based on our readings of sociological literature, what factors might have an influence on smoking?), but we also need to make sure that any variable (factor) we think of, actually exists in our data-set. So let’s look at our data-set and search for some possible factors.\nFor this example exercise I propose that we consider the following three variables as our independent variables: sex, age_dv and finnow [“Subjective financial situation - current”].\nIf this were your assessment research question, you would start by first examining each variable separately using descriptive statistics, and examining how predictors are associated with the dependent variable using bivariate descriptive statistics. After that, you would want to check how all the factors considered together might influence smoking behaviour, for which a logistic regression is the appropriate method."
  },
  {
    "objectID": "Materials/Labs/Worksheet_5.html#step-1-find-describe-and-understand-your-variables",
    "href": "Materials/Labs/Worksheet_5.html#step-1-find-describe-and-understand-your-variables",
    "title": "Worksheet 9",
    "section": "Step 1: Find, describe and understand your variables",
    "text": "Step 1: Find, describe and understand your variables\nWe have already practised descriptive statistics a lot, so this step should be easy by now. We also already know how the “age_dv” and “sex” variables look like, but for the sake of practice - as you would also do in your assignment! - let’s run some descriptive statistics on them too. “Smoker” and “finnow” (or “=current financial situation, financial situation now) are new variables to us.\n\n\n### Complete the code below\n\n\n# Descriptive statistics for 'sex':\n\n...\n\n# Descriptive statistics for 'age_dv':\n\n...\n\n# Descriptive statistics for 'smoker':\n\n...\n\n\n# Descriptive statistics for 'finnow':\n\n...\n\n\n\nQuestions\nExamine the descriptive results from the tables you’ve produced and answer:\n\nWhat is the valid percentage of smokers in the dataset?\nWhat type of variable is finnow?\nWhat is the total number of survey respondents who declared that they are finding it quite or very difficult to get by with their available financial resources?\n\n\n\nCoding tip: Use select() with summary()\nThere’s an easy way to get some quick summary statistics for a selected small number of variables of interest from a dataset. We can first use select() to select out the variables we want, then ask for a summary() on the whole of that selected mini-dataset:\n\nukhls %&gt;% \n  select(sex, age_dv, smoker, finnow) %&gt;% \n  summary()\n\n     sex            age_dv        smoker     \n male  :17925   Min.   : 16.00   Yes : 5519  \n female:21368   1st Qu.: 34.00   No  :32058  \n                Median : 49.00   NA's: 1716  \n                Mean   : 48.77               \n                3rd Qu.: 63.00               \n                Max.   :102.00               \n                        finnow     \n Living comfortably        :12459  \n Doing alright             :14748  \n Just about getting by     : 7570  \n Finding it quite difficult: 1882  \n Finding it very difficult :  800  \n NA's                      : 1834  \n\n\nNote that this only shows basic descriptives, so for example we do not get the percentage distribution of the frequencies in the frequency tables for categorical variables, so we can’t easily answer all the questions above using this."
  },
  {
    "objectID": "Materials/Labs/Worksheet_5.html#step-2-modify-your-variables-if-needed",
    "href": "Materials/Labs/Worksheet_5.html#step-2-modify-your-variables-if-needed",
    "title": "Worksheet 9",
    "section": "Step 2: Modify your variables (if needed)",
    "text": "Step 2: Modify your variables (if needed)\nNow that we know how our variables look like, let’s think: are there any variables that require some modification? Not necessarily, in this case, becasue we don’t have any obvious mistakes or superfluous labels/categories in these variables. We know from previous weeks that recoding the age variable by median-centring it may help with the interpretation of the regression results because then the “Intercept” coefficient would not refer to someone who is aged 0, but to someone who has the median age in our dataset. Because we usually do not need to interpret the “intercept” in multiple regression (with many predictors included in the model), we can leave the age variable as it is for now. The only change that we may want to ponder is whether combining the last two levels/categories of the financial situation variable may be sensible. The reason for this may be because these two levels contain far fewer responses than the other three categories, and it’s always better to have a balanced categorical variable. However, the number of responses in each category is still large enough not to cause statistical complications for the model. The other reason why we may want to collapse the last two categories into one, is that it will make reading the regression output far more manageable. We know from the previous week that if we include a categorical variable as predictor into a regression model, we need to break it down into a series of new binary/dummy variables, one for each level/category. We know that R does this automatically for us if we tell it that our variable is a factor (i.e. categorical), but then in the output we will have quite a few variables to interpret, each with its own regression coefficient. Finally, if we wanted to collapse the last two categories, we could do so because it would make sense conceptually to group together those who are “finding it … difficult”. We may even be able to make an argument that those who are “just about getting by” are closer to those who are finding it difficult than to those who are “doing alright”, and we may decide to collapse the last three categories together. These are all subjective choices for us as data analysts to make. We need to consider both statistical/mathematical criteria (e.g. the number of cases/responses in the various categories) as well as substantive sociological knowledge about the topic (in this, case, self-assessed poverty). Cutting out or collapsing data always means that we are throwing away additional information that could potentially be useful. It’s a trade-off that we need to consider. But as a first step in any analysis, once we have considered what transformations was be possible or necessary, I suggest that you always fit a first model on the complete dataset to get a sense of how the model behaves, and then decide whether any transformation may be preferable. So, for now, we won’t change anything."
  },
  {
    "objectID": "Materials/Labs/Worksheet_5.html#step-3-describe-the-relationship-between-your-variables",
    "href": "Materials/Labs/Worksheet_5.html#step-3-describe-the-relationship-between-your-variables",
    "title": "Worksheet 9",
    "section": "Step 3: Describe the relationship between your variables",
    "text": "Step 3: Describe the relationship between your variables\nWe have many variables here, so we will need to focus on the most useful comparisons. What we are interested in is how our predictor variables relate to our outcome variable, smoker.\n\nBoxplots to compare group medians\nWe have already practiced describing the relationship between numeric and categorical variables visually using box-plots, and we can apply the same here to compare the median ages of smokers and non-smokers in our dataset:\n\nboxplot(age_dv ~ smoker, data = ukhls)\n\n\n\n\n\n\n\n\n\nQuestions\n\nIs the median age of smokers higher than that of non-smokers?\nAre there any extreme outliers?\n\n\nTo describe the relationship between two categorical variables, we need to a method we haven’t practiced before: a cross-tabulation or contingency table. You should be very familiar with these types of tables. They show the distribution of a categorical variable contingent on another categorical variable.\nAgain, there are several ways to get simple contingency tables and there are numerous functions is various packages that make more complex or nicely formatted contingency tables. For simplicity, we’ll only look at the options available using base-R and the packages we are already familiar with.\n\n\nCrosstabulations: the xtabs() function\nUsing the base-R function xtabs() we can write:\n\nxtabs( ~ smoker + sex, data = ukhls)\n\n      sex\nsmoker  male female\n   Yes  2706   2813\n   No  14062  17996\n\n\nThis shows us the frequency distribution of the smoker variable by each category of the sex variable, and vice-versa, depending on whether we read it row-wise or column-wise. If we want to also see the marginal totals (for columns and rows), we can pipe on the table to the addmargins() function:\n\nxtabs( ~ smoker + sex, data = ukhls) %&gt;% addmargins()\n\n      sex\nsmoker  male female   Sum\n   Yes  2706   2813  5519\n   No  14062  17996 32058\n   Sum 16768  20809 37577\n\n\nIf instead of frequencies we would like to see the proportions (percentages), we can pass the xtabs table on to the prop.table() function, where we should also select whether we are interested in row or column percentages using the margin = option (where 1 stands for rows and 2 for columns). It’s up to us to decide which option is best for our analysis. We can do both in turn, because they help us answer different questions.\nIn general, if we have a clear outcome variable - like smoker in our case - then we would want to place its categories in the rows and show the values of the predictor (sex in our case) in the columns. If so, then column proportions would tell us the ratio of smokers and non-smokers among women and men, while row percentages would tell us the ratio of men/women among the smokers or non-smokers:\n\n\n# row proportions:\n\nxtabs( ~ smoker + sex, data = ukhls) %&gt;% prop.table(margin = 1)\n\n      sex\nsmoker      male    female\n   Yes 0.4903062 0.5096938\n   No  0.4386425 0.5613575\n\n\n# column proportions:\n\nxtabs( ~ smoker + sex, data = ukhls) %&gt;% prop.table(margin = 2)\n\n      sex\nsmoker      male    female\n   Yes 0.1613788 0.1351819\n   No  0.8386212 0.8648181\n\n\nIf we’re annoyed by the many decimals and want to see a cleaner table rounded down to two decimal places, we can pass the prop.table further to a round() function, where we also specify the desired number of decimal places. Let’s say we want to show only 2 decimals (we can also simplify the previous command by not spelling out “margins =” in the command and simply giving it’s value):\n\nxtabs( ~ smoker + sex, data = ukhls) %&gt;% \n  prop.table(1) %&gt;% \n  round(2)\n\n      sex\nsmoker male female\n   Yes 0.49   0.51\n   No  0.44   0.56\n\n\nxtabs( ~ smoker + sex, data = ukhls) %&gt;% \n  prop.table(2) %&gt;% \n  round(2)\n\n      sex\nsmoker male female\n   Yes 0.16   0.14\n   No  0.84   0.86\n\n\nIf we have row proportions, the values across rows should add up to 1 (i.e. 100%); if we have column proportins, then they should add up to 1 vertically, across columns; if they don’t, then we’ve done something wrong. For example, if we do not specify a margin =, then by default the proportions will be calculated per cell (rather than by row or column) respective to the total number of respondents.\nIf we prefer to convert the proportions to percentages, we can simply multiply the prop.table by 100:\n\n# row proportions\n\nxtabs( ~ smoker + sex, data = ukhls) %&gt;% \n  prop.table(1) * 100\n\n      sex\nsmoker     male   female\n   Yes 49.03062 50.96938\n   No  43.86425 56.13575\n\n\n# column proportions\n\nxtabs( ~ smoker + sex, data = ukhls) %&gt;% \n  prop.table(2) * 100\n\n      sex\nsmoker     male   female\n   Yes 16.13788 13.51819\n   No  83.86212 86.48181\n\n\n\n\nCrosstabulations: the flat_table() function from {sjmisc}\nWe’ve already used the {sjmisc} package, and it contains another function that may be useful for crosstabulations: flat_table(). It gives similar results to those above, but the code may be a bit more simple. We just need to list the variables we want to cross-tabulate, separated by commas; to show proportions, we can add an additional argument margin = and choose either “row” or “col” as a value. The results are shown directly as percentages (i.e. proportions * 100) rounded down to two decimals:\n\nsjmisc::flat_table(smoker, sex, data = ukhls)\n\n       sex  male female\nsmoker                 \nYes         2706   2813\nNo         14062  17996\n\n\n# or as row percentages:\n\nsjmisc::flat_table(smoker, sex, data = ukhls , margin = \"row\")\n\n       sex  male female\nsmoker                 \nYes        49.03  50.97\nNo         43.86  56.14\n\n\n# or as column percentages:\n\nsjmisc::flat_table(smoker, sex, data = ukhls , margin = \"col\")\n\n       sex  male female\nsmoker                 \nYes        16.14  13.52\nNo         83.86  86.48\n\n\nThe issue with this function is that it cannot include marginal totals; for that, the xtabs() %&gt;% addmargins() option would be the way to go.\n\nQuestions\nSo, what have we learned so far about the relationship between sex and smoking behaviours? - Are there more men or women among the non-smokers? - Is the percentage of smokers among men higher or lower than among women?\n\n\nTask\nUsing the functions we’ve learnt above, summarise the relationship between smoking behaviour and financial situation using cross-tabulations. Tip: because the finnow variable has so many levels (categories), it’s visually better to place it in the rows.\n\n### Complete the tasks below:\n\n# Contingency table of 'finnow' by 'smoker' showing frequencies and marginal totals\n\n...\n\n\n# Contingency table of 'finnow' by 'smoker' showing column percentages:\n\n...\n\n# Contingency table of 'finnow' by 'smoker' showing column percentages:\n\n...\n\n\n\nQuestions\nWhat have we learned about the relationship between subjective financial situation and smoking behaviour? - Among which financial-situation group is the proportion of smokers the highest? - Which financial-situation group has the highest percentage among the smokers? And explain why this is the case, give the answer to the previous question! (tip: look again at the frequency contingency table too) - Overall, based on these results, would you say that there is an association between financial situation and smoking behaviour? If so, why do you think that?"
  },
  {
    "objectID": "Materials/Labs/Worksheet_5.html#step-4-model-the-relationship-between-your-variables",
    "href": "Materials/Labs/Worksheet_5.html#step-4-model-the-relationship-between-your-variables",
    "title": "Worksheet 9",
    "section": "Step 4: Model the relationship between your variables",
    "text": "Step 4: Model the relationship between your variables\nOf course, we need to check whether your answer to the last question above stands up against statistical validation. This is where regression modelling comes in. We can fit three separate bivariate models here, checking in turn the predictive power of each of the three predictor variables, as we have done in previous weeks. Then, our final aim is to fit a model that estimates the effect of financial situation on smoking behaviour while also accounting for the effect of age and sex; that is, a multiple regression with more than a single predictor. In this exercise we will fit the final, full model to get an idea of the overall picture, but you can then check on your own how simpler models with fewer predictors behave.\nOur challenge compared to previous weeks is that now our outcome variable of interest, smoker, is not a numeric/scale variable but a dichotomous, categorical one. So the units in which we can measure any effect on this outcome “scale” of 0/1 are no longer that easy to get our heads around. That means that interpreting the results is where the challenge will be.\nThe actual fitting of the model is fairly simple: we can use the function that fits a generalised linear model (glm()) using similar syntax to that of linear *m**odel (lm()), with the only difference that we should specify which “family” of distributions the outcome variable belongs to; a binary/dichotomous outcome has a so-called “binomial” distribution, and that’s what we need to tell R:\n\nmodel_logistic &lt;- glm(smoker ~ finnow + sex + age_dv, family = binomial, data = ukhls)\n\nWith the code above, we have saved the logistic regression output as an object called ‘model_logistic’, so we can perform some additional work on that model, such as getting a summary of it. Previously we have used both the summary() function from base R as well as the summ() function from the {jtools} package. This latter gives us better formatted summmaries and has several further options that will come extremely useful now wich logistic regressions; so let’s ask for a summ():\n\njtools::summ(model_logistic, digits=3)\n\n\n\nMODEL INFO:\nObservations: 37449 (1844 missing obs. deleted)\nDependent Variable: smoker\nType: Generalized linear model\n  Family: binomial \n  Link function: logit \n\nMODEL FIT:\nχ²(6) = 1253.769, p = 0.000\nPseudo-R² (Cragg-Uhler) = 0.058\nPseudo-R² (McFadden) = 0.040\nAIC = 30034.905, BIC = 30094.620 \n\nStandard errors: MLE\n---------------------------------------------------------------\n                                 Est.    S.E.    z val.       p\n---------------------------- -------- ------- --------- -------\n(Intercept)                     1.720   0.055    31.324   0.000\nfinnowDoing alright            -0.517   0.040   -12.832   0.000\nfinnowJust about getting       -1.007   0.043   -23.360   0.000\nby                                                             \nfinnowFinding it quite         -1.329   0.062   -21.607   0.000\ndifficult                                                      \nfinnowFinding it very          -1.630   0.082   -19.872   0.000\ndifficult                                                      \nsexfemale                       0.238   0.030     8.005   0.000\nage_dv                          0.010   0.001    12.454   0.000\n---------------------------------------------------------------\n\n\n\nFrom log-odds to Odds Ratios\nThe coefficients (Est.imates) returned by our logit model are a bit difficult to interpret intuitively because they are measured on a strange mathematical scale called the log-odds, but which can be converted back to probabilities and odds ratios if we want. The advantage of log odds is that it follows the same logic we already know from linear regression: either an increase or a decrease of x value - for instance, here, being female as opposed to male is associated with 0.238 higher log-odds of being a non-smoker (remember that the smoker variable is coded such that yes is the first category (0) and no is the second category (1), so we are getting results for the probability of the second (1) category compared to the first (0), so for being non-smokers). But what this actually means in practice is very unclear.\nIt is more common to report odds ratios (OR) instead. An odds ratio less than 1 means that a unit increase in the value of the predictor variable leads to a decrease in the odds that the outcome is equal to its value 1 (being a non-smoker in our case). An odds ratio greater than 1 means that an increase in the predictor leads to an increase in the odds that the outcome is 1 (non-smoker). In general, the percent change in the odds given a one-unit change in the predictor can be determined as:\n\\[\nPercent Chance in Odds = 100\\times(OR-1)\n\\]\nThe summ() function of the {jtools} package makes it easier to request odds ratios (same as “exponentiated coefficients”) with the exp = option:\n\njtools::summ(model_logistic, digits=3, exp = TRUE)\n\n\n\nMODEL INFO:\nObservations: 37449 (1844 missing obs. deleted)\nDependent Variable: smoker\nType: Generalized linear model\n  Family: binomial \n  Link function: logit \n\nMODEL FIT:\nχ²(6) = 1253.769, p = 0.000\nPseudo-R² (Cragg-Uhler) = 0.058\nPseudo-R² (McFadden) = 0.040\nAIC = 30034.905, BIC = 30094.620 \n\nStandard errors: MLE\n--------------------------------------------------------------------------\n                               exp(Est.)    2.5%   97.5%    z val.       p\n---------------------------- ----------- ------- ------- --------- -------\n(Intercept)                        5.583   5.013   6.217    31.324   0.000\nfinnowDoing alright                0.596   0.551   0.645   -12.832   0.000\nfinnowJust about getting           0.365   0.336   0.398   -23.360   0.000\nby                                                                        \nfinnowFinding it quite             0.265   0.235   0.299   -21.607   0.000\ndifficult                                                                 \nfinnowFinding it very              0.196   0.167   0.230   -19.872   0.000\ndifficult                                                                 \nsexfemale                          1.269   1.197   1.345     8.005   0.000\nage_dv                             1.010   1.009   1.012    12.454   0.000\n--------------------------------------------------------------------------\n\n\nWe see that we are also getting confidence intervals instead of standard errors by default, which can be again easier to interpret.\nThe ‘exp(Est.)’ (for “exponentiated coefficient”) estimates shown are the odds ratios, and while they look very different from what we are used to from linear regression (i.e. they do not take negative values but run from 0 to infinity, with anything between 0 and 1 meaning a decrease and anything above 1 meaning an increase), they are easier to interpret in terms of percentage change in the odds of being a non-smoker:\n\nthe odds of being a non-smoker if you are a woman are 100*(1.269-1)= 26.9% higher than if you were a man;\nthe odds of being a non-smoker increase by 100*(1.010-1)= 1% with each additional year of age\nthe odds of being a non-smoker if you find it very difficult to manage financially as opposed to Living comfortably are 100*(0.196-1)= -80.4% - in other words, much lower. Remember that here the living comfortably variable is the base or reference level to which all other categories of ‘finnow’ compare\nand so on for the other estimates; basically, we subtract a 1 from the estimate number and move the decimal point two spots to the right to get a percentage of odds.\n\nIf you are the kind of person who likes betting on horse races in the weekend, the concept of odds will make perfect sense; the res of us can take away the main idea that we either see an increase or a decrease of a certain amount in respect to the outcome, so we gain a general understanding of how the various factor we are assessing affect the probability of the outcome (being a non-smoker)."
  },
  {
    "objectID": "Materials/Labs/Worksheet_5.html#exercise-2-make-it-simple",
    "href": "Materials/Labs/Worksheet_5.html#exercise-2-make-it-simple",
    "title": "Worksheet 9",
    "section": "Exercise 2: Make it simple",
    "text": "Exercise 2: Make it simple\nWe started with a complex model to get an idea of the whole picture. But it’s generally useful to have a quick look at simpler models too that only include a subset of the variables of interest. For example, it would be interesting to see what the effect of financial situation on smoking is if we do not also control for age and gender. Try to fit this model and check if any of the main results change significantly.\n\n# Fit a simpler model"
  },
  {
    "objectID": "Materials/Labs/Worksheet_5.html#exercise-3-on-your-own",
    "href": "Materials/Labs/Worksheet_5.html#exercise-3-on-your-own",
    "title": "Worksheet 9",
    "section": "Exercise 3: On your own",
    "text": "Exercise 3: On your own\nIf there’s time left, chose another set of variables that can help explain a theoretically/sociologically interesting question and following the steps in Example 1,work towards fitting a logistic regression model.\nTIP: if you have looked at the Assignment 2 research question options you will know that there is a question there that can be answered using logistic regression; if you’re interested in that one, you can give it a go!"
  },
  {
    "objectID": "Structure.html",
    "href": "Structure.html",
    "title": "Timetable",
    "section": "",
    "text": "calendR::calendR(year = 2021, month = 8, papersize = \"A0\")\n\n\n\n\n\n\n\n Back to top"
  }
]